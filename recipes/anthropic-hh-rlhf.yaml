# Recipe for anthropic/hh-rlhf
# Human preference data for training helpful and harmless AI assistants

name: anthropic/hh-rlhf
version: "1.0"

source:
  type: huggingface
  id: Anthropic/hh-rlhf

generation:
  synthetic_ratio: 0.0
  human_ratio: 1.0
  teacher_models: []
  methods:
    - type: human_annotation
      platform: Upwork
      description: |
        Crowdworkers were asked to have conversations with an AI assistant
        and choose between two possible responses at each turn. The chosen
        response was labeled as preferred.
    - type: red_teaming
      description: |
        Additional data collected through red-teaming exercises where
        annotators attempted to elicit harmful responses.

cost:
  estimated_total_usd: 500000
  breakdown:
    api_calls: 0
    human_annotation: 450000
    compute: 50000
  confidence: low

reproducibility:
  score: 8
  available:
    - full_dataset
    - methodology_paper
    - annotation_guidelines
    - dataset_statistics
    - data_format_specification
  missing:
    - exact_annotator_pool
    - quality_control_details
    - inter_annotator_agreement
  notes: |
    This is one of the most well-documented RLHF datasets. The paper provides
    detailed methodology, and the full dataset is publicly available. The main
    missing elements are specific details about annotator selection and training.

metadata:
  num_examples: 170000
  languages:
    - en
  license: mit
  tags:
    - rlhf
    - human-feedback
    - preference
    - helpfulness
    - harmlessness
  authors:
    - Anthropic
  paper_url: https://arxiv.org/abs/2204.05862
  homepage_url: https://github.com/anthropics/hh-rlhf
