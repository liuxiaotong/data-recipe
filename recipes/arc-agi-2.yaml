# Recipe for ARC-AGI-2
# A benchmark for testing AI reasoning capabilities

name: ARC-AGI-2
version: "2.0"

source:
  type: github
  id: arcprize/ARC-AGI-2
  url: https://github.com/arcprize/ARC-AGI-2
  homepage: https://arcprize.org/arc-agi/2/

description: |
  ARC-AGI-2 is a benchmark designed to stress test the efficiency and capability
  of state-of-the-art AI reasoning systems. The tasks challenge AI systems on
  symbolic interpretation, compositional reasoning, and contextual rule applicationâ€”
  areas where current models struggle despite humans solving every task.

generation:
  synthetic_ratio: 0.0
  human_ratio: 1.0
  teacher_models: []
  methods:
    - type: human_design
      description: Tasks designed through controlled human testing
      participants: 400+
      location: San Diego
      date: early 2025
    - type: human_verification
      description: All evaluation tasks verified solvable by at least two humans in under two attempts
      human_performance: 60%

data_composition:
  training_set:
    count: 1000
    description: Uncalibrated, public tasks
  public_eval_set:
    count: 120
    description: Calibrated tasks
  semi_private_eval_set:
    count: 120
    description: Limited third-party exposure
  private_eval_set:
    count: 120
    description: No third-party exposure
  total_tasks: 1360

cost:
  estimated_total_usd: 100000
  breakdown:
    human_design: 80000
    human_verification: 20000
  confidence: low
  notes: Cost estimated based on 400+ participant study

reproducibility:
  score: 8
  available:
    - full_dataset_public
    - task_specifications
    - evaluation_methodology
    - human_baseline_performance
    - github_repository
  missing:
    - exact_design_guidelines
    - participant_selection_criteria
    - individual_task_creation_process
  notes: |
    High reproducibility for using the benchmark, but creating similar
    tasks would require significant human effort and testing infrastructure.

metadata:
  num_examples: 1360
  task_type: visual_reasoning
  languages:
    - visual
  license: unknown
  tags:
    - reasoning
    - agi-benchmark
    - visual-puzzles
    - human-designed
    - efficiency-focused
  authors:
    - ARC Prize Foundation
  release_date: "2025"
  predecessor: ARC-AGI-1 (2019)
  paper_url: null
