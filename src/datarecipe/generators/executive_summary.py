"""Executive summary generator for decision makers.

Generates a 1-page executive summary with:
- Recommendation (go/no-go)
- Value assessment (1-10 score)
- Use cases and expected outcomes
- ROI analysis
- Key risks
"""

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Optional


class Recommendation(Enum):
    """Recommendation levels."""

    HIGHLY_RECOMMENDED = "highly_recommended"
    RECOMMENDED = "recommended"
    CONDITIONAL = "conditional"
    NOT_RECOMMENDED = "not_recommended"


@dataclass
class ValueAssessment:
    """Value assessment for a dataset."""

    # Overall score (1-10)
    score: float = 5.0

    # Recommendation
    recommendation: Recommendation = Recommendation.CONDITIONAL
    recommendation_reason: str = ""

    # Use cases
    primary_use_case: str = ""
    secondary_use_cases: list[str] = field(default_factory=list)

    # Expected outcomes
    expected_outcomes: list[str] = field(default_factory=list)

    # ROI analysis
    roi_ratio: float = 1.0  # Expected value / Cost
    roi_explanation: str = ""
    payback_scenarios: list[str] = field(default_factory=list)

    # Risks
    risks: list[dict[str, str]] = field(default_factory=list)  # [{level, description, mitigation}]

    # Competitive analysis
    alternatives: list[str] = field(default_factory=list)
    competitive_advantage: str = ""


# Dataset type configurations
DATASET_TYPE_CONFIG = {
    "preference": {
        "primary_use_case": "è®­ç»ƒå¥–åŠ±æ¨¡åž‹ (Reward Model) æˆ–ç›´æŽ¥åå¥½ä¼˜åŒ– (DPO)",
        "secondary_use_cases": [
            "RLHF è®­ç»ƒæµç¨‹",
            "æ¨¡åž‹å¯¹é½ (Alignment)",
            "å“åº”è´¨é‡è¯„ä¼°",
        ],
        "expected_outcomes": [
            "æå‡æ¨¡åž‹å“åº”è´¨é‡å’Œç”¨æˆ·æ»¡æ„åº¦",
            "å‡å°‘æœ‰å®³/ä¸å½“è¾“å‡º",
            "å¢žå¼ºæ¨¡åž‹éµå¾ªäººç±»åå¥½çš„èƒ½åŠ›",
        ],
        "value_multiplier": 1.5,  # High value for alignment
    },
    "evaluation": {
        "primary_use_case": "æ¨¡åž‹èƒ½åŠ›è¯„æµ‹å’ŒåŸºå‡†æµ‹è¯•",
        "secondary_use_cases": [
            "æ¨¡åž‹é€‰åž‹ä¾æ®",
            "è®­ç»ƒæ•ˆæžœéªŒè¯",
            "èƒ½åŠ›å·®è·åˆ†æž",
        ],
        "expected_outcomes": [
            "é‡åŒ–æ¨¡åž‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨çŽ°",
            "è¯†åˆ«æ¨¡åž‹èƒ½åŠ›çŸ­æ¿",
            "æ”¯æŒæ¨¡åž‹è¿­ä»£å†³ç­–",
        ],
        "value_multiplier": 1.2,
    },
    "sft": {
        "primary_use_case": "ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning)",
        "secondary_use_cases": [
            "é¢†åŸŸé€‚é…",
            "æŒ‡ä»¤éµå¾ªèƒ½åŠ›æå‡",
            "ç‰¹å®šä»»åŠ¡ä¼˜åŒ–",
        ],
        "expected_outcomes": [
            "æå‡æ¨¡åž‹åœ¨ç›®æ ‡é¢†åŸŸçš„è¡¨çŽ°",
            "å¢žå¼ºæŒ‡ä»¤ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›",
            "å®šåˆ¶åŒ–æ¨¡åž‹è¡Œä¸º",
        ],
        "value_multiplier": 1.3,
    },
    "swe_bench": {
        "primary_use_case": "ä»£ç ç”Ÿæˆå’Œè½¯ä»¶å·¥ç¨‹èƒ½åŠ›è¯„æµ‹",
        "secondary_use_cases": [
            "ä»£ç åŠ©æ‰‹æ¨¡åž‹è¯„ä¼°",
            "è‡ªåŠ¨åŒ–ä»£ç ä¿®å¤ç ”ç©¶",
            "è½¯ä»¶å·¥ç¨‹ AI åŸºå‡†",
        ],
        "expected_outcomes": [
            "è¯„ä¼°æ¨¡åž‹ä»£ç ç†è§£å’Œç”Ÿæˆèƒ½åŠ›",
            "è¯†åˆ«ä»£ç ä»»åŠ¡ä¸­çš„èƒ½åŠ›å·®è·",
            "æ”¯æŒä»£ç æ¨¡åž‹é€‰åž‹",
        ],
        "value_multiplier": 1.4,
    },
}

# Risk templates by factor
RISK_TEMPLATES = {
    "high_cost": {
        "level": "é«˜",
        "description": "å¤åˆ»æˆæœ¬è¾ƒé«˜ï¼Œéœ€è¦å……è¶³é¢„ç®—",
        "mitigation": "åˆ†é˜¶æ®µå®žæ–½ï¼Œä¼˜å…ˆå®Œæˆæ ¸å¿ƒå­é›†",
    },
    "expert_required": {
        "level": "ä¸­",
        "description": "éœ€è¦é¢†åŸŸä¸“å®¶å‚ä¸Žï¼Œäººæ‰èŽ·å–å¯èƒ½å›°éš¾",
        "mitigation": "æå‰å‚¨å¤‡äººæ‰ï¼Œæˆ–è€ƒè™‘å¤–åŒ…åˆä½œ",
    },
    "quality_variance": {
        "level": "ä¸­",
        "description": "æ ‡æ³¨è´¨é‡å¯èƒ½å­˜åœ¨æ³¢åŠ¨",
        "mitigation": "å»ºç«‹ä¸¥æ ¼ QA æµç¨‹ï¼Œè®¾ç½®è´¨é‡é—¨æ§›",
    },
    "time_intensive": {
        "level": "ä¸­",
        "description": "é¡¹ç›®å‘¨æœŸè¾ƒé•¿",
        "mitigation": "åˆç†è§„åˆ’é‡Œç¨‹ç¢‘ï¼Œè®¾ç½®é˜¶æ®µæ€§äº¤ä»˜",
    },
    "data_freshness": {
        "level": "ä½Ž",
        "description": "æ•°æ®å¯èƒ½éšæ—¶é—´è¿‡æ—¶",
        "mitigation": "å»ºç«‹æŒç»­æ›´æ–°æœºåˆ¶",
    },
}


class ExecutiveSummaryGenerator:
    """Generate executive summary for decision makers."""

    def __init__(self):
        pass

    def generate(
        self,
        dataset_id: str,
        dataset_type: str,
        sample_count: int,
        reproduction_cost: dict[str, float],
        human_percentage: float,
        complexity_metrics: Optional[Any] = None,
        phased_breakdown: Optional[Any] = None,
        llm_analysis: Optional[Any] = None,
        enhanced_context: Optional[Any] = None,
    ) -> ValueAssessment:
        """Generate value assessment for a dataset.

        Args:
            dataset_id: Dataset identifier
            dataset_type: Type of dataset (preference, evaluation, etc.)
            sample_count: Number of samples
            reproduction_cost: Cost breakdown dict
            human_percentage: Human work percentage
            complexity_metrics: Complexity analysis result
            phased_breakdown: Phased cost breakdown
            llm_analysis: LLM analysis result
            enhanced_context: LLM-enhanced context (optional)

        Returns:
            ValueAssessment object
        """
        assessment = ValueAssessment()

        # Get type config
        config = DATASET_TYPE_CONFIG.get(dataset_type, {})

        # Set use cases - prefer LLM-enhanced if available
        ec = enhanced_context
        if ec and getattr(ec, "generated", False) and ec.tailored_use_cases:
            assessment.primary_use_case = ec.tailored_use_cases[0]
            assessment.secondary_use_cases = ec.tailored_use_cases[1:]
        else:
            assessment.primary_use_case = config.get("primary_use_case", "é€šç”¨æ•°æ®é›†ï¼Œç”¨é€”å¾…å®š")
            assessment.secondary_use_cases = config.get("secondary_use_cases", [])
        assessment.expected_outcomes = config.get("expected_outcomes", [])

        # Calculate value score (1-10)
        score = self._calculate_value_score(
            dataset_type=dataset_type,
            sample_count=sample_count,
            reproduction_cost=reproduction_cost,
            human_percentage=human_percentage,
            complexity_metrics=complexity_metrics,
            config=config,
        )
        assessment.score = score

        # Determine recommendation
        assessment.recommendation, assessment.recommendation_reason = self._get_recommendation(
            score=score,
            reproduction_cost=reproduction_cost,
            dataset_type=dataset_type,
        )

        # Calculate ROI
        assessment.roi_ratio, assessment.roi_explanation = self._calculate_roi(
            dataset_type=dataset_type,
            reproduction_cost=reproduction_cost,
            sample_count=sample_count,
            config=config,
        )

        # Generate payback scenarios - prefer LLM-enhanced
        if ec and getattr(ec, "generated", False) and ec.tailored_roi_scenarios:
            assessment.payback_scenarios = ec.tailored_roi_scenarios
        else:
            assessment.payback_scenarios = self._generate_payback_scenarios(
                dataset_type=dataset_type,
                reproduction_cost=reproduction_cost,
            )

        # Assess risks - prefer LLM-enhanced
        if ec and getattr(ec, "generated", False) and ec.tailored_risks:
            assessment.risks = ec.tailored_risks
        else:
            assessment.risks = self._assess_risks(
                reproduction_cost=reproduction_cost,
                human_percentage=human_percentage,
                complexity_metrics=complexity_metrics,
            )

        # Find alternatives
        assessment.alternatives = self._find_alternatives(dataset_id, dataset_type)
        if ec and getattr(ec, "generated", False) and ec.competitive_positioning:
            assessment.competitive_advantage = ec.competitive_positioning
        else:
            assessment.competitive_advantage = self._get_competitive_advantage(
                dataset_id=dataset_id,
                dataset_type=dataset_type,
                sample_count=sample_count,
            )

        return assessment

    def _calculate_value_score(
        self,
        dataset_type: str,
        sample_count: int,
        reproduction_cost: dict[str, float],
        human_percentage: float,
        complexity_metrics: Optional[Any],
        config: dict,
    ) -> float:
        """Calculate value score (1-10)."""
        score = 5.0  # Base score

        # Type multiplier
        type_multiplier = config.get("value_multiplier", 1.0)
        score *= type_multiplier

        # Size factor (larger is generally better)
        if sample_count >= 10000:
            score += 1.5
        elif sample_count >= 1000:
            score += 1.0
        elif sample_count >= 100:
            score += 0.5
        elif sample_count < 50:
            score -= 1.0

        # Cost efficiency factor
        total_cost = reproduction_cost.get("total", 0)
        cost_per_sample = total_cost / sample_count if sample_count > 0 else 0

        if cost_per_sample < 1:
            score += 1.0  # Very cost efficient
        elif cost_per_sample < 5:
            score += 0.5
        elif cost_per_sample > 50:
            score -= 1.0  # Expensive

        # Complexity factor (moderate complexity is good)
        if complexity_metrics:
            difficulty = getattr(complexity_metrics, "difficulty_score", 2.0)
            if 1.5 <= difficulty <= 3.0:
                score += 0.5  # Good complexity range
            elif difficulty > 4.0:
                score -= 0.5  # Too complex

        # Normalize to 1-10 range
        score = max(1.0, min(10.0, score))

        return round(score, 1)

    def _get_recommendation(
        self,
        score: float,
        reproduction_cost: dict[str, float],
        dataset_type: str,
    ) -> tuple:
        """Determine recommendation based on score and factors."""
        total_cost = reproduction_cost.get("total", 0)

        if score >= 8.0:
            return (
                Recommendation.HIGHLY_RECOMMENDED,
                f"æ•°æ®é›†ä»·å€¼é«˜ (è¯„åˆ† {score}/10)ï¼Œå¼ºçƒˆå»ºè®®å¤åˆ»",
            )
        elif score >= 6.0:
            return (Recommendation.RECOMMENDED, f"æ•°æ®é›†ä»·å€¼è‰¯å¥½ (è¯„åˆ† {score}/10)ï¼Œå»ºè®®å¤åˆ»")
        elif score >= 4.0:
            if total_cost > 10000:
                return (
                    Recommendation.CONDITIONAL,
                    f"æ•°æ®é›†æœ‰ä¸€å®šä»·å€¼ (è¯„åˆ† {score}/10)ï¼Œä½†æˆæœ¬è¾ƒé«˜ï¼Œå»ºè®®è¯„ä¼°é¢„ç®—åŽå†³å®š",
                )
            else:
                return (
                    Recommendation.CONDITIONAL,
                    f"æ•°æ®é›†æœ‰ä¸€å®šä»·å€¼ (è¯„åˆ† {score}/10)ï¼Œå»ºè®®æ ¹æ®å…·ä½“éœ€æ±‚å†³å®š",
                )
        else:
            return (
                Recommendation.NOT_RECOMMENDED,
                f"æ•°æ®é›†ä»·å€¼è¾ƒä½Ž (è¯„åˆ† {score}/10)ï¼Œå»ºè®®å¯»æ‰¾æ›¿ä»£æ–¹æ¡ˆ",
            )

    def _calculate_roi(
        self,
        dataset_type: str,
        reproduction_cost: dict[str, float],
        sample_count: int,
        config: dict,
    ) -> tuple:
        """Calculate ROI ratio and explanation."""
        total_cost = reproduction_cost.get("total", 0)
        if total_cost == 0:
            return (0, "æ— æ³•è®¡ç®— ROIï¼ˆæˆæœ¬ä¸ºé›¶ï¼‰")

        # Estimate value based on type and size
        # These are rough estimates for illustration
        value_multiplier = config.get("value_multiplier", 1.0)

        # Base value per sample (in USD equivalent)
        if dataset_type == "preference":
            base_value_per_sample = 2.0  # High value for alignment
        elif dataset_type == "evaluation":
            base_value_per_sample = 1.5
        elif dataset_type == "sft":
            base_value_per_sample = 1.0
        elif dataset_type == "swe_bench":
            base_value_per_sample = 3.0  # High value for code
        else:
            base_value_per_sample = 0.5

        estimated_value = sample_count * base_value_per_sample * value_multiplier
        roi_ratio = estimated_value / total_cost

        if roi_ratio >= 3.0:
            explanation = f"é¢„è®¡æŠ•èµ„å›žæŠ¥çŽ‡ {roi_ratio:.1f}xï¼ŒæŠ•èµ„ä»·å€¼é«˜"
        elif roi_ratio >= 1.5:
            explanation = f"é¢„è®¡æŠ•èµ„å›žæŠ¥çŽ‡ {roi_ratio:.1f}xï¼ŒæŠ•èµ„ä»·å€¼è‰¯å¥½"
        elif roi_ratio >= 1.0:
            explanation = f"é¢„è®¡æŠ•èµ„å›žæŠ¥çŽ‡ {roi_ratio:.1f}xï¼ŒæŽ¥è¿‘æ”¶æ”¯å¹³è¡¡"
        else:
            explanation = f"é¢„è®¡æŠ•èµ„å›žæŠ¥çŽ‡ {roi_ratio:.1f}xï¼Œéœ€è¦è°¨æ…Žè¯„ä¼°"

        return (round(roi_ratio, 2), explanation)

    def _generate_payback_scenarios(
        self,
        dataset_type: str,
        reproduction_cost: dict[str, float],
    ) -> list[str]:
        """Generate payback scenarios."""
        reproduction_cost.get("total", 0)

        scenarios = []

        if dataset_type == "preference":
            scenarios = [
                "åœºæ™¯ A: ç”¨äºŽå†…éƒ¨æ¨¡åž‹å¯¹é½ï¼Œé¿å… 1 æ¬¡é‡å¤§ PR å±æœºå³å¯æ”¶å›žæˆæœ¬",
                "åœºæ™¯ B: æå‡ç”¨æˆ·æ»¡æ„åº¦ 5%ï¼ŒæŒ‰ç”¨æˆ·ä»·å€¼è®¡ç®—å¯åœ¨ 3 ä¸ªæœˆå†…å›žæœ¬",
                "åœºæ™¯ C: ä½œä¸ºæ•°æ®èµ„äº§å‡ºå”®/æŽˆæƒç»™å…¶ä»–å›¢é˜Ÿ",
            ]
        elif dataset_type == "evaluation":
            scenarios = [
                "åœºæ™¯ A: é¿å…é€‰æ‹©é”™è¯¯æ¨¡åž‹å¯¼è‡´çš„è¿”å·¥æˆæœ¬",
                "åœºæ™¯ B: ç¼©çŸ­æ¨¡åž‹é€‰åž‹å‘¨æœŸï¼ŒèŠ‚çœå›¢é˜Ÿæ—¶é—´",
                "åœºæ™¯ C: ä½œä¸ºæ ‡å‡†åŒ–è¯„æµ‹åŸºå‡†æŒç»­å¤ç”¨",
            ]
        elif dataset_type == "sft":
            scenarios = [
                "åœºæ™¯ A: å¾®è°ƒåŽæ¨¡åž‹æ€§èƒ½æå‡å¸¦æ¥çš„ä¸šåŠ¡ä»·å€¼",
                "åœºæ™¯ B: å‡å°‘å¯¹æ˜‚è´µ API çš„ä¾èµ–",
                "åœºæ™¯ C: æž„å»ºå·®å¼‚åŒ–èƒ½åŠ›å½¢æˆç«žäº‰å£åž’",
            ]
        elif dataset_type == "swe_bench":
            scenarios = [
                "åœºæ™¯ A: è¯„ä¼°ä»£ç åŠ©æ‰‹æŠ•èµ„å†³ç­–",
                "åœºæ™¯ B: æå‡å¼€å‘æ•ˆçŽ‡å¸¦æ¥çš„äººåŠ›æˆæœ¬èŠ‚çœ",
                "åœºæ™¯ C: æŠ€æœ¯é¢†å…ˆå¸¦æ¥çš„å“ç‰Œä»·å€¼",
            ]
        else:
            scenarios = [
                "åœºæ™¯ A: ç›´æŽ¥ä¸šåŠ¡åº”ç”¨ä»·å€¼",
                "åœºæ™¯ B: ç ”ç©¶å’ŒæŠ€æœ¯å‚¨å¤‡ä»·å€¼",
                "åœºæ™¯ C: æ•°æ®èµ„äº§é•¿æœŸä»·å€¼",
            ]

        return scenarios

    def _assess_risks(
        self,
        reproduction_cost: dict[str, float],
        human_percentage: float,
        complexity_metrics: Optional[Any],
    ) -> list[dict[str, str]]:
        """Assess project risks."""
        risks = []

        total_cost = reproduction_cost.get("total", 0)

        # Cost risk
        if total_cost > 50000:
            risks.append(RISK_TEMPLATES["high_cost"])

        # Expert requirement risk
        if complexity_metrics:
            domain = getattr(complexity_metrics, "primary_domain", None)
            if domain:
                domain_value = domain.value if hasattr(domain, "value") else str(domain)
                if domain_value in ["medical", "legal", "finance"]:
                    risks.append(RISK_TEMPLATES["expert_required"])

        # Human-heavy risk
        if human_percentage > 85:
            risks.append(RISK_TEMPLATES["quality_variance"])

        # Time risk (estimate based on cost)
        if total_cost > 20000:
            risks.append(RISK_TEMPLATES["time_intensive"])

        # Always include data freshness as low risk
        risks.append(RISK_TEMPLATES["data_freshness"])

        return risks

    def _find_alternatives(self, dataset_id: str, dataset_type: str) -> list[str]:
        """Find alternative datasets."""
        # Try to get from knowledge base
        try:
            from datarecipe.knowledge import KnowledgeBase

            kb = KnowledgeBase()
            similar = kb.find_similar_datasets(dataset_type, limit=5)
            return [s.dataset_id for s in similar if s.dataset_id != dataset_id][:3]
        except (ImportError, AttributeError, TypeError):
            pass

        # Fallback to known alternatives
        alternatives_map = {
            "preference": [
                "Anthropic/hh-rlhf",
                "OpenAI/summarize_from_feedback",
                "stanfordnlp/SHP",
            ],
            "evaluation": [
                "MMLU",
                "HellaSwag",
                "TruthfulQA",
            ],
            "sft": [
                "OpenAssistant/oasst1",
                "databricks/dolly-15k",
                "tatsu-lab/alpaca",
            ],
            "swe_bench": [
                "princeton-nlp/SWE-bench",
                "bigcode/the-stack",
                "codeparrot/github-code",
            ],
        }

        alts = alternatives_map.get(dataset_type, [])
        return [a for a in alts if a.lower() != dataset_id.lower()][:3]

    def _get_competitive_advantage(
        self,
        dataset_id: str,
        dataset_type: str,
        sample_count: int,
    ) -> str:
        """Describe competitive advantage of this dataset."""
        org = dataset_id.split("/")[0] if "/" in dataset_id else ""

        advantages = []

        # Org reputation
        top_orgs = ["anthropic", "openai", "google", "meta", "microsoft", "tencent", "alibaba"]
        if org.lower() in top_orgs:
            advantages.append(f"æ¥è‡ªçŸ¥åæœºæž„ ({org})ï¼Œæ•°æ®è´¨é‡æœ‰ä¿éšœ")

        # Size advantage
        if sample_count >= 10000:
            advantages.append("æ•°æ®è§„æ¨¡å¤§ï¼Œè¦†ç›–é¢å¹¿")
        elif sample_count >= 1000:
            advantages.append("æ•°æ®è§„æ¨¡é€‚ä¸­ï¼Œé€‚åˆå¾®è°ƒ")

        # Type-specific advantages
        if dataset_type == "preference":
            advantages.append("æ”¯æŒ RLHF/DPO è®­ç»ƒï¼Œç¬¦åˆå½“å‰å¯¹é½ç ”ç©¶è¶‹åŠ¿")
        elif dataset_type == "swe_bench":
            advantages.append("é¢å‘ä»£ç èƒ½åŠ›è¯„æµ‹ï¼Œå¸‚åœºä¸Šç¨€ç¼º")

        return "ï¼›".join(advantages) if advantages else "æ ‡å‡†æ•°æ®é›†ï¼Œæ— ç‰¹æ®Šä¼˜åŠ¿"

    def to_markdown(
        self,
        assessment: ValueAssessment,
        dataset_id: str,
        dataset_type: str,
        reproduction_cost: dict[str, float],
        phased_breakdown: Optional[Any] = None,
    ) -> str:
        """Generate executive summary markdown."""
        lines = []

        # Header
        lines.append(f"# {dataset_id} æ‰§è¡Œæ‘˜è¦")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> æ•°æ®é›†ç±»åž‹: {dataset_type}")
        lines.append("")

        # Decision box
        lines.append("---")
        lines.append("")

        # Recommendation with visual indicator
        rec = assessment.recommendation
        if rec == Recommendation.HIGHLY_RECOMMENDED:
            rec_icon = "ðŸŸ¢"
            rec_text = "å¼ºçƒˆæŽ¨è"
        elif rec == Recommendation.RECOMMENDED:
            rec_icon = "ðŸŸ¢"
            rec_text = "æŽ¨è"
        elif rec == Recommendation.CONDITIONAL:
            rec_icon = "ðŸŸ¡"
            rec_text = "æœ‰æ¡ä»¶æŽ¨è"
        else:
            rec_icon = "ðŸ”´"
            rec_text = "ä¸æŽ¨è"

        lines.append(f"## {rec_icon} å†³ç­–å»ºè®®: {rec_text}")
        lines.append("")
        lines.append(f"**è¯„åˆ†**: {assessment.score}/10")
        lines.append("")
        lines.append(f"**ç†ç”±**: {assessment.recommendation_reason}")
        lines.append("")

        # Quick stats
        lines.append("### å…³é”®æŒ‡æ ‡")
        lines.append("")
        total_cost = reproduction_cost.get("total", 0)
        human_cost = reproduction_cost.get("human", 0)
        lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
        lines.append("|------|------|")
        lines.append(f"| æ€»æˆæœ¬ | ${total_cost:,.0f} |")
        lines.append(
            f"| äººå·¥æˆæœ¬ | ${human_cost:,.0f} ({human_cost / total_cost * 100:.0f}%) |"
            if total_cost > 0
            else f"| äººå·¥æˆæœ¬ | ${human_cost:,.0f} |"
        )
        lines.append(f"| æŠ•èµ„å›žæŠ¥çŽ‡ | {assessment.roi_ratio:.1f}x |")
        lines.append("")

        # Use cases
        lines.append("---")
        lines.append("")
        lines.append("## ç”¨é€”ä¸Žä»·å€¼")
        lines.append("")
        lines.append("### ä¸»è¦ç”¨é€”")
        lines.append(f"**{assessment.primary_use_case}**")
        lines.append("")

        if assessment.secondary_use_cases:
            lines.append("### å…¶ä»–ç”¨é€”")
            for use_case in assessment.secondary_use_cases:
                lines.append(f"- {use_case}")
            lines.append("")

        if assessment.expected_outcomes:
            lines.append("### é¢„æœŸæˆæžœ")
            for outcome in assessment.expected_outcomes:
                lines.append(f"- {outcome}")
            lines.append("")

        # ROI Analysis
        lines.append("---")
        lines.append("")
        lines.append("## ROI åˆ†æž")
        lines.append("")
        lines.append(f"**{assessment.roi_explanation}**")
        lines.append("")

        if assessment.payback_scenarios:
            lines.append("### å›žæŠ¥åœºæ™¯")
            for scenario in assessment.payback_scenarios:
                lines.append(f"- {scenario}")
            lines.append("")

        # Risks
        lines.append("---")
        lines.append("")
        lines.append("## é£Žé™©è¯„ä¼°")
        lines.append("")

        if assessment.risks:
            lines.append("| é£Žé™©ç­‰çº§ | æè¿° | ç¼“è§£æŽªæ–½ |")
            lines.append("|----------|------|----------|")
            for risk in assessment.risks:
                level = risk.get("level", "ä¸­")
                desc = risk.get("description", "")
                mitigation = risk.get("mitigation", "")
                lines.append(f"| {level} | {desc} | {mitigation} |")
            lines.append("")

        # Alternatives
        lines.append("---")
        lines.append("")
        lines.append("## æ›¿ä»£æ–¹æ¡ˆ")
        lines.append("")

        if assessment.alternatives:
            lines.append("å¯è€ƒè™‘çš„æ›¿ä»£æ•°æ®é›†:")
            for alt in assessment.alternatives:
                lines.append(f"- {alt}")
            lines.append("")
        else:
            lines.append("æš‚æ— å·²çŸ¥æ›¿ä»£æ–¹æ¡ˆã€‚")
            lines.append("")

        if assessment.competitive_advantage:
            lines.append(f"**æœ¬æ•°æ®é›†ä¼˜åŠ¿**: {assessment.competitive_advantage}")
            lines.append("")

        # Footer
        lines.append("---")
        lines.append("")
        lines.append("> æœ¬æ‘˜è¦ç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆï¼Œä¾›å†³ç­–å‚è€ƒ")

        return "\n".join(lines)

    def to_dict(self, assessment: ValueAssessment) -> dict:
        """Convert assessment to dictionary."""
        return {
            "score": assessment.score,
            "recommendation": assessment.recommendation.value,
            "recommendation_reason": assessment.recommendation_reason,
            "primary_use_case": assessment.primary_use_case,
            "secondary_use_cases": assessment.secondary_use_cases,
            "expected_outcomes": assessment.expected_outcomes,
            "roi_ratio": assessment.roi_ratio,
            "roi_explanation": assessment.roi_explanation,
            "payback_scenarios": assessment.payback_scenarios,
            "risks": assessment.risks,
            "alternatives": assessment.alternatives,
            "competitive_advantage": assessment.competitive_advantage,
        }
