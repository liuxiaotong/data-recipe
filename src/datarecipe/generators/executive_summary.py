"""Executive summary generator for decision makers.

Generates a 1-page executive summary with:
- Recommendation (go/no-go)
- Value assessment (1-10 score)
- Use cases and expected outcomes
- ROI analysis
- Key risks
"""

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any


class Recommendation(Enum):
    """Recommendation levels."""

    HIGHLY_RECOMMENDED = "highly_recommended"
    RECOMMENDED = "recommended"
    CONDITIONAL = "conditional"
    NOT_RECOMMENDED = "not_recommended"


@dataclass
class ValueAssessment:
    """Value assessment for a dataset."""

    # Overall score (1-10)
    score: float = 5.0

    # Recommendation
    recommendation: Recommendation = Recommendation.CONDITIONAL
    recommendation_reason: str = ""

    # Use cases
    primary_use_case: str = ""
    secondary_use_cases: list[str] = field(default_factory=list)

    # Expected outcomes
    expected_outcomes: list[str] = field(default_factory=list)

    # ROI analysis
    roi_ratio: float = 1.0  # Expected value / Cost
    roi_explanation: str = ""
    payback_scenarios: list[str] = field(default_factory=list)

    # Risks
    risks: list[dict[str, str]] = field(default_factory=list)  # [{level, description, mitigation}]

    # Competitive analysis
    alternatives: list[str] = field(default_factory=list)
    competitive_advantage: str = ""


# Dataset type configurations
DATASET_TYPE_CONFIG = {
    "preference": {
        "primary_use_case": "ËÆ≠ÁªÉÂ•ñÂä±Ê®°Âûã (Reward Model) ÊàñÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ (DPO)",
        "secondary_use_cases": [
            "RLHF ËÆ≠ÁªÉÊµÅÁ®ã",
            "Ê®°ÂûãÂØπÈΩê (Alignment)",
            "ÂìçÂ∫îË¥®ÈáèËØÑ‰º∞",
        ],
        "expected_outcomes": [
            "ÊèêÂçáÊ®°ÂûãÂìçÂ∫îË¥®ÈáèÂíåÁî®Êà∑Êª°ÊÑèÂ∫¶",
            "ÂáèÂ∞ëÊúâÂÆ≥/‰∏çÂΩìËæìÂá∫",
            "Â¢ûÂº∫Ê®°ÂûãÈÅµÂæ™‰∫∫Á±ªÂÅèÂ•ΩÁöÑËÉΩÂäõ",
        ],
        "value_multiplier": 1.5,  # High value for alignment
    },
    "evaluation": {
        "primary_use_case": "Ê®°ÂûãËÉΩÂäõËØÑÊµãÂíåÂü∫ÂáÜÊµãËØï",
        "secondary_use_cases": [
            "Ê®°ÂûãÈÄâÂûã‰æùÊçÆ",
            "ËÆ≠ÁªÉÊïàÊûúÈ™åËØÅ",
            "ËÉΩÂäõÂ∑ÆË∑ùÂàÜÊûê",
        ],
        "expected_outcomes": [
            "ÈáèÂåñÊ®°ÂûãÂú®ÁâπÂÆö‰ªªÂä°‰∏äÁöÑË°®Áé∞",
            "ËØÜÂà´Ê®°ÂûãËÉΩÂäõÁü≠Êùø",
            "ÊîØÊåÅÊ®°ÂûãËø≠‰ª£ÂÜ≥Á≠ñ",
        ],
        "value_multiplier": 1.2,
    },
    "sft": {
        "primary_use_case": "ÁõëÁù£ÂæÆË∞É (Supervised Fine-Tuning)",
        "secondary_use_cases": [
            "È¢ÜÂüüÈÄÇÈÖç",
            "Êåá‰ª§ÈÅµÂæ™ËÉΩÂäõÊèêÂçá",
            "ÁâπÂÆö‰ªªÂä°‰ºòÂåñ",
        ],
        "expected_outcomes": [
            "ÊèêÂçáÊ®°ÂûãÂú®ÁõÆÊ†áÈ¢ÜÂüüÁöÑË°®Áé∞",
            "Â¢ûÂº∫Êåá‰ª§ÁêÜËß£ÂíåÊâßË°åËÉΩÂäõ",
            "ÂÆöÂà∂ÂåñÊ®°ÂûãË°å‰∏∫",
        ],
        "value_multiplier": 1.3,
    },
    "swe_bench": {
        "primary_use_case": "‰ª£Á†ÅÁîüÊàêÂíåËΩØ‰ª∂Â∑•Á®ãËÉΩÂäõËØÑÊµã",
        "secondary_use_cases": [
            "‰ª£Á†ÅÂä©ÊâãÊ®°ÂûãËØÑ‰º∞",
            "Ëá™Âä®Âåñ‰ª£Á†Å‰øÆÂ§çÁ†îÁ©∂",
            "ËΩØ‰ª∂Â∑•Á®ã AI Âü∫ÂáÜ",
        ],
        "expected_outcomes": [
            "ËØÑ‰º∞Ê®°Âûã‰ª£Á†ÅÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõ",
            "ËØÜÂà´‰ª£Á†Å‰ªªÂä°‰∏≠ÁöÑËÉΩÂäõÂ∑ÆË∑ù",
            "ÊîØÊåÅ‰ª£Á†ÅÊ®°ÂûãÈÄâÂûã",
        ],
        "value_multiplier": 1.4,
    },
}

# Risk templates by factor
RISK_TEMPLATES = {
    "high_cost": {
        "level": "È´ò",
        "description": "Â§çÂàªÊàêÊú¨ËæÉÈ´òÔºåÈúÄË¶ÅÂÖÖË∂≥È¢ÑÁÆó",
        "mitigation": "ÂàÜÈò∂ÊÆµÂÆûÊñΩÔºå‰ºòÂÖàÂÆåÊàêÊ†∏ÂøÉÂ≠êÈõÜ",
    },
    "expert_required": {
        "level": "‰∏≠",
        "description": "ÈúÄË¶ÅÈ¢ÜÂüü‰∏ìÂÆ∂ÂèÇ‰∏éÔºå‰∫∫ÊâçËé∑ÂèñÂèØËÉΩÂõ∞Èöæ",
        "mitigation": "ÊèêÂâçÂÇ®Â§á‰∫∫ÊâçÔºåÊàñËÄÉËôëÂ§ñÂåÖÂêà‰Ωú",
    },
    "quality_variance": {
        "level": "‰∏≠",
        "description": "Ê†áÊ≥®Ë¥®ÈáèÂèØËÉΩÂ≠òÂú®Ê≥¢Âä®",
        "mitigation": "Âª∫Á´ã‰∏•Ê†º QA ÊµÅÁ®ãÔºåËÆæÁΩÆË¥®ÈáèÈó®Êßõ",
    },
    "time_intensive": {
        "level": "‰∏≠",
        "description": "È°πÁõÆÂë®ÊúüËæÉÈïø",
        "mitigation": "ÂêàÁêÜËßÑÂàíÈáåÁ®ãÁ¢ëÔºåËÆæÁΩÆÈò∂ÊÆµÊÄß‰∫§‰ªò",
    },
    "data_freshness": {
        "level": "‰Ωé",
        "description": "Êï∞ÊçÆÂèØËÉΩÈöèÊó∂Èó¥ËøáÊó∂",
        "mitigation": "Âª∫Á´ãÊåÅÁª≠Êõ¥Êñ∞Êú∫Âà∂",
    },
}


class ExecutiveSummaryGenerator:
    """Generate executive summary for decision makers."""

    def __init__(self):
        pass

    def generate(
        self,
        dataset_id: str,
        dataset_type: str,
        sample_count: int,
        reproduction_cost: dict[str, float],
        human_percentage: float,
        complexity_metrics: Any | None = None,
        phased_breakdown: Any | None = None,
        llm_analysis: Any | None = None,
        enhanced_context: Any | None = None,
    ) -> ValueAssessment:
        """Generate value assessment for a dataset.

        Args:
            dataset_id: Dataset identifier
            dataset_type: Type of dataset (preference, evaluation, etc.)
            sample_count: Number of samples
            reproduction_cost: Cost breakdown dict
            human_percentage: Human work percentage
            complexity_metrics: Complexity analysis result
            phased_breakdown: Phased cost breakdown
            llm_analysis: LLM analysis result
            enhanced_context: LLM-enhanced context (optional)

        Returns:
            ValueAssessment object
        """
        assessment = ValueAssessment()

        # Get type config
        config = DATASET_TYPE_CONFIG.get(dataset_type, {})

        # Set use cases - prefer LLM-enhanced if available
        ec = enhanced_context
        if ec and getattr(ec, "generated", False) and ec.tailored_use_cases:
            assessment.primary_use_case = ec.tailored_use_cases[0]
            assessment.secondary_use_cases = ec.tailored_use_cases[1:]
        else:
            assessment.primary_use_case = config.get("primary_use_case", "ÈÄöÁî®Êï∞ÊçÆÈõÜÔºåÁî®ÈÄîÂæÖÂÆö")
            assessment.secondary_use_cases = config.get("secondary_use_cases", [])
        assessment.expected_outcomes = config.get("expected_outcomes", [])

        # Calculate value score (1-10)
        score = self._calculate_value_score(
            dataset_type=dataset_type,
            sample_count=sample_count,
            reproduction_cost=reproduction_cost,
            human_percentage=human_percentage,
            complexity_metrics=complexity_metrics,
            config=config,
        )
        assessment.score = score

        # Determine recommendation
        assessment.recommendation, assessment.recommendation_reason = self._get_recommendation(
            score=score,
            reproduction_cost=reproduction_cost,
            dataset_type=dataset_type,
        )

        # Calculate ROI
        assessment.roi_ratio, assessment.roi_explanation = self._calculate_roi(
            dataset_type=dataset_type,
            reproduction_cost=reproduction_cost,
            sample_count=sample_count,
            config=config,
        )

        # Generate payback scenarios - prefer LLM-enhanced
        if ec and getattr(ec, "generated", False) and ec.tailored_roi_scenarios:
            assessment.payback_scenarios = ec.tailored_roi_scenarios
        else:
            assessment.payback_scenarios = self._generate_payback_scenarios(
                dataset_type=dataset_type,
                reproduction_cost=reproduction_cost,
            )

        # Assess risks - prefer LLM-enhanced
        if ec and getattr(ec, "generated", False) and ec.tailored_risks:
            assessment.risks = ec.tailored_risks
        else:
            assessment.risks = self._assess_risks(
                reproduction_cost=reproduction_cost,
                human_percentage=human_percentage,
                complexity_metrics=complexity_metrics,
            )

        # Find alternatives
        assessment.alternatives = self._find_alternatives(dataset_id, dataset_type)
        if ec and getattr(ec, "generated", False) and ec.competitive_positioning:
            assessment.competitive_advantage = ec.competitive_positioning
        else:
            assessment.competitive_advantage = self._get_competitive_advantage(
                dataset_id=dataset_id,
                dataset_type=dataset_type,
                sample_count=sample_count,
            )

        return assessment

    def _calculate_value_score(
        self,
        dataset_type: str,
        sample_count: int,
        reproduction_cost: dict[str, float],
        human_percentage: float,
        complexity_metrics: Any | None,
        config: dict,
    ) -> float:
        """Calculate value score (1-10)."""
        score = 5.0  # Base score

        # Type multiplier
        type_multiplier = config.get("value_multiplier", 1.0)
        score *= type_multiplier

        # Size factor (larger is generally better)
        if sample_count >= 10000:
            score += 1.5
        elif sample_count >= 1000:
            score += 1.0
        elif sample_count >= 100:
            score += 0.5
        elif sample_count < 50:
            score -= 1.0

        # Cost efficiency factor
        total_cost = reproduction_cost.get("total", 0)
        cost_per_sample = total_cost / sample_count if sample_count > 0 else 0

        if cost_per_sample < 1:
            score += 1.0  # Very cost efficient
        elif cost_per_sample < 5:
            score += 0.5
        elif cost_per_sample > 50:
            score -= 1.0  # Expensive

        # Complexity factor (moderate complexity is good)
        if complexity_metrics:
            difficulty = getattr(complexity_metrics, "difficulty_score", 2.0)
            if 1.5 <= difficulty <= 3.0:
                score += 0.5  # Good complexity range
            elif difficulty > 4.0:
                score -= 0.5  # Too complex

        # Normalize to 1-10 range
        score = max(1.0, min(10.0, score))

        return round(score, 1)

    def _get_recommendation(
        self,
        score: float,
        reproduction_cost: dict[str, float],
        dataset_type: str,
    ) -> tuple[Recommendation, str]:
        """Determine recommendation based on score and factors."""
        total_cost = reproduction_cost.get("total", 0)

        if score >= 8.0:
            return (
                Recommendation.HIGHLY_RECOMMENDED,
                f"Êï∞ÊçÆÈõÜ‰ª∑ÂÄºÈ´ò (ËØÑÂàÜ {score}/10)ÔºåÂº∫ÁÉàÂª∫ËÆÆÂ§çÂàª",
            )
        elif score >= 6.0:
            return (Recommendation.RECOMMENDED, f"Êï∞ÊçÆÈõÜ‰ª∑ÂÄºËâØÂ•Ω (ËØÑÂàÜ {score}/10)ÔºåÂª∫ËÆÆÂ§çÂàª")
        elif score >= 4.0:
            if total_cost > 10000:
                return (
                    Recommendation.CONDITIONAL,
                    f"Êï∞ÊçÆÈõÜÊúâ‰∏ÄÂÆö‰ª∑ÂÄº (ËØÑÂàÜ {score}/10)Ôºå‰ΩÜÊàêÊú¨ËæÉÈ´òÔºåÂª∫ËÆÆËØÑ‰º∞È¢ÑÁÆóÂêéÂÜ≥ÂÆö",
                )
            else:
                return (
                    Recommendation.CONDITIONAL,
                    f"Êï∞ÊçÆÈõÜÊúâ‰∏ÄÂÆö‰ª∑ÂÄº (ËØÑÂàÜ {score}/10)ÔºåÂª∫ËÆÆÊ†πÊçÆÂÖ∑‰ΩìÈúÄÊ±ÇÂÜ≥ÂÆö",
                )
        else:
            return (
                Recommendation.NOT_RECOMMENDED,
                f"Êï∞ÊçÆÈõÜ‰ª∑ÂÄºËæÉ‰Ωé (ËØÑÂàÜ {score}/10)ÔºåÂª∫ËÆÆÂØªÊâæÊõø‰ª£ÊñπÊ°à",
            )

    def _calculate_roi(
        self,
        dataset_type: str,
        reproduction_cost: dict[str, float],
        sample_count: int,
        config: dict,
    ) -> tuple[float, str]:
        """Calculate ROI ratio and explanation."""
        total_cost = reproduction_cost.get("total", 0)
        if total_cost == 0:
            return (0, "Êó†Ê≥ïËÆ°ÁÆó ROIÔºàÊàêÊú¨‰∏∫Èõ∂Ôºâ")

        # Estimate value based on type and size
        # These are rough estimates for illustration
        value_multiplier = config.get("value_multiplier", 1.0)

        # Base value per sample (in USD equivalent)
        if dataset_type == "preference":
            base_value_per_sample = 2.0  # High value for alignment
        elif dataset_type == "evaluation":
            base_value_per_sample = 1.5
        elif dataset_type == "sft":
            base_value_per_sample = 1.0
        elif dataset_type == "swe_bench":
            base_value_per_sample = 3.0  # High value for code
        else:
            base_value_per_sample = 0.5

        estimated_value = sample_count * base_value_per_sample * value_multiplier
        roi_ratio = estimated_value / total_cost

        if roi_ratio >= 3.0:
            explanation = f"È¢ÑËÆ°ÊäïËµÑÂõûÊä•Áéá {roi_ratio:.1f}xÔºåÊäïËµÑ‰ª∑ÂÄºÈ´ò"
        elif roi_ratio >= 1.5:
            explanation = f"È¢ÑËÆ°ÊäïËµÑÂõûÊä•Áéá {roi_ratio:.1f}xÔºåÊäïËµÑ‰ª∑ÂÄºËâØÂ•Ω"
        elif roi_ratio >= 1.0:
            explanation = f"È¢ÑËÆ°ÊäïËµÑÂõûÊä•Áéá {roi_ratio:.1f}xÔºåÊé•ËøëÊî∂ÊîØÂπ≥Ë°°"
        else:
            explanation = f"È¢ÑËÆ°ÊäïËµÑÂõûÊä•Áéá {roi_ratio:.1f}xÔºåÈúÄË¶ÅË∞®ÊÖéËØÑ‰º∞"

        return (round(roi_ratio, 2), explanation)

    def _generate_payback_scenarios(
        self,
        dataset_type: str,
        reproduction_cost: dict[str, float],
    ) -> list[str]:
        """Generate payback scenarios."""
        reproduction_cost.get("total", 0)

        scenarios = []

        if dataset_type == "preference":
            scenarios = [
                "Âú∫ÊôØ A: Áî®‰∫éÂÜÖÈÉ®Ê®°ÂûãÂØπÈΩêÔºåÈÅøÂÖç 1 Ê¨°ÈáçÂ§ß PR Âç±Êú∫Âç≥ÂèØÊî∂ÂõûÊàêÊú¨",
                "Âú∫ÊôØ B: ÊèêÂçáÁî®Êà∑Êª°ÊÑèÂ∫¶ 5%ÔºåÊåâÁî®Êà∑‰ª∑ÂÄºËÆ°ÁÆóÂèØÂú® 3 ‰∏™ÊúàÂÜÖÂõûÊú¨",
                "Âú∫ÊôØ C: ‰Ωú‰∏∫Êï∞ÊçÆËµÑ‰∫ßÂá∫ÂîÆ/ÊéàÊùÉÁªôÂÖ∂‰ªñÂõ¢Èòü",
            ]
        elif dataset_type == "evaluation":
            scenarios = [
                "Âú∫ÊôØ A: ÈÅøÂÖçÈÄâÊã©ÈîôËØØÊ®°ÂûãÂØºËá¥ÁöÑËøîÂ∑•ÊàêÊú¨",
                "Âú∫ÊôØ B: Áº©Áü≠Ê®°ÂûãÈÄâÂûãÂë®ÊúüÔºåËäÇÁúÅÂõ¢ÈòüÊó∂Èó¥",
                "Âú∫ÊôØ C: ‰Ωú‰∏∫Ê†áÂáÜÂåñËØÑÊµãÂü∫ÂáÜÊåÅÁª≠Â§çÁî®",
            ]
        elif dataset_type == "sft":
            scenarios = [
                "Âú∫ÊôØ A: ÂæÆË∞ÉÂêéÊ®°ÂûãÊÄßËÉΩÊèêÂçáÂ∏¶Êù•ÁöÑ‰∏öÂä°‰ª∑ÂÄº",
                "Âú∫ÊôØ B: ÂáèÂ∞ëÂØπÊòÇË¥µ API ÁöÑ‰æùËµñ",
                "Âú∫ÊôØ C: ÊûÑÂª∫Â∑ÆÂºÇÂåñËÉΩÂäõÂΩ¢ÊàêÁ´û‰∫âÂ£ÅÂûí",
            ]
        elif dataset_type == "swe_bench":
            scenarios = [
                "Âú∫ÊôØ A: ËØÑ‰º∞‰ª£Á†ÅÂä©ÊâãÊäïËµÑÂÜ≥Á≠ñ",
                "Âú∫ÊôØ B: ÊèêÂçáÂºÄÂèëÊïàÁéáÂ∏¶Êù•ÁöÑ‰∫∫ÂäõÊàêÊú¨ËäÇÁúÅ",
                "Âú∫ÊôØ C: ÊäÄÊúØÈ¢ÜÂÖàÂ∏¶Êù•ÁöÑÂìÅÁâå‰ª∑ÂÄº",
            ]
        else:
            scenarios = [
                "Âú∫ÊôØ A: Áõ¥Êé•‰∏öÂä°Â∫îÁî®‰ª∑ÂÄº",
                "Âú∫ÊôØ B: Á†îÁ©∂ÂíåÊäÄÊúØÂÇ®Â§á‰ª∑ÂÄº",
                "Âú∫ÊôØ C: Êï∞ÊçÆËµÑ‰∫ßÈïøÊúü‰ª∑ÂÄº",
            ]

        return scenarios

    def _assess_risks(
        self,
        reproduction_cost: dict[str, float],
        human_percentage: float,
        complexity_metrics: Any | None,
    ) -> list[dict[str, str]]:
        """Assess project risks."""
        risks = []

        total_cost = reproduction_cost.get("total", 0)

        # Cost risk
        if total_cost > 50000:
            risks.append(RISK_TEMPLATES["high_cost"])

        # Expert requirement risk
        if complexity_metrics:
            domain = getattr(complexity_metrics, "primary_domain", None)
            if domain:
                domain_value = domain.value if hasattr(domain, "value") else str(domain)
                if domain_value in ["medical", "legal", "finance"]:
                    risks.append(RISK_TEMPLATES["expert_required"])

        # Human-heavy risk
        if human_percentage > 85:
            risks.append(RISK_TEMPLATES["quality_variance"])

        # Time risk (estimate based on cost)
        if total_cost > 20000:
            risks.append(RISK_TEMPLATES["time_intensive"])

        # Always include data freshness as low risk
        risks.append(RISK_TEMPLATES["data_freshness"])

        return risks

    def _find_alternatives(self, dataset_id: str, dataset_type: str) -> list[str]:
        """Find alternative datasets."""
        # Try to get from knowledge base
        try:
            from datarecipe.knowledge import KnowledgeBase

            kb = KnowledgeBase()
            similar = kb.find_similar_datasets(dataset_type, limit=5)
            return [s.dataset_id for s in similar if s.dataset_id != dataset_id][:3]
        except (ImportError, AttributeError, TypeError):
            pass

        # Fallback to known alternatives
        alternatives_map = {
            "preference": [
                "Anthropic/hh-rlhf",
                "OpenAI/summarize_from_feedback",
                "stanfordnlp/SHP",
            ],
            "evaluation": [
                "MMLU",
                "HellaSwag",
                "TruthfulQA",
            ],
            "sft": [
                "OpenAssistant/oasst1",
                "databricks/dolly-15k",
                "tatsu-lab/alpaca",
            ],
            "swe_bench": [
                "princeton-nlp/SWE-bench",
                "bigcode/the-stack",
                "codeparrot/github-code",
            ],
        }

        alts = alternatives_map.get(dataset_type, [])
        return [a for a in alts if a.lower() != dataset_id.lower()][:3]

    def _get_competitive_advantage(
        self,
        dataset_id: str,
        dataset_type: str,
        sample_count: int,
    ) -> str:
        """Describe competitive advantage of this dataset."""
        org = dataset_id.split("/")[0] if "/" in dataset_id else ""

        advantages = []

        # Org reputation
        top_orgs = ["anthropic", "openai", "google", "meta", "microsoft", "tencent", "alibaba"]
        if org.lower() in top_orgs:
            advantages.append(f"Êù•Ëá™Áü•ÂêçÊú∫ÊûÑ ({org})ÔºåÊï∞ÊçÆË¥®ÈáèÊúâ‰øùÈöú")

        # Size advantage
        if sample_count >= 10000:
            advantages.append("Êï∞ÊçÆËßÑÊ®°Â§ßÔºåË¶ÜÁõñÈù¢Âπø")
        elif sample_count >= 1000:
            advantages.append("Êï∞ÊçÆËßÑÊ®°ÈÄÇ‰∏≠ÔºåÈÄÇÂêàÂæÆË∞É")

        # Type-specific advantages
        if dataset_type == "preference":
            advantages.append("ÊîØÊåÅ RLHF/DPO ËÆ≠ÁªÉÔºåÁ¨¶ÂêàÂΩìÂâçÂØπÈΩêÁ†îÁ©∂Ë∂ãÂäø")
        elif dataset_type == "swe_bench":
            advantages.append("Èù¢Âêë‰ª£Á†ÅËÉΩÂäõËØÑÊµãÔºåÂ∏ÇÂú∫‰∏äÁ®ÄÁº∫")

        return "Ôºõ".join(advantages) if advantages else "Ê†áÂáÜÊï∞ÊçÆÈõÜÔºåÊó†ÁâπÊÆä‰ºòÂäø"

    def to_markdown(
        self,
        assessment: ValueAssessment,
        dataset_id: str,
        dataset_type: str,
        reproduction_cost: dict[str, float],
        phased_breakdown: Any | None = None,
    ) -> str:
        """Generate executive summary markdown."""
        lines = []

        # Header
        lines.append(f"# {dataset_id} ÊâßË°åÊëòË¶Å")
        lines.append("")
        lines.append(f"> ÁîüÊàêÊó∂Èó¥: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> Êï∞ÊçÆÈõÜÁ±ªÂûã: {dataset_type}")
        lines.append("")

        # Decision box
        lines.append("---")
        lines.append("")

        # Recommendation with visual indicator
        rec = assessment.recommendation
        if rec == Recommendation.HIGHLY_RECOMMENDED:
            rec_icon = "üü¢"
            rec_text = "Âº∫ÁÉàÊé®Ëçê"
        elif rec == Recommendation.RECOMMENDED:
            rec_icon = "üü¢"
            rec_text = "Êé®Ëçê"
        elif rec == Recommendation.CONDITIONAL:
            rec_icon = "üü°"
            rec_text = "ÊúâÊù°‰ª∂Êé®Ëçê"
        else:
            rec_icon = "üî¥"
            rec_text = "‰∏çÊé®Ëçê"

        lines.append(f"## {rec_icon} ÂÜ≥Á≠ñÂª∫ËÆÆ: {rec_text}")
        lines.append("")
        lines.append(f"**ËØÑÂàÜ**: {assessment.score}/10")
        lines.append("")
        lines.append(f"**ÁêÜÁî±**: {assessment.recommendation_reason}")
        lines.append("")

        # Quick stats
        lines.append("### ÂÖ≥ÈîÆÊåáÊ†á")
        lines.append("")
        total_cost = reproduction_cost.get("total", 0)
        human_cost = reproduction_cost.get("human", 0)
        lines.append("| ÊåáÊ†á | Êï∞ÂÄº |")
        lines.append("|------|------|")
        lines.append(f"| ÊÄªÊàêÊú¨ | ${total_cost:,.0f} |")
        lines.append(
            f"| ‰∫∫Â∑•ÊàêÊú¨ | ${human_cost:,.0f} ({human_cost / total_cost * 100:.0f}%) |"
            if total_cost > 0
            else f"| ‰∫∫Â∑•ÊàêÊú¨ | ${human_cost:,.0f} |"
        )
        lines.append(f"| ÊäïËµÑÂõûÊä•Áéá | {assessment.roi_ratio:.1f}x |")
        lines.append("")

        # Use cases
        lines.append("---")
        lines.append("")
        lines.append("## Áî®ÈÄî‰∏é‰ª∑ÂÄº")
        lines.append("")
        lines.append("### ‰∏ªË¶ÅÁî®ÈÄî")
        lines.append(f"**{assessment.primary_use_case}**")
        lines.append("")

        if assessment.secondary_use_cases:
            lines.append("### ÂÖ∂‰ªñÁî®ÈÄî")
            for use_case in assessment.secondary_use_cases:
                lines.append(f"- {use_case}")
            lines.append("")

        if assessment.expected_outcomes:
            lines.append("### È¢ÑÊúüÊàêÊûú")
            for outcome in assessment.expected_outcomes:
                lines.append(f"- {outcome}")
            lines.append("")

        # ROI Analysis
        lines.append("---")
        lines.append("")
        lines.append("## ROI ÂàÜÊûê")
        lines.append("")
        lines.append(f"**{assessment.roi_explanation}**")
        lines.append("")

        if assessment.payback_scenarios:
            lines.append("### ÂõûÊä•Âú∫ÊôØ")
            for scenario in assessment.payback_scenarios:
                lines.append(f"- {scenario}")
            lines.append("")

        # Risks
        lines.append("---")
        lines.append("")
        lines.append("## È£éÈô©ËØÑ‰º∞")
        lines.append("")

        if assessment.risks:
            lines.append("| È£éÈô©Á≠âÁ∫ß | ÊèèËø∞ | ÁºìËß£Êé™ÊñΩ |")
            lines.append("|----------|------|----------|")
            for risk in assessment.risks:
                level = risk.get("level", "‰∏≠")
                desc = risk.get("description", "")
                mitigation = risk.get("mitigation", "")
                lines.append(f"| {level} | {desc} | {mitigation} |")
            lines.append("")

        # Alternatives
        lines.append("---")
        lines.append("")
        lines.append("## Êõø‰ª£ÊñπÊ°à")
        lines.append("")

        if assessment.alternatives:
            lines.append("ÂèØËÄÉËôëÁöÑÊõø‰ª£Êï∞ÊçÆÈõÜ:")
            for alt in assessment.alternatives:
                lines.append(f"- {alt}")
            lines.append("")
        else:
            lines.append("ÊöÇÊó†Â∑≤Áü•Êõø‰ª£ÊñπÊ°à„ÄÇ")
            lines.append("")

        if assessment.competitive_advantage:
            lines.append(f"**Êú¨Êï∞ÊçÆÈõÜ‰ºòÂäø**: {assessment.competitive_advantage}")
            lines.append("")

        # Footer
        lines.append("---")
        lines.append("")
        lines.append("> Êú¨ÊëòË¶ÅÁî± DataRecipe Ëá™Âä®ÁîüÊàêÔºå‰æõÂÜ≥Á≠ñÂèÇËÄÉ")

        return "\n".join(lines)

    def to_dict(self, assessment: ValueAssessment) -> dict[str, Any]:
        """Convert assessment to dictionary."""
        return {
            "score": assessment.score,
            "recommendation": assessment.recommendation.value,
            "recommendation_reason": assessment.recommendation_reason,
            "primary_use_case": assessment.primary_use_case,
            "secondary_use_cases": assessment.secondary_use_cases,
            "expected_outcomes": assessment.expected_outcomes,
            "roi_ratio": assessment.roi_ratio,
            "roi_explanation": assessment.roi_explanation,
            "payback_scenarios": assessment.payback_scenarios,
            "risks": assessment.risks,
            "alternatives": assessment.alternatives,
            "competitive_advantage": assessment.competitive_advantage,
        }

    # ------------------------------------------------------------------
    # analyze-spec pipeline: generate from SpecificationAnalysis
    # ------------------------------------------------------------------

    def spec_to_markdown(
        self,
        analysis: Any,
        target_size: int,
        region: str,
        cost_per_item: float,
        enhanced_context: Any = None,
    ) -> str:
        """Generate EXECUTIVE_SUMMARY.md from a SpecificationAnalysis object.

        Used by the analyze-spec pipeline (vs to_markdown which is for deep-analyze).
        """
        total_cost = cost_per_item * target_size
        human_cost = total_cost * (analysis.estimated_human_percentage / 100)

        # Determine recommendation
        if analysis.estimated_difficulty == "expert":
            recommendation = "ÊúâÊù°‰ª∂Êé®Ëçê"
            rec_icon = "üü°"
            score = 5.5
        elif analysis.estimated_difficulty == "hard":
            recommendation = "Êé®Ëçê"
            rec_icon = "üü¢"
            score = 6.5
        else:
            recommendation = "Âº∫ÁÉàÊé®Ëçê"
            rec_icon = "üü¢"
            score = 7.5

        lines = []
        lines.append(f"# {analysis.project_name} ÊâßË°åÊëòË¶Å")
        lines.append("")
        lines.append(f"> ÁîüÊàêÊó∂Èó¥: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> Êï∞ÊçÆÈõÜÁ±ªÂûã: {analysis.dataset_type}")
        lines.append(f"> ÁõÆÊ†áËßÑÊ®°: {target_size} Êù°")
        lines.append("")
        lines.append("---")
        lines.append("")

        # Decision box
        lines.append(f"## {rec_icon} ÂÜ≥Á≠ñÂª∫ËÆÆ: {recommendation}")
        lines.append("")
        lines.append(f"**ËØÑÂàÜ**: {score}/10")
        lines.append("")
        lines.append(f"**ÁêÜÁî±**: Êï∞ÊçÆÈõÜ‰ª∑ÂÄºËâØÂ•Ω (ËØÑÂàÜ {score}/10)Ôºå{recommendation}")
        lines.append("")

        # Key metrics
        lines.append("### ÂÖ≥ÈîÆÊåáÊ†á")
        lines.append("")
        lines.append("| ÊåáÊ†á | Êï∞ÂÄº |")
        lines.append("|------|------|")
        lines.append(f"| ÊÄªÊàêÊú¨ | ${total_cost:,.0f} |")
        lines.append(
            f"| ‰∫∫Â∑•ÊàêÊú¨ | ${human_cost:,.0f} ({analysis.estimated_human_percentage:.0f}%) |"
        )
        lines.append(f"| ÈöæÂ∫¶ | {analysis.estimated_difficulty} |")
        lines.append(f"| È¢ÜÂüü | {analysis.estimated_domain} |")
        lines.append("")

        # Use cases
        ec = enhanced_context
        lines.append("---")
        lines.append("")
        lines.append("## Áî®ÈÄî‰∏é‰ª∑ÂÄº")
        lines.append("")
        if ec and ec.generated and ec.dataset_purpose_summary:
            lines.append(f"**‰∏ªË¶ÅÁî®ÈÄî**: {ec.dataset_purpose_summary}")
        else:
            lines.append(f"**‰∏ªË¶ÅÁî®ÈÄî**: {analysis.description or analysis.task_description}")
        lines.append("")

        if ec and ec.generated and ec.tailored_use_cases:
            lines.append("### ÂÖ∑‰ΩìÂ∫îÁî®Âú∫ÊôØ")
            lines.append("")
            for i, uc in enumerate(ec.tailored_use_cases, 1):
                lines.append(f"{i}. {uc}")
            lines.append("")

        if ec and ec.generated and ec.tailored_roi_scenarios:
            lines.append("### ÊäïËµÑÂõûÊä•ÂàÜÊûê")
            lines.append("")
            for i, roi in enumerate(ec.tailored_roi_scenarios, 1):
                lines.append(f"{i}. {roi}")
            lines.append("")

        if ec and ec.generated and ec.competitive_positioning:
            lines.append("### Á´û‰∫âÂÆö‰Ωç")
            lines.append("")
            lines.append(ec.competitive_positioning)
            lines.append("")

        # Risks
        lines.append("---")
        lines.append("")
        lines.append("## È£éÈô©ËØÑ‰º∞")
        lines.append("")
        lines.append("| È£éÈô©Á≠âÁ∫ß | ÊèèËø∞ | ÁºìËß£Êé™ÊñΩ |")
        lines.append("|----------|------|----------|")

        if ec and ec.generated and ec.tailored_risks:
            for risk in ec.tailored_risks:
                level = risk.get("level", "‰∏≠")
                desc = risk.get("description", "")
                mit = risk.get("mitigation", "")
                lines.append(f"| {level} | {desc} | {mit} |")
        else:
            if (
                "AI" in str(analysis.forbidden_items)
                or "ai" in str(analysis.forbidden_items).lower()
            ):
                lines.append(
                    "| È´ò | Á¶ÅÊ≠¢‰ΩøÁî®AIÁîüÊàêÂÜÖÂÆπÔºåÂÖ®‰∫∫Â∑•ÊàêÊú¨È´ò | ‰∏•Ê†ºÂÆ°Ê†∏ÊµÅÁ®ãÔºåÁ°Æ‰øùÊï∞ÊçÆÂéüÂàõÊÄß |"
                )

            if analysis.estimated_difficulty in ["hard", "expert"]:
                lines.append("| ‰∏≠ | ÈöæÂ∫¶ËæÉÈ´òÔºåÈúÄË¶Å‰∏ì‰∏ö‰∫∫Âëò | ÊèêÂâçÂÇ®Â§á‰∫∫ÊâçÔºåÂä†Âº∫ÂüπËÆ≠ |")

            if analysis.has_images:
                lines.append("| ‰∏≠ | ÂåÖÂê´ÂõæÁâáÔºåÂà∂‰ΩúÊàêÊú¨ËæÉÈ´ò | Âª∫Á´ãÂõæÁâáÁ¥†ÊùêÂ∫ìÔºåËßÑËåÉÂà∂‰ΩúÊµÅÁ®ã |")

            lines.append("| ‰Ωé | Ê†áÊ≥®Ë¥®ÈáèÂèØËÉΩÊ≥¢Âä® | Âª∫Á´ãQAÊµÅÁ®ãÔºåÂÆöÊúüÊ†°ÂáÜ |")
        lines.append("")

        # Similar datasets
        if analysis.similar_datasets:
            lines.append("---")
            lines.append("")
            lines.append("## Á±ª‰ººÊï∞ÊçÆÈõÜ")
            lines.append("")
            for ds in analysis.similar_datasets:
                lines.append(f"- {ds}")
            lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> Êú¨ÊëòË¶ÅÁî± DataRecipe ‰ªéÈúÄÊ±ÇÊñáÊ°£Ëá™Âä®ÁîüÊàê")

        return "\n".join(lines)

    def spec_to_dict(
        self,
        analysis: Any,
        target_size: int,
        cost_per_item: float,
    ) -> dict:
        """Convert SpecificationAnalysis to executive summary dict."""
        total_cost = cost_per_item * target_size
        human_cost = total_cost * (analysis.estimated_human_percentage / 100)
        api_cost = total_cost - human_cost

        if analysis.estimated_difficulty == "expert":
            recommendation, score = "ÊúâÊù°‰ª∂Êé®Ëçê", 5.5
        elif analysis.estimated_difficulty == "hard":
            recommendation, score = "Êé®Ëçê", 6.5
        else:
            recommendation, score = "Âº∫ÁÉàÊé®Ëçê", 7.5

        return {
            "project_name": analysis.project_name,
            "recommendation": recommendation,
            "score": score,
            "total_cost": total_cost,
            "human_cost": human_cost,
            "api_cost": api_cost,
            "human_percentage": analysis.estimated_human_percentage,
            "difficulty": analysis.estimated_difficulty,
            "domain": analysis.estimated_domain,
        }
