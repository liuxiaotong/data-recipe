"""Generate output documents from specification analysis."""

import json
import os
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional

from datarecipe.analyzers.spec_analyzer import FieldDefinition, SpecificationAnalysis
from datarecipe.pipeline import assemble_pipeline
from datarecipe.task_profiles import get_task_profile


@dataclass
class SpecOutputResult:
    """Result of specification output generation."""

    success: bool = True
    error: str = ""
    output_dir: str = ""
    files_generated: List[str] = field(default_factory=list)


class SpecOutputGenerator:
    """Generate all output documents from specification analysis."""

    def __init__(self, output_dir: str = "./spec_output"):
        self.output_dir = output_dir

    def generate(
        self,
        analysis: SpecificationAnalysis,
        target_size: int = 100,
        region: str = "china",
        enhanced_context=None,
    ) -> SpecOutputResult:
        """Generate all output documents.

        Args:
            analysis: SpecificationAnalysis from spec analyzer
            target_size: Target dataset size for cost estimation
            region: Region for cost calculation

        Returns:
            SpecOutputResult with generated files
        """
        result = SpecOutputResult()

        try:
            # Create output directory with structure
            project_name = analysis.project_name or "spec_analysis"
            safe_name = project_name.replace("/", "_").replace(" ", "_")
            output_dir = os.path.join(self.output_dir, safe_name)

            # Create subdirectories
            subdirs = {
                "decision": "01_å†³ç­–å‚è€ƒ",
                "project": "02_é¡¹ç›®ç®¡ç†",
                "annotation": "03_æ ‡æ³¨è§„èŒƒ",
                "guide": "04_å¤åˆ»æŒ‡å—",
                "cost": "05_æˆæœ¬åˆ†æ",
                "data": "06_åŸå§‹æ•°æ®",
                "templates": "07_æ¨¡æ¿",
                "ai_agent": "08_AI_Agent",
                "samples": "09_æ ·ä¾‹æ•°æ®",
            }
            for key, subdir in subdirs.items():
                os.makedirs(os.path.join(output_dir, subdir), exist_ok=True)

            result.output_dir = output_dir

            # Generate each document
            self._generate_annotation_spec(analysis, output_dir, subdirs, result, enhanced_context=enhanced_context)
            self._generate_executive_summary(analysis, output_dir, subdirs, target_size, region, result, enhanced_context=enhanced_context)
            self._generate_milestone_plan(analysis, output_dir, subdirs, target_size, region, result, enhanced_context=enhanced_context)
            self._generate_cost_breakdown(analysis, output_dir, subdirs, target_size, region, result)
            self._generate_industry_benchmark(analysis, output_dir, subdirs, target_size, region, result)
            self._generate_raw_analysis(analysis, output_dir, subdirs, result)

            # Generate production documents
            self._generate_training_guide(analysis, output_dir, subdirs, result)
            self._generate_qa_checklist(analysis, output_dir, subdirs, result)
            self._generate_data_template(analysis, output_dir, subdirs, result)
            self._generate_production_sop(analysis, output_dir, subdirs, result)
            self._generate_data_schema(analysis, output_dir, subdirs, result)

            # Generate validation guides for all strategies
            self._generate_validation_guide(analysis, output_dir, subdirs, result)

            # Generate AI Agent layer
            self._generate_ai_agent_context(analysis, output_dir, subdirs, target_size, region, result)
            self._generate_ai_workflow_state(analysis, output_dir, subdirs, result)
            self._generate_ai_reasoning_traces(analysis, output_dir, subdirs, target_size, region, result)
            self._generate_ai_pipeline(analysis, output_dir, subdirs, result)
            self._generate_ai_readme(analysis, output_dir, subdirs, result)

            # Generate sample data
            self._generate_think_po_samples(analysis, output_dir, subdirs, target_size, result)

            self._generate_readme(analysis, output_dir, subdirs, result)

        except Exception as e:
            result.success = False
            result.error = str(e)

        return result

    def _generate_annotation_spec(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
        enhanced_context=None,
    ):
        """Generate ANNOTATION_SPEC.md."""
        lines = []

        lines.append(f"# {analysis.project_name} æ ‡æ³¨è§„èŒƒ")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> æ•°æ®ç±»å‹: {analysis.dataset_type}")
        if analysis.has_images:
            lines.append(f"> åŒ…å«å›¾ç‰‡: æ˜¯ ({analysis.image_count} å¼ )")
        lines.append("")
        lines.append("---")
        lines.append("")

        # Section 1: Task Type Description
        lines.append("## ä¸€ã€é¢˜ç›®ç±»å‹æè¿°")
        lines.append("")
        lines.append(f"**ä»»åŠ¡åç§°**: {analysis.task_type}")
        lines.append("")
        lines.append(f"**ä»»åŠ¡è¯´æ˜**: {analysis.task_description}")
        lines.append("")

        if analysis.cognitive_requirements:
            lines.append("**è®¤çŸ¥è¦æ±‚**:")
            for req in analysis.cognitive_requirements:
                lines.append(f"- {req}")
            lines.append("")

        if analysis.reasoning_chain:
            lines.append("**æ¨ç†é“¾**:")
            lines.append("")
            lines.append("```")
            lines.append(" â†’ ".join(analysis.reasoning_chain))
            lines.append("```")
            lines.append("")

        # Section 2: Data Requirements
        lines.append("## äºŒã€æ•°æ®è¦æ±‚")
        lines.append("")

        if analysis.data_requirements:
            for i, req in enumerate(analysis.data_requirements, 1):
                lines.append(f"{i}. {req}")
            lines.append("")

        # Section 3: Quality Constraints
        lines.append("## ä¸‰ã€è´¨é‡çº¦æŸ")
        lines.append("")

        if analysis.forbidden_items:
            lines.append("### ç¦æ­¢é¡¹ âš ï¸")
            lines.append("")
            for item in analysis.forbidden_items:
                lines.append(f"- âŒ {item}")
            lines.append("")

        if analysis.quality_constraints:
            lines.append("### è´¨é‡æ ‡å‡†")
            lines.append("")
            for constraint in analysis.quality_constraints:
                lines.append(f"- {constraint}")
            lines.append("")

        if analysis.difficulty_criteria:
            lines.append("### éš¾åº¦éªŒè¯")
            lines.append("")
            lines.append(f"{analysis.difficulty_criteria}")
            lines.append("")

        # Section 4: Data Structure
        lines.append("## å››ã€æ•°æ®ç»“æ„")
        lines.append("")

        if analysis.fields:
            lines.append("| å­—æ®µå | ç±»å‹ | å¿…å¡« | è¯´æ˜ |")
            lines.append("|--------|------|------|------|")
            for f in analysis.fields:
                name = f.get("name", "")
                ftype = f.get("type", "string")
                required = "æ˜¯" if f.get("required", True) else "å¦"
                desc = f.get("description", "")
                lines.append(f"| {name} | {ftype} | {required} | {desc} |")
            lines.append("")

        if analysis.field_requirements:
            lines.append("### å­—æ®µè¯¦ç»†è¦æ±‚")
            lines.append("")
            for fname, freq in analysis.field_requirements.items():
                lines.append(f"**{fname}**: {freq}")
                lines.append("")

        # Section 5: Examples
        lines.append("## äº”ã€ç¤ºä¾‹")
        lines.append("")

        for i, example in enumerate(analysis.examples[:3], 1):
            lines.append(f"### ç¤ºä¾‹ {i}")
            lines.append("")

            if example.get("has_image"):
                lines.append("**[åŒ…å«å›¾ç‰‡]**")
                lines.append("")

            if example.get("question"):
                lines.append("**é¢˜ç›®**:")
                lines.append("")
                lines.append(f"> {example['question']}")
                lines.append("")

            if example.get("answer"):
                lines.append(f"**ç­”æ¡ˆ**: {example['answer']}")
                lines.append("")

            if example.get("scoring_rubric"):
                lines.append("**æ‰“åˆ†æ ‡å‡†**:")
                lines.append("")
                lines.append(f"{example['scoring_rubric']}")
                lines.append("")

            lines.append("---")
            lines.append("")

        # Section 6: Scoring Rubric
        if analysis.scoring_rubric:
            lines.append("## å…­ã€æ‰“åˆ†æ ‡å‡†")
            lines.append("")
            lines.append("| åˆ†æ•° | æ ‡å‡† |")
            lines.append("|------|------|")
            for rubric in analysis.scoring_rubric:
                score = rubric.get("score", "")
                criteria = rubric.get("criteria", "")
                lines.append(f"| {score} | {criteria} |")
            lines.append("")

        # Section 7: Domain-specific guidance (LLM enhanced)
        ec = enhanced_context
        if ec and ec.generated and ec.domain_specific_guidelines:
            lines.append("## ä¸ƒã€é¢†åŸŸæ ‡æ³¨æŒ‡å¯¼")
            lines.append("")
            lines.append(ec.domain_specific_guidelines)
            lines.append("")

            if ec.quality_pitfalls:
                lines.append("### å¸¸è§é”™è¯¯")
                lines.append("")
                for i, pitfall in enumerate(ec.quality_pitfalls, 1):
                    lines.append(f"{i}. {pitfall}")
                lines.append("")

            if ec.example_analysis:
                lines.append("### æ ·æœ¬åˆ†æ")
                lines.append("")
                for ex in ec.example_analysis:
                    idx = ex.get("sample_index", "?")
                    lines.append(f"**æ ·æœ¬ {idx}**")
                    lines.append(f"- ä¼˜ç‚¹: {ex.get('strengths', '')}")
                    lines.append(f"- æ”¹è¿›: {ex.get('weaknesses', '')}")
                    lines.append(f"- å»ºè®®: {ex.get('annotation_tips', '')}")
                    lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> æœ¬è§„èŒƒç”± DataRecipe ä»éœ€æ±‚æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ")

        # Write file
        spec_path = os.path.join(output_dir, subdirs["annotation"], "ANNOTATION_SPEC.md")
        with open(spec_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(f"{subdirs['annotation']}/ANNOTATION_SPEC.md")

        # Also save as JSON
        spec_dict = {
            "project_name": analysis.project_name,
            "dataset_type": analysis.dataset_type,
            "task_type": analysis.task_type,
            "task_description": analysis.task_description,
            "cognitive_requirements": analysis.cognitive_requirements,
            "reasoning_chain": analysis.reasoning_chain,
            "data_requirements": analysis.data_requirements,
            "quality_constraints": analysis.quality_constraints,
            "forbidden_items": analysis.forbidden_items,
            "difficulty_criteria": analysis.difficulty_criteria,
            "fields": analysis.fields,
            "field_requirements": analysis.field_requirements,
            "examples": analysis.examples,
            "scoring_rubric": analysis.scoring_rubric,
        }
        json_path = os.path.join(output_dir, subdirs["annotation"], "annotation_spec.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(spec_dict, f, indent=2, ensure_ascii=False)
        result.files_generated.append(f"{subdirs['annotation']}/annotation_spec.json")

    def _generate_executive_summary(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        target_size: int,
        region: str,
        result: SpecOutputResult,
        enhanced_context=None,
    ):
        """Generate EXECUTIVE_SUMMARY.md."""
        # Calculate cost estimates
        cost_per_item = self._estimate_cost_per_item(analysis, region)
        total_cost = cost_per_item * target_size
        human_cost = total_cost * (analysis.estimated_human_percentage / 100)
        api_cost = total_cost - human_cost

        # Determine recommendation
        if analysis.estimated_difficulty == "expert":
            recommendation = "æœ‰æ¡ä»¶æ¨è"
            rec_icon = "ğŸŸ¡"
            score = 5.5
        elif analysis.estimated_difficulty == "hard":
            recommendation = "æ¨è"
            rec_icon = "ğŸŸ¢"
            score = 6.5
        else:
            recommendation = "å¼ºçƒˆæ¨è"
            rec_icon = "ğŸŸ¢"
            score = 7.5

        lines = []
        lines.append(f"# {analysis.project_name} æ‰§è¡Œæ‘˜è¦")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> æ•°æ®é›†ç±»å‹: {analysis.dataset_type}")
        lines.append(f"> ç›®æ ‡è§„æ¨¡: {target_size} æ¡")
        lines.append("")
        lines.append("---")
        lines.append("")

        # Decision box
        lines.append(f"## {rec_icon} å†³ç­–å»ºè®®: {recommendation}")
        lines.append("")
        lines.append(f"**è¯„åˆ†**: {score}/10")
        lines.append("")
        lines.append(f"**ç†ç”±**: æ•°æ®é›†ä»·å€¼è‰¯å¥½ (è¯„åˆ† {score}/10)ï¼Œ{recommendation}")
        lines.append("")

        # Key metrics
        lines.append("### å…³é”®æŒ‡æ ‡")
        lines.append("")
        lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
        lines.append("|------|------|")
        lines.append(f"| æ€»æˆæœ¬ | ${total_cost:,.0f} |")
        lines.append(f"| äººå·¥æˆæœ¬ | ${human_cost:,.0f} ({analysis.estimated_human_percentage:.0f}%) |")
        lines.append(f"| éš¾åº¦ | {analysis.estimated_difficulty} |")
        lines.append(f"| é¢†åŸŸ | {analysis.estimated_domain} |")
        lines.append("")

        # Use cases
        ec = enhanced_context
        lines.append("---")
        lines.append("")
        lines.append("## ç”¨é€”ä¸ä»·å€¼")
        lines.append("")
        if ec and ec.generated and ec.dataset_purpose_summary:
            lines.append(f"**ä¸»è¦ç”¨é€”**: {ec.dataset_purpose_summary}")
        else:
            lines.append(f"**ä¸»è¦ç”¨é€”**: {analysis.description or analysis.task_description}")
        lines.append("")

        if ec and ec.generated and ec.tailored_use_cases:
            lines.append("### å…·ä½“åº”ç”¨åœºæ™¯")
            lines.append("")
            for i, uc in enumerate(ec.tailored_use_cases, 1):
                lines.append(f"{i}. {uc}")
            lines.append("")

        if ec and ec.generated and ec.tailored_roi_scenarios:
            lines.append("### æŠ•èµ„å›æŠ¥åˆ†æ")
            lines.append("")
            for i, roi in enumerate(ec.tailored_roi_scenarios, 1):
                lines.append(f"{i}. {roi}")
            lines.append("")

        if ec and ec.generated and ec.competitive_positioning:
            lines.append("### ç«äº‰å®šä½")
            lines.append("")
            lines.append(ec.competitive_positioning)
            lines.append("")

        # Risks
        lines.append("---")
        lines.append("")
        lines.append("## é£é™©è¯„ä¼°")
        lines.append("")
        lines.append("| é£é™©ç­‰çº§ | æè¿° | ç¼“è§£æªæ–½ |")
        lines.append("|----------|------|----------|")

        if ec and ec.generated and ec.tailored_risks:
            for risk in ec.tailored_risks:
                level = risk.get("level", "ä¸­")
                desc = risk.get("description", "")
                mit = risk.get("mitigation", "")
                lines.append(f"| {level} | {desc} | {mit} |")
        else:
            if "AI" in str(analysis.forbidden_items) or "ai" in str(analysis.forbidden_items).lower():
                lines.append("| é«˜ | ç¦æ­¢ä½¿ç”¨AIç”Ÿæˆå†…å®¹ï¼Œå…¨äººå·¥æˆæœ¬é«˜ | ä¸¥æ ¼å®¡æ ¸æµç¨‹ï¼Œç¡®ä¿æ•°æ®åŸåˆ›æ€§ |")

            if analysis.estimated_difficulty in ["hard", "expert"]:
                lines.append("| ä¸­ | éš¾åº¦è¾ƒé«˜ï¼Œéœ€è¦ä¸“ä¸šäººå‘˜ | æå‰å‚¨å¤‡äººæ‰ï¼ŒåŠ å¼ºåŸ¹è®­ |")

            if analysis.has_images:
                lines.append("| ä¸­ | åŒ…å«å›¾ç‰‡ï¼Œåˆ¶ä½œæˆæœ¬è¾ƒé«˜ | å»ºç«‹å›¾ç‰‡ç´ æåº“ï¼Œè§„èŒƒåˆ¶ä½œæµç¨‹ |")

            lines.append("| ä½ | æ ‡æ³¨è´¨é‡å¯èƒ½æ³¢åŠ¨ | å»ºç«‹QAæµç¨‹ï¼Œå®šæœŸæ ¡å‡† |")
        lines.append("")

        # Similar datasets
        if analysis.similar_datasets:
            lines.append("---")
            lines.append("")
            lines.append("## ç±»ä¼¼æ•°æ®é›†")
            lines.append("")
            for ds in analysis.similar_datasets:
                lines.append(f"- {ds}")
            lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> æœ¬æ‘˜è¦ç”± DataRecipe ä»éœ€æ±‚æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ")

        # Write file
        path = os.path.join(output_dir, subdirs["decision"], "EXECUTIVE_SUMMARY.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(f"{subdirs['decision']}/EXECUTIVE_SUMMARY.md")

        # Save JSON
        summary_dict = {
            "project_name": analysis.project_name,
            "recommendation": recommendation,
            "score": score,
            "total_cost": total_cost,
            "human_cost": human_cost,
            "api_cost": api_cost,
            "human_percentage": analysis.estimated_human_percentage,
            "difficulty": analysis.estimated_difficulty,
            "domain": analysis.estimated_domain,
        }
        json_path = os.path.join(output_dir, subdirs["decision"], "executive_summary.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(summary_dict, f, indent=2, ensure_ascii=False)
        result.files_generated.append(f"{subdirs['decision']}/executive_summary.json")

    def _generate_milestone_plan(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        target_size: int,
        region: str,
        result: SpecOutputResult,
        enhanced_context=None,
    ):
        """Generate MILESTONE_PLAN.md."""
        # Estimate duration based on difficulty
        difficulty_days = {
            "easy": 14,
            "medium": 21,
            "hard": 30,
            "expert": 45,
        }
        total_days = difficulty_days.get(analysis.estimated_difficulty, 30)

        lines = []
        lines.append(f"# {analysis.project_name} é‡Œç¨‹ç¢‘è®¡åˆ’")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> æ•°æ®é›†ç±»å‹: {analysis.dataset_type}")
        lines.append(f"> ç›®æ ‡è§„æ¨¡: {target_size} æ¡")
        lines.append(f"> é¢„ä¼°å·¥æœŸ: {total_days} å·¥ä½œæ—¥")
        lines.append("")
        lines.append("---")
        lines.append("")

        # Progress visualization
        lines.append("## é¡¹ç›®æ¦‚è§ˆ")
        lines.append("")
        lines.append("```")
        lines.append("é˜¶æ®µè¿›åº¦:")
        lines.append("M1 é¡¹ç›®å¯åŠ¨ä¸è§„èŒƒåˆ¶å®š    â–ˆâ–ˆâ–ˆ                  15%")
        lines.append("M2 è¯•ç‚¹æ ‡æ³¨ä¸æ ‡å‡†æ ¡å‡†    â–ˆâ–ˆ                   10%")
        lines.append("M3 ä¸»ä½“æ ‡æ³¨ - ç¬¬ä¸€æ‰¹æ¬¡  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               30%")
        lines.append("M4 ä¸»ä½“æ ‡æ³¨ - ç¬¬äºŒæ‰¹æ¬¡  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               30%")
        lines.append("M5 è´¨é‡å®¡æ ¸ä¸äº¤ä»˜      â–ˆâ–ˆâ–ˆ                  15%")
        lines.append("```")
        lines.append("")

        # Team composition
        lines.append("### å›¢é˜Ÿé…ç½®")
        lines.append("")
        lines.append("| è§’è‰² | äººæ•° | è¯´æ˜ |")
        lines.append("|------|------|------|")
        lines.append("| é¡¹ç›®ç»ç† | 1 | æ•´ä½“åè°ƒ |")

        if analysis.estimated_difficulty in ["hard", "expert"]:
            lines.append("| é¢†åŸŸä¸“å®¶ | 2-3 | è§„åˆ™è®¾è®¡ã€è´¨é‡æŠŠæ§ |")
        else:
            lines.append("| é¢†åŸŸä¸“å®¶ | 1-2 | è§„åˆ™è®¾è®¡ã€è´¨é‡æŠŠæ§ |")

        lines.append("| QA | 1-2 | è´¨é‡æŠ½æ£€ |")

        if analysis.has_images:
            lines.append("| å›¾ç‰‡åˆ¶ä½œ | 2-3 | åŸåˆ›å›¾ç‰‡è®¾è®¡ |")

        annotator_count = max(2, target_size // 50)
        lines.append(f"| æ ‡æ³¨å‘˜ | {annotator_count}-{annotator_count + 2} | æ•°æ®ç”Ÿäº§ |")
        lines.append("")

        # Milestones
        lines.append("---")
        lines.append("")
        lines.append("## é‡Œç¨‹ç¢‘è¯¦æƒ…")
        lines.append("")

        milestones = [
            ("M1", "é¡¹ç›®å¯åŠ¨ä¸è§„èŒƒåˆ¶å®š", "å®Œæˆé¡¹ç›®åˆå§‹åŒ–ã€åˆ¶å®šæ ‡æ³¨è§„èŒƒå’Œè´¨é‡æ ‡å‡†",
             ["æ ‡æ³¨æŒ‡å—æ–‡æ¡£ v1.0", "Schema å®šä¹‰ä¸ç¤ºä¾‹", "æ ‡æ³¨å·¥å…·é…ç½®å®Œæˆ", "å›¢é˜ŸåŸ¹è®­ææ–™"]),
            ("M2", "è¯•ç‚¹æ ‡æ³¨ä¸æ ‡å‡†æ ¡å‡†", "å®Œæˆè¯•ç‚¹æ‰¹æ¬¡ï¼ŒéªŒè¯æ ‡æ³¨æµç¨‹å’Œè´¨é‡æ ‡å‡†",
             [f"è¯•ç‚¹æ•°æ® ({max(5, target_size // 20)} æ¡)", "æ ‡æ³¨ä¸€è‡´æ€§æŠ¥å‘Š", "æµç¨‹é—®é¢˜æ¸…å•ä¸è§£å†³æ–¹æ¡ˆ"]),
            ("M3", "ä¸»ä½“æ ‡æ³¨ - ç¬¬ä¸€æ‰¹æ¬¡", "å®Œæˆ 40% çš„æ ‡æ³¨é‡",
             [f"å·²æ ‡æ³¨æ•°æ® ({int(target_size * 0.4)} æ¡)", "è´¨é‡å‘¨æŠ¥"]),
            ("M4", "ä¸»ä½“æ ‡æ³¨ - ç¬¬äºŒæ‰¹æ¬¡", "å®Œæˆå‰©ä½™ 60% çš„æ ‡æ³¨é‡",
             [f"å·²æ ‡æ³¨æ•°æ® ({target_size} æ¡)", "è´¨é‡å‘¨æŠ¥"]),
            ("M5", "è´¨é‡å®¡æ ¸ä¸äº¤ä»˜", "å®Œæˆæœ€ç»ˆè´¨é‡å®¡æ ¸å’Œæ•°æ®äº¤ä»˜",
             ["æœ€ç»ˆæ•°æ®é›†", "è´¨é‡æŠ¥å‘Š", "æ•°æ®æ–‡æ¡£"]),
        ]

        for mid, name, desc, deliverables in milestones:
            lines.append(f"### {mid}: {name}")
            lines.append("")
            lines.append(f"**æè¿°**: {desc}")
            lines.append("")
            lines.append("**äº¤ä»˜ç‰©**:")
            for d in deliverables:
                lines.append(f"- [ ] {d}")
            lines.append("")

        # Acceptance criteria
        lines.append("---")
        lines.append("")
        lines.append("## éªŒæ”¶æ ‡å‡†")
        lines.append("")
        lines.append("| ç±»åˆ« | æŒ‡æ ‡ | é˜ˆå€¼ |")
        lines.append("|------|------|------|")
        lines.append("| ä¸€è‡´æ€§ | Cohen's Kappa | â‰¥ 0.7 |")
        lines.append("| å‡†ç¡®æ€§ | ä¸“å®¶å®¡æ ¸é€šè¿‡ç‡ | â‰¥ 95% |")
        lines.append("| å®Œæ•´æ€§ | ç©ºå€¼ç‡ | = 0% |")

        if analysis.difficulty_criteria:
            lines.append(f"| éš¾åº¦ | {analysis.difficulty_criteria[:30]}... | é€šè¿‡éªŒè¯ |")

        lines.append("")

        # Risks
        lines.append("---")
        lines.append("")
        lines.append("## é£é™©ç®¡ç†")
        lines.append("")

        if analysis.forbidden_items:
            lines.append("### R1: æ•°æ®åˆè§„æ€§é£é™©")
            lines.append("")
            lines.append("- **æ¦‚ç‡**: ğŸŸ¡ ä¸­")
            lines.append("- **å½±å“**: ğŸ”´ é«˜")
            lines.append("- **ç¼“è§£æªæ–½**: ä¸¥æ ¼å®¡æ ¸æµç¨‹ï¼Œç¡®ä¿ä¸å«AIå†…å®¹")
            lines.append("")

        lines.append("### R2: è´¨é‡ä¸ç¨³å®šé£é™©")
        lines.append("")
        lines.append("- **æ¦‚ç‡**: ğŸŸ¡ ä¸­")
        lines.append("- **å½±å“**: ğŸŸ¡ ä¸­")
        lines.append("- **ç¼“è§£æªæ–½**: åŠ å¼ºåŸ¹è®­ï¼Œå®šæœŸæ ¡å‡†ï¼Œå»ºç«‹QAæµç¨‹")
        lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> æœ¬è®¡åˆ’ç”± DataRecipe ä»éœ€æ±‚æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ")

        # Write file
        path = os.path.join(output_dir, subdirs["project"], "MILESTONE_PLAN.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(f"{subdirs['project']}/MILESTONE_PLAN.md")

        # Save JSON
        plan_dict = {
            "project_name": analysis.project_name,
            "target_size": target_size,
            "total_days": total_days,
            "milestones": [
                {"id": mid, "name": name, "description": desc, "deliverables": deliverables}
                for mid, name, desc, deliverables in milestones
            ],
        }
        json_path = os.path.join(output_dir, subdirs["project"], "milestone_plan.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(plan_dict, f, indent=2, ensure_ascii=False)
        result.files_generated.append(f"{subdirs['project']}/milestone_plan.json")

    def _generate_cost_breakdown(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        target_size: int,
        region: str,
        result: SpecOutputResult,
    ):
        """Generate COST_BREAKDOWN.md."""
        cost_per_item = self._estimate_cost_per_item(analysis, region)
        total_cost = cost_per_item * target_size
        human_cost = total_cost * (analysis.estimated_human_percentage / 100)

        # Design phase (fixed costs)
        design_cost = 2000 if analysis.estimated_difficulty in ["hard", "expert"] else 1200

        # Production phase (variable costs)
        production_cost = human_cost * 0.7

        # QA phase
        qa_cost = human_cost * 0.2

        # Contingency
        contingency = total_cost * 0.15

        grand_total = design_cost + production_cost + qa_cost + contingency

        lines = []
        lines.append(f"# {analysis.project_name} æˆæœ¬æ˜ç»†")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> ç›®æ ‡è§„æ¨¡: {target_size} æ¡")
        lines.append(f"> å•æ¡æˆæœ¬: ${cost_per_item:.2f}")
        lines.append("")
        lines.append("---")
        lines.append("")

        lines.append("## é˜¶æ®µä¸€ï¼šè®¾è®¡é˜¶æ®µï¼ˆå›ºå®šæˆæœ¬ï¼‰")
        lines.append("")
        lines.append("| é¡¹ç›® | æˆæœ¬ |")
        lines.append("|------|------|")
        lines.append(f"| Schema è®¾è®¡ | ${design_cost * 0.3:.0f} |")
        lines.append(f"| æ ‡æ³¨æŒ‡å—ç¼–å†™ | ${design_cost * 0.4:.0f} |")
        lines.append(f"| è¯•ç‚¹æµ‹è¯• | ${design_cost * 0.2:.0f} |")
        lines.append(f"| å·¥å…·é…ç½® | ${design_cost * 0.1:.0f} |")
        lines.append(f"| **å°è®¡** | **${design_cost:.0f}** |")
        lines.append("")

        lines.append("## é˜¶æ®µäºŒï¼šç”Ÿäº§é˜¶æ®µï¼ˆå˜åŠ¨æˆæœ¬ï¼‰")
        lines.append("")
        lines.append("| é¡¹ç›® | æˆæœ¬ | å•ä»· |")
        lines.append("|------|------|------|")
        lines.append(f"| äººå·¥æ ‡æ³¨ | ${production_cost:.0f} | ${production_cost / target_size:.2f}/æ¡ |")

        if analysis.has_images:
            img_cost = target_size * 5  # $5 per image
            lines.append(f"| å›¾ç‰‡åˆ¶ä½œ | ${img_cost:.0f} | $5/å¼  |")
            production_cost += img_cost

        lines.append(f"| **å°è®¡** | **${production_cost:.0f}** | |")
        lines.append("")

        lines.append("## é˜¶æ®µä¸‰ï¼šè´¨é‡é˜¶æ®µ")
        lines.append("")
        lines.append("| é¡¹ç›® | æˆæœ¬ |")
        lines.append("|------|------|")
        lines.append(f"| QA æŠ½æ£€ | ${qa_cost * 0.6:.0f} |")
        lines.append(f"| è¿”å·¥ä¿®æ­£ | ${qa_cost * 0.3:.0f} |")
        lines.append(f"| ä¸“å®¶å¤æ ¸ | ${qa_cost * 0.1:.0f} |")
        lines.append(f"| **å°è®¡** | **${qa_cost:.0f}** |")
        lines.append("")

        lines.append("## æ±‡æ€»")
        lines.append("")
        lines.append("| é˜¶æ®µ | æˆæœ¬ | å æ¯” |")
        lines.append("|------|------|------|")
        lines.append(f"| è®¾è®¡é˜¶æ®µ | ${design_cost:.0f} | {design_cost / grand_total * 100:.1f}% |")
        lines.append(f"| ç”Ÿäº§é˜¶æ®µ | ${production_cost:.0f} | {production_cost / grand_total * 100:.1f}% |")
        lines.append(f"| è´¨é‡é˜¶æ®µ | ${qa_cost:.0f} | {qa_cost / grand_total * 100:.1f}% |")
        lines.append(f"| é£é™©é¢„ç•™ (15%) | ${contingency:.0f} | 15% |")
        lines.append(f"| **æ€»è®¡** | **${grand_total:.0f}** | 100% |")
        lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> æœ¬æˆæœ¬ä¼°ç®—ç”± DataRecipe ä»éœ€æ±‚æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ")

        # Write file
        path = os.path.join(output_dir, subdirs["cost"], "COST_BREAKDOWN.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(f"{subdirs['cost']}/COST_BREAKDOWN.md")

        # Save JSON
        cost_dict = {
            "target_size": target_size,
            "cost_per_item": cost_per_item,
            "design_cost": design_cost,
            "production_cost": production_cost,
            "qa_cost": qa_cost,
            "contingency": contingency,
            "grand_total": grand_total,
        }
        json_path = os.path.join(output_dir, subdirs["cost"], "cost_breakdown.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(cost_dict, f, indent=2, ensure_ascii=False)
        result.files_generated.append(f"{subdirs['cost']}/cost_breakdown.json")

    def _generate_industry_benchmark(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        target_size: int,
        region: str,
        result: SpecOutputResult,
    ):
        """Generate INDUSTRY_BENCHMARK.md."""
        cost_per_item = self._estimate_cost_per_item(analysis, region)
        total_cost = cost_per_item * target_size

        # Get benchmark data
        benchmarks = {
            "evaluation": {"min": 5, "avg": 15, "max": 50},
            "multimodal": {"min": 10, "avg": 25, "max": 80},
            "reasoning": {"min": 8, "avg": 20, "max": 60},
        }
        benchmark = benchmarks.get(analysis.dataset_type, {"min": 5, "avg": 15, "max": 50})

        lines = []
        lines.append(f"# {analysis.project_name} è¡Œä¸šåŸºå‡†å¯¹æ¯”")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> æ•°æ®é›†ç±»å‹: {analysis.dataset_type}")
        lines.append("")
        lines.append("---")
        lines.append("")

        lines.append("## é¡¹ç›®æ¦‚å†µ")
        lines.append("")
        lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
        lines.append("|------|------|")
        lines.append(f"| æ ·æœ¬æ•°é‡ | {target_size:,} |")
        lines.append(f"| æ€»æˆæœ¬ | ${total_cost:,.0f} |")
        lines.append(f"| å•æ¡æˆæœ¬ | ${cost_per_item:.2f} |")
        lines.append(f"| äººå·¥å æ¯” | {analysis.estimated_human_percentage:.0f}% |")
        lines.append("")

        lines.append("## è¡Œä¸šåŸºå‡†")
        lines.append("")
        lines.append(f"**æ•°æ®ç±»å‹**: {analysis.dataset_type}")
        lines.append("")
        lines.append("### å•æ¡æˆæœ¬åŸºå‡†")
        lines.append("")
        lines.append("```")
        lines.append(f"æœ€ä½: ${benchmark['min']:.2f}/æ¡")
        lines.append(f"å¹³å‡: ${benchmark['avg']:.2f}/æ¡")
        lines.append(f"æœ€é«˜: ${benchmark['max']:.2f}/æ¡")
        lines.append("```")
        lines.append("")

        # Rating
        if cost_per_item < benchmark["avg"]:
            rating = "ğŸŸ¢ æˆæœ¬ä½äºè¡Œä¸šå¹³å‡"
        elif cost_per_item <= benchmark["max"]:
            rating = "ğŸŸ¡ æˆæœ¬åœ¨åˆç†èŒƒå›´å†…"
        else:
            rating = "ğŸ”´ æˆæœ¬é«˜äºè¡Œä¸šåŸºå‡†"

        lines.append(f"**æˆæœ¬è¯„çº§**: {rating}")
        lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> åŸºå‡†æ•°æ®æ¥æºäºè¡Œä¸šè°ƒç ”ï¼Œä»…ä¾›å‚è€ƒ")

        # Write file
        path = os.path.join(output_dir, subdirs["project"], "INDUSTRY_BENCHMARK.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(f"{subdirs['project']}/INDUSTRY_BENCHMARK.md")

    def _generate_raw_analysis(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
    ):
        """Generate raw analysis JSON."""
        path = os.path.join(output_dir, subdirs["data"], "spec_analysis.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(analysis.to_dict(), f, indent=2, ensure_ascii=False)
        result.files_generated.append(f"{subdirs['data']}/spec_analysis.json")

    def _generate_readme(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
    ):
        """Generate README.md."""
        lines = []
        lines.append(f"# {analysis.project_name} åˆ†æäº§å‡º")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> æ•°æ®ç±»å‹: {analysis.dataset_type}")
        lines.append(f"> æ¥æº: éœ€æ±‚æ–‡æ¡£åˆ†æ")
        lines.append("")
        lines.append("## ç›®å½•ç»“æ„")
        lines.append("")
        lines.append("```")
        lines.append(f"{os.path.basename(output_dir)}/")
        lines.append("â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶")
        lines.append("â”‚")
        lines.append(f"â”œâ”€â”€ {subdirs['decision']}/           # ğŸ‘” å†³ç­–å±‚")
        lines.append("â”‚   â””â”€â”€ EXECUTIVE_SUMMARY.md     # æ‰§è¡Œæ‘˜è¦")
        lines.append("â”‚")
        lines.append(f"â”œâ”€â”€ {subdirs['project']}/           # ğŸ“‹ é¡¹ç›®ç®¡ç†")
        lines.append("â”‚   â”œâ”€â”€ MILESTONE_PLAN.md        # é‡Œç¨‹ç¢‘è®¡åˆ’")
        lines.append("â”‚   â””â”€â”€ INDUSTRY_BENCHMARK.md    # è¡Œä¸šåŸºå‡†")
        lines.append("â”‚")
        lines.append(f"â”œâ”€â”€ {subdirs['annotation']}/           # ğŸ“ æ ‡æ³¨å›¢é˜Ÿ")
        lines.append("â”‚   â”œâ”€â”€ ANNOTATION_SPEC.md       # æ ‡æ³¨è§„èŒƒ")
        lines.append("â”‚   â”œâ”€â”€ TRAINING_GUIDE.md        # åŸ¹è®­æ‰‹å†Œ")
        lines.append("â”‚   â””â”€â”€ QA_CHECKLIST.md          # è´¨æ£€æ¸…å•")
        lines.append("â”‚")
        lines.append(f"â”œâ”€â”€ {subdirs['guide']}/           # ğŸ“– å¤åˆ»æŒ‡å—")
        lines.append("â”‚   â”œâ”€â”€ PRODUCTION_SOP.md        # ç”Ÿäº§æµç¨‹")
        lines.append("â”‚   â”œâ”€â”€ DATA_SCHEMA.json         # æ•°æ®æ ¼å¼")
        if analysis.has_difficulty_validation():
            lines.append("â”‚   â””â”€â”€ DIFFICULTY_VALIDATION.md # éš¾åº¦éªŒè¯")
        lines.append("â”‚")
        lines.append(f"â”œâ”€â”€ {subdirs['cost']}/           # ğŸ’° æˆæœ¬åˆ†æ")
        lines.append("â”‚   â””â”€â”€ COST_BREAKDOWN.md        # æˆæœ¬æ˜ç»†")
        lines.append("â”‚")
        lines.append(f"â”œâ”€â”€ {subdirs['templates']}/              # ğŸ“‹ æ¨¡æ¿")
        lines.append("â”‚   â””â”€â”€ data_template.json       # æ•°æ®æ¨¡æ¿")
        lines.append("â”‚")
        lines.append(f"â”œâ”€â”€ {subdirs['data']}/           # ğŸ“Š åŸå§‹æ•°æ®")
        lines.append("â”‚   â””â”€â”€ spec_analysis.json       # åˆ†ææ•°æ®")
        lines.append("â”‚")
        lines.append(f"â”œâ”€â”€ {subdirs['ai_agent']}/            # ğŸ¤– AI Agent")
        lines.append("â”‚   â”œâ”€â”€ agent_context.json       # èšåˆå…¥å£")
        lines.append("â”‚   â”œâ”€â”€ workflow_state.json      # å·¥ä½œæµçŠ¶æ€")
        lines.append("â”‚   â”œâ”€â”€ reasoning_traces.json    # æ¨ç†é“¾")
        lines.append("â”‚   â””â”€â”€ pipeline.yaml            # å¯æ‰§è¡Œæµæ°´çº¿")
        lines.append("â”‚")
        lines.append(f"â””â”€â”€ {subdirs['samples']}/           # ğŸ§ª æ ·ä¾‹æ•°æ®")
        lines.append("    â”œâ”€â”€ samples.json             # æ ·ä¾‹æ•°æ®")
        lines.append("    â””â”€â”€ SAMPLE_GUIDE.md          # æ ·ä¾‹æŒ‡å—")
        lines.append("```")
        lines.append("")
        lines.append("## å¿«é€Ÿå¯¼èˆª")
        lines.append("")
        lines.append("| ç›®æ ‡ | æŸ¥çœ‹æ–‡ä»¶ |")
        lines.append("|------|----------|")
        lines.append(f"| **å¿«é€Ÿå†³ç­–** | `{subdirs['decision']}/EXECUTIVE_SUMMARY.md` |")
        lines.append(f"| **é¡¹ç›®è§„åˆ’** | `{subdirs['project']}/MILESTONE_PLAN.md` |")
        lines.append(f"| **æ ‡æ³¨å¤–åŒ…** | `{subdirs['annotation']}/ANNOTATION_SPEC.md` |")
        lines.append(f"| **æ ‡æ³¨åŸ¹è®­** | `{subdirs['annotation']}/TRAINING_GUIDE.md` |")
        lines.append(f"| **ç”Ÿäº§æµç¨‹** | `{subdirs['guide']}/PRODUCTION_SOP.md` |")
        lines.append(f"| **æ•°æ®æ¨¡æ¿** | `{subdirs['templates']}/data_template.json` |")
        lines.append(f"| **æˆæœ¬é¢„ç®—** | `{subdirs['cost']}/COST_BREAKDOWN.md` |")
        lines.append(f"| **AI Agent** | `{subdirs['ai_agent']}/agent_context.json` |")
        lines.append(f"| **æ ·ä¾‹æ•°æ®** | `{subdirs['samples']}/SAMPLE_GUIDE.md` |")
        lines.append("")
        lines.append("---")
        lines.append("")
        lines.append("> ç”± DataRecipe analyze-spec å‘½ä»¤ç”Ÿæˆ")

        path = os.path.join(output_dir, "README.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append("README.md")

    def _generate_training_guide(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
    ):
        """Generate TRAINING_GUIDE.md - annotator training manual."""
        lines = []
        lines.append(f"# {analysis.project_name} æ ‡æ³¨å‘˜åŸ¹è®­æ‰‹å†Œ")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append("")
        lines.append("---")
        lines.append("")

        # Section 1: Project Overview
        lines.append("## ä¸€ã€é¡¹ç›®æ¦‚è¿°")
        lines.append("")
        lines.append("### 1.1 é¡¹ç›®ç›®æ ‡")
        lines.append("")
        lines.append(f"{analysis.description or analysis.task_description}")
        lines.append("")

        lines.append("### 1.2 æ ¸å¿ƒèƒ½åŠ›è¦æ±‚")
        lines.append("")
        if analysis.cognitive_requirements:
            for req in analysis.cognitive_requirements:
                lines.append(f"- {req}")
        lines.append("")

        if analysis.reasoning_chain:
            lines.append("### 1.3 æ¨ç†é“¾")
            lines.append("")
            lines.append("```")
            lines.append(" â†’ ".join(analysis.reasoning_chain))
            lines.append("```")
            lines.append("")

        # Section 2: Data Requirements
        lines.append("---")
        lines.append("")
        lines.append("## äºŒã€æ•°æ®è¦æ±‚")
        lines.append("")

        if analysis.forbidden_items:
            lines.append("### 2.1 å¿…é¡»éµå®ˆçš„è§„åˆ™")
            lines.append("")
            for item in analysis.forbidden_items:
                lines.append(f"- âŒ {item}")
            lines.append("")

        if analysis.quality_constraints:
            lines.append("### 2.2 è´¨é‡æ ‡å‡†")
            lines.append("")
            for constraint in analysis.quality_constraints:
                lines.append(f"- âœ… {constraint}")
            lines.append("")

        # Section 3: Field Descriptions
        lines.append("---")
        lines.append("")
        lines.append("## ä¸‰ã€å­—æ®µè¯´æ˜")
        lines.append("")

        if analysis.fields:
            for f in analysis.fields:
                name = f.get("name", "")
                ftype = f.get("type", "string")
                required = "æ˜¯" if f.get("required", True) else "å¦"
                desc = f.get("description", "")
                lines.append(f"### {name}")
                lines.append("")
                lines.append(f"- **ç±»å‹**: {ftype}")
                lines.append(f"- **å¿…å¡«**: {required}")
                lines.append(f"- **è¯´æ˜**: {desc}")
                if analysis.field_requirements.get(name):
                    lines.append(f"- **å…·ä½“è¦æ±‚**: {analysis.field_requirements[name]}")
                lines.append("")

        # Section 4: Examples
        lines.append("---")
        lines.append("")
        lines.append("## å››ã€ç¤ºä¾‹è®²è§£")
        lines.append("")

        for i, example in enumerate(analysis.examples[:3], 1):
            lines.append(f"### 4.{i} ä¼˜ç§€ç¤ºä¾‹åˆ†æ")
            lines.append("")
            lines.append(f"#### ç¤ºä¾‹ {i}")
            lines.append("")

            if example.get("question"):
                lines.append(f"**é¢˜ç›®**: {example['question'][:100]}...")
                lines.append("")

            if example.get("answer"):
                lines.append(f"**ç­”æ¡ˆ**: {example['answer']}")
                lines.append("")

            if example.get("scoring_rubric"):
                lines.append(f"**è¯„åˆ†æ ‡å‡†**: {example['scoring_rubric']}")
                lines.append("")

            lines.append("**ä¼˜ç§€åŸå› **:")
            lines.append("- é¢˜æ„æ¸…æ™°ï¼Œæ— æ­§ä¹‰")
            lines.append("- ç­”æ¡ˆæ˜ç¡®")
            lines.append("- è¯„åˆ†æ ‡å‡†å…·ä½“")
            lines.append("")

        # Section 5: Common Errors
        lines.append("---")
        lines.append("")
        lines.append("## äº”ã€å¸¸è§é”™è¯¯")
        lines.append("")

        lines.append("### 5.1 é¢˜ç›®è®¾è®¡é”™è¯¯")
        lines.append("")
        lines.append("| é”™è¯¯ç±»å‹ | ç¤ºä¾‹ | æ­£ç¡®åšæ³• |")
        lines.append("|----------|------|----------|")
        lines.append("| é¢˜æ„æ¨¡ç³Š | \"æ‰¾æœ€å¥½çš„è·¯çº¿\" | \"æ‰¾è·ç¦»æœ€çŸ­çš„è·¯çº¿\" |")
        lines.append("| ä¿¡æ¯ä¸è¶³ | ç¼ºå°‘å…³é”®æ•°æ® | ç¡®ä¿æ‰€æœ‰å¿…è¦ä¿¡æ¯éƒ½å·²ç»™å‡º |")
        lines.append("| ç­”æ¡ˆä¸å”¯ä¸€ | æœªè¯´æ˜å¤šè§£æƒ…å†µ | åˆ—å‡ºæ‰€æœ‰æ­£ç¡®ç­”æ¡ˆ |")
        lines.append("")

        if analysis.has_images:
            lines.append("### 5.2 å›¾ç‰‡åˆ¶ä½œé”™è¯¯")
            lines.append("")
            lines.append("| é”™è¯¯ç±»å‹ | åæœ | é¿å…æ–¹æ³• |")
            lines.append("|----------|------|----------|")
            lines.append("| ä½¿ç”¨ AI ç”Ÿå›¾ | æ•°æ®ä½œåºŸï¼Œä¸æ”¯ä»˜è´¹ç”¨ | ä»…ä½¿ç”¨æ‰‹ç»˜/è½¯ä»¶ç»˜å›¾/ç…§ç‰‡ |")
            lines.append("| å›¾ç‰‡æ¨¡ç³Š | æ— æ³•è¯„æµ‹ | ç¡®ä¿åˆ†è¾¨ç‡ â‰¥ 800x600 |")
            lines.append("| å›¾æ–‡ä¸åŒ¹é… | é¢˜ç›®æ— æ•ˆ | æ ¸å¯¹å›¾ç‰‡ä¸é¢˜ç›®æè¿°ä¸€è‡´ |")
            lines.append("")

        lines.append("### 5.3 è¯„åˆ†æ ‡å‡†é”™è¯¯")
        lines.append("")
        lines.append("| é”™è¯¯ç±»å‹ | ç¤ºä¾‹ | æ­£ç¡®åšæ³• |")
        lines.append("|----------|------|----------|")
        lines.append("| æ ‡å‡†æ¨¡ç³Š | \"å›ç­”æ­£ç¡®å¾—åˆ†\" | æ˜ç¡®ä»€ä¹ˆæ ·çš„å›ç­”ç®—æ­£ç¡® |")
        lines.append("| é—æ¼æƒ…å†µ | åªå†™æ»¡åˆ†æ¡ä»¶ | åŒ…å«æ»¡åˆ†ã€éƒ¨åˆ†åˆ†ã€é›¶åˆ†æ¡ä»¶ |")
        lines.append("")

        # Section 6: Self-Check List
        lines.append("---")
        lines.append("")
        lines.append("## å…­ã€è‡ªæ£€æ¸…å•")
        lines.append("")
        lines.append("æäº¤å‰è¯·é€é¡¹æ£€æŸ¥ï¼š")
        lines.append("")

        if analysis.has_images:
            lines.append("### å›¾ç‰‡")
            lines.append("- [ ] é AI ç”Ÿæˆ")
            lines.append("- [ ] æ¸…æ™°åº¦è¾¾æ ‡")
            lines.append("- [ ] ä¸é¢˜ç›®å¼ºç›¸å…³")
            lines.append("")

        lines.append("### é¢˜ç›®")
        lines.append("- [ ] é¢˜æ„æ¸…æ™°")
        lines.append("- [ ] æ— æ­§ä¹‰")
        lines.append("- [ ] æ ¼å¼è¦æ±‚æ˜ç¡®")
        lines.append("")

        lines.append("### ç­”æ¡ˆ")
        lines.append("- [ ] ç­”æ¡ˆæ­£ç¡®")
        lines.append("- [ ] å¤šè§£å·²å…¨éƒ¨åˆ—å‡º")
        lines.append("- [ ] æ ¼å¼ç¬¦åˆè¦æ±‚")
        lines.append("")

        lines.append("### è§£æ")
        lines.append("- [ ] æ­¥éª¤å®Œæ•´")
        lines.append("- [ ] é€»è¾‘æ¸…æ™°")
        lines.append("- [ ] ä¸ç­”æ¡ˆä¸€è‡´")
        lines.append("")

        lines.append("### è¯„åˆ†æ ‡å‡†")
        lines.append("- [ ] åŒ…å«æ»¡åˆ†æ¡ä»¶")
        lines.append("- [ ] åŒ…å«é›¶åˆ†æ¡ä»¶")
        lines.append("- [ ] æ¡ä»¶å…·ä½“å¯åˆ¤")
        lines.append("")

        # Add difficulty validation to checklist if enabled
        if analysis.has_difficulty_validation():
            diff_val = analysis.difficulty_validation
            lines.append("### éš¾åº¦éªŒè¯")
            lines.append(f"- [ ] å·²å®Œæˆ {diff_val.get('test_count', 3)} æ¬¡æµ‹è¯•")
            lines.append(f"- [ ] æ­£ç¡®æ¬¡æ•° â‰¤ {diff_val.get('max_correct', 1)}")
            lines.append("- [ ] è®°å½•å·²ä¿å­˜")
            lines.append("")

        # FAQ
        lines.append("---")
        lines.append("")
        lines.append("## ä¸ƒã€FAQ")
        lines.append("")
        lines.append("**Q: å›¾ç‰‡å¯ä»¥ç”¨ç½‘ä¸Šä¸‹è½½çš„å—ï¼Ÿ**")
        lines.append("A: ä¸å¯ä»¥ï¼Œå­˜åœ¨ç‰ˆæƒé£é™©ï¼Œä¸”å¯èƒ½è¢« AI è¯†åˆ«ã€‚è¯·ä½¿ç”¨åŸåˆ›å›¾ç‰‡ã€‚")
        lines.append("")

        if analysis.has_difficulty_validation():
            diff_val = analysis.difficulty_validation
            max_correct = diff_val.get("max_correct", 1)
            lines.append(f"**Q: å¦‚æœæ¨¡å‹ç­”å¯¹ {max_correct + 1} æ¬¡æ€ä¹ˆåŠï¼Ÿ**")
            lines.append("A: é¢˜ç›®æ— æ•ˆï¼Œéœ€è¦å¢åŠ éš¾åº¦åé‡æ–°éªŒè¯ã€‚")
            lines.append("")

        lines.append("**Q: è¯„åˆ†æ ‡å‡†å¿…é¡»æ˜¯ 1 åˆ†å’Œ 0 åˆ†å—ï¼Ÿ**")
        lines.append("A: å¯ä»¥æœ‰éƒ¨åˆ†å¾—åˆ†ï¼ˆå¦‚ 0.5 åˆ†ï¼‰ï¼Œä½†éœ€æ˜ç¡®è¯´æ˜æ¡ä»¶ã€‚")
        lines.append("")

        lines.append("**Q: å¤šè½®å¯¹è¯é¢˜ç›®æ€ä¹ˆå¤„ç†ï¼Ÿ**")
        lines.append("A: æ¯è½®ä½œä¸ºç‹¬ç«‹å­—æ®µï¼Œæ˜ç¡®æ ‡æ³¨è½®æ¬¡å’Œå‰åä¾èµ–å…³ç³»ã€‚")
        lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> æœ¬æ‰‹å†Œç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ")

        path = os.path.join(output_dir, subdirs["annotation"], "TRAINING_GUIDE.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(f"{subdirs['annotation']}/TRAINING_GUIDE.md")

    def _generate_qa_checklist(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
    ):
        """Generate QA_CHECKLIST.md - quality assurance checklist."""
        lines = []
        lines.append(f"# {analysis.project_name} è´¨é‡æ£€æŸ¥æ¸…å•")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append("")
        lines.append("---")
        lines.append("")

        # Section 1: Single Data Check
        lines.append("## ä¸€ã€å•æ¡æ•°æ®æ£€æŸ¥")
        lines.append("")

        if analysis.has_images:
            lines.append("### 1.1 å›¾ç‰‡æ£€æŸ¥")
            lines.append("")
            lines.append("| æ£€æŸ¥é¡¹ | é€šè¿‡æ ‡å‡† | æ£€æŸ¥ç»“æœ |")
            lines.append("|--------|----------|----------|")
            lines.append("| åŸåˆ›æ€§ | é AI ç”Ÿæˆï¼Œæ— ç‰ˆæƒé—®é¢˜ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
            lines.append("| æ¸…æ™°åº¦ | åˆ†è¾¨ç‡ â‰¥ 800x600ï¼Œæ–‡å­—å¯è¯» | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
            lines.append("| ç›¸å…³æ€§ | å›¾ç‰‡ä¸é¢˜ç›®å¼ºç›¸å…³ï¼Œæ— å›¾æ— æ³•è§£é¢˜ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
            lines.append("| æ ¼å¼ | PNG/JPGï¼Œå¤§å° â‰¤ 5MB | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
            lines.append("")

        lines.append("### 1.2 é¢˜ç›®æ£€æŸ¥")
        lines.append("")
        lines.append("| æ£€æŸ¥é¡¹ | é€šè¿‡æ ‡å‡† | æ£€æŸ¥ç»“æœ |")
        lines.append("|--------|----------|----------|")
        lines.append("| æ¸…æ™°åº¦ | é¢˜æ„æ˜ç¡®ï¼Œæ— æ­§ä¹‰ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("| å®Œæ•´æ€§ | è§£é¢˜æ‰€éœ€ä¿¡æ¯å®Œæ•´ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("| åŸåˆ›æ€§ | é AI ç”Ÿæˆ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("| æ ¼å¼è¦æ±‚ | å·²è¯´æ˜ç­”æ¡ˆè¾“å‡ºæ ¼å¼ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("")

        lines.append("### 1.3 ç­”æ¡ˆæ£€æŸ¥")
        lines.append("")
        lines.append("| æ£€æŸ¥é¡¹ | é€šè¿‡æ ‡å‡† | æ£€æŸ¥ç»“æœ |")
        lines.append("|--------|----------|----------|")
        lines.append("| æ­£ç¡®æ€§ | ç­”æ¡ˆæ­£ç¡®ï¼Œå¯éªŒè¯ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("| å®Œæ•´æ€§ | å¤šè§£å·²å…¨éƒ¨åˆ—å‡º | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("| æ ¼å¼ | ç¬¦åˆé¢˜ç›®è¦æ±‚çš„æ ¼å¼ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("")

        lines.append("### 1.4 è§£ææ£€æŸ¥")
        lines.append("")
        lines.append("| æ£€æŸ¥é¡¹ | é€šè¿‡æ ‡å‡† | æ£€æŸ¥ç»“æœ |")
        lines.append("|--------|----------|----------|")
        lines.append("| å®Œæ•´æ€§ | æ­¥éª¤å®Œæ•´ï¼Œä»é¢˜ç›®åˆ°ç­”æ¡ˆ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("| æ­£ç¡®æ€§ | é€»è¾‘æ­£ç¡®ï¼Œä¸ç­”æ¡ˆä¸€è‡´ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("| åŸåˆ›æ€§ | é AI ç”Ÿæˆ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("")

        lines.append("### 1.5 è¯„åˆ†æ ‡å‡†æ£€æŸ¥")
        lines.append("")
        lines.append("| æ£€æŸ¥é¡¹ | é€šè¿‡æ ‡å‡† | æ£€æŸ¥ç»“æœ |")
        lines.append("|--------|----------|----------|")
        lines.append("| æ»¡åˆ†æ¡ä»¶ | å·²æ˜ç¡®è¯´æ˜ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("| é›¶åˆ†æ¡ä»¶ | å·²æ˜ç¡®è¯´æ˜ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("| å¯æ“ä½œæ€§ | æ¡ä»¶å…·ä½“ï¼Œå¯å®¢è§‚åˆ¤å®š | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("")

        # Difficulty validation check if enabled
        if analysis.has_difficulty_validation():
            diff_val = analysis.difficulty_validation
            lines.append("### 1.6 éš¾åº¦éªŒè¯æ£€æŸ¥")
            lines.append("")
            lines.append("| æ£€æŸ¥é¡¹ | é€šè¿‡æ ‡å‡† | æ£€æŸ¥ç»“æœ |")
            lines.append("|--------|----------|----------|")
            lines.append(f"| æµ‹è¯•æ¬¡æ•° | å·²å®Œæˆ {diff_val.get('test_count', 3)} æ¬¡æµ‹è¯• | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
            lines.append(f"| æ­£ç¡®æ¬¡æ•° | â‰¤ {diff_val.get('max_correct', 1)} æ¬¡ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
            lines.append("| è®°å½•å®Œæ•´ | ä¸‰æ¬¡å›ç­”å’Œåˆ¤å®šéƒ½æœ‰è®°å½• | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
            lines.append("")

        # Section 2: Batch Check
        lines.append("---")
        lines.append("")
        lines.append("## äºŒã€æ‰¹é‡æ•°æ®æ£€æŸ¥")
        lines.append("")

        lines.append("### 2.1 æ•°æ®å®Œæ•´æ€§")
        lines.append("")
        lines.append("| æ£€æŸ¥é¡¹ | é€šè¿‡æ ‡å‡† | æ£€æŸ¥ç»“æœ |")
        lines.append("|--------|----------|----------|")
        lines.append("| æ•°é‡ | è¾¾åˆ°ç›®æ ‡æ•°é‡ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("| å¿…å¡«å­—æ®µ | æ‰€æœ‰å¿…å¡«å­—æ®µå·²å¡«å†™ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        if analysis.has_images:
            lines.append("| å›¾ç‰‡æ–‡ä»¶ | æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—® | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("")

        lines.append("### 2.2 æ•°æ®æ ¼å¼")
        lines.append("")
        lines.append("| æ£€æŸ¥é¡¹ | é€šè¿‡æ ‡å‡† | æ£€æŸ¥ç»“æœ |")
        lines.append("|--------|----------|----------|")
        lines.append("| JSON æœ‰æ•ˆæ€§ | JSON æ ¼å¼æ­£ç¡®ï¼Œå¯è§£æ | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("| Schema ç¬¦åˆ | ç¬¦åˆ DATA_SCHEMA.json | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("| ç¼–ç æ ¼å¼ | UTF-8 ç¼–ç  | â˜ é€šè¿‡ â˜ ä¸é€šè¿‡ |")
        lines.append("")

        lines.append("### 2.3 æŠ½æ£€ç»Ÿè®¡")
        lines.append("")
        lines.append("| æŒ‡æ ‡ | ç›®æ ‡ | å®é™… | è¾¾æ ‡ |")
        lines.append("|------|------|------|------|")
        lines.append("| æŠ½æ£€ç‡ | â‰¥ 20% | ___ % | â˜ |")
        lines.append("| é€šè¿‡ç‡ | â‰¥ 95% | ___ % | â˜ |")
        lines.append("| è¿”å·¥ç‡ | â‰¤ 10% | ___ % | â˜ |")
        lines.append("")

        # Section 3: Review Process
        lines.append("---")
        lines.append("")
        lines.append("## ä¸‰ã€å®¡æ ¸æµç¨‹")
        lines.append("")

        lines.append("### 3.1 è‡ªæ£€ï¼ˆç”Ÿäº§è€…ï¼‰")
        lines.append("")
        lines.append("```")
        lines.append("å®Œæˆæ•°æ® â†’ å¯¹ç…§æ¸…å•è‡ªæ£€ â†’ ä¿®æ­£é—®é¢˜ â†’ æäº¤äº’å®¡")
        lines.append("```")
        lines.append("")

        lines.append("### 3.2 äº’å®¡ï¼ˆåŒçº§ï¼‰")
        lines.append("")
        lines.append("```")
        lines.append("æ¥æ”¶æ•°æ® â†’ äº¤å‰æ£€æŸ¥ â†’ æ ‡è®°é—®é¢˜ â†’ åé¦ˆ/é€šè¿‡")
        lines.append("```")
        lines.append("")

        lines.append("### 3.3 ä¸“å®¶æŠ½æ£€ï¼ˆQAï¼‰")
        lines.append("")
        lines.append("```")
        lines.append("éšæœºæŠ½å– 20% â†’ æ·±åº¦æ£€æŸ¥ â†’ æ±‡æ€»é—®é¢˜ â†’ åé¦ˆ/ç»ˆå®¡")
        lines.append("```")
        lines.append("")

        # Section 4: Issue Tracking
        lines.append("---")
        lines.append("")
        lines.append("## å››ã€é—®é¢˜è®°å½•è¡¨")
        lines.append("")
        lines.append("| é¢˜ç›®ID | é—®é¢˜ç±»å‹ | é—®é¢˜æè¿° | ä¸¥é‡ç¨‹åº¦ | å¤„ç†çŠ¶æ€ |")
        lines.append("|--------|----------|----------|----------|----------|")
        lines.append("| | | | â˜é«˜ â˜ä¸­ â˜ä½ | â˜å¾…ä¿® â˜å·²ä¿® â˜å·²éªŒ |")
        lines.append("| | | | â˜é«˜ â˜ä¸­ â˜ä½ | â˜å¾…ä¿® â˜å·²ä¿® â˜å·²éªŒ |")
        lines.append("| | | | â˜é«˜ â˜ä¸­ â˜ä½ | â˜å¾…ä¿® â˜å·²ä¿® â˜å·²éªŒ |")
        lines.append("")

        # Section 5: Acceptance Criteria
        lines.append("---")
        lines.append("")
        lines.append("## äº”ã€éªŒæ”¶æ ‡å‡†")
        lines.append("")
        lines.append("| æŒ‡æ ‡ | é˜ˆå€¼ | è¯´æ˜ |")
        lines.append("|------|------|------|")
        lines.append("| æ•°æ®å®Œæ•´ç‡ | 100% | æ‰€æœ‰å¿…å¡«å­—æ®µå®Œæ•´ |")
        lines.append("| ä¸“å®¶å®¡æ ¸é€šè¿‡ç‡ | â‰¥ 95% | æŠ½æ£€é€šè¿‡æ¯”ä¾‹ |")
        if analysis.has_difficulty_validation():
            lines.append("| éš¾åº¦éªŒè¯é€šè¿‡ç‡ | 100% | æ‰€æœ‰æ•°æ®é€šè¿‡æ¨¡å‹éªŒè¯ |")
        lines.append("| æ ¼å¼æ­£ç¡®ç‡ | 100% | JSON æ ¼å¼å’Œ Schema ç¬¦åˆ |")
        lines.append("")

        # Section: Structured field constraints (Upgrade 6)
        constraints = analysis.parsed_constraints
        if constraints:
            lines.append("---")
            lines.append("")
            lines.append("## å…­ã€ç»“æ„åŒ–å­—æ®µçº¦æŸ")
            lines.append("")
            # Group by field_name
            from collections import defaultdict
            by_field: dict = defaultdict(list)
            for c in constraints:
                by_field[c.field_name].append(c)

            for fname, fcs in by_field.items():
                display_name = fname if fname != "_global" else "å…¨å±€çº¦æŸ"
                lines.append(f"### {display_name}")
                lines.append("")
                lines.append("| çº¦æŸç±»å‹ | è§„åˆ™ | ä¸¥é‡çº§åˆ« | å¯è‡ªåŠ¨æ£€æŸ¥ |")
                lines.append("|----------|------|----------|------------|")
                for c in fcs:
                    auto = "æ˜¯" if c.auto_checkable else "å¦"
                    lines.append(f"| {c.constraint_type} | {c.rule} | {c.severity} | {auto} |")
                lines.append("")

        # Section: Quality gates (Upgrade 4)
        if analysis.quality_gates:
            lines.append("---")
            lines.append("")
            lines.append("## ä¸ƒã€è´¨é‡é—¨ç¦")
            lines.append("")
            lines.append("| é—¨ç¦ | æŒ‡æ ‡ | æ¡ä»¶ | é˜ˆå€¼ | çº§åˆ« |")
            lines.append("|------|------|------|------|------|")
            for gate in analysis.quality_gates:
                lines.append(
                    f"| {gate.get('name', gate.get('gate_id', ''))} "
                    f"| {gate.get('metric', '')} "
                    f"| {gate.get('operator', '')} "
                    f"| {gate.get('threshold', '')} "
                    f"| {gate.get('severity', 'blocker')} |"
                )
            lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> æœ¬æ£€æŸ¥æ¸…å•ç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ")

        path = os.path.join(output_dir, subdirs["annotation"], "QA_CHECKLIST.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(f"{subdirs['annotation']}/QA_CHECKLIST.md")

    def _generate_difficulty_validation(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
    ):
        """Generate DIFFICULTY_VALIDATION.md - only when difficulty validation is configured."""
        diff_val = analysis.difficulty_validation
        if not diff_val:
            return

        model_name = diff_val.get("model", "æœªæŒ‡å®šæ¨¡å‹")
        settings = diff_val.get("settings", "é»˜è®¤è®¾ç½®")
        test_count = diff_val.get("test_count", 3)
        max_correct = diff_val.get("max_correct", 1)
        pass_criteria = diff_val.get("pass_criteria", f"è·‘ {test_count} æ¬¡ï¼Œæ­£ç¡®æ¬¡æ•° â‰¤ {max_correct} æ¬¡")

        lines = []
        lines.append(f"# {analysis.project_name} éš¾åº¦éªŒè¯æµç¨‹")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append("")
        lines.append("---")
        lines.append("")

        # Section 1: Purpose
        lines.append("## ä¸€ã€éªŒè¯ç›®çš„")
        lines.append("")
        lines.append("ç¡®ä¿é¢˜ç›®å¯¹å½“å‰ä¸»æµå¤§æ¨¡å‹å…·æœ‰è¶³å¤Ÿéš¾åº¦ï¼Œé¿å…ç”Ÿäº§æ— æ•ˆæ•°æ®ã€‚")
        lines.append("")
        lines.append(f"**æœ‰æ•ˆæ•°æ®æ ‡å‡†ï¼š** {model_name} {settings}è·‘ {test_count} æ¬¡ï¼Œæ­£ç¡®æ¬¡æ•° â‰¤ {max_correct} æ¬¡")
        lines.append("")

        # Section 2: Environment Setup
        lines.append("---")
        lines.append("")
        lines.append("## äºŒã€éªŒè¯ç¯å¢ƒé…ç½®")
        lines.append("")

        lines.append("### 2.1 æ¨¡å‹è®¾ç½®")
        lines.append("")
        lines.append("| é…ç½®é¡¹ | å€¼ |")
        lines.append("|--------|-----|")
        lines.append(f"| æ¨¡å‹åç§° | {model_name} |")
        lines.append(f"| é…ç½®è®¾ç½® | {settings} |")
        lines.append("| æ¸©åº¦ | é»˜è®¤ |")
        lines.append("| æœ€å¤§ token | é»˜è®¤ |")
        lines.append("")

        lines.append("### 2.2 æµ‹è¯•å¹³å°")
        lines.append("")
        lines.append("æ¨èä½¿ç”¨ä»¥ä¸‹å¹³å°è¿›è¡Œæµ‹è¯•ï¼š")
        lines.append(f"- {model_name} å®˜æ–¹ Web ç•Œé¢")
        lines.append("- API è°ƒç”¨ï¼ˆå¦‚æœ‰æƒé™ï¼‰")
        lines.append("")

        # Section 3: Validation Process
        lines.append("---")
        lines.append("")
        lines.append("## ä¸‰ã€éªŒè¯æµç¨‹")
        lines.append("")

        lines.append("### æ­¥éª¤ 1: å‡†å¤‡æµ‹è¯•è¾“å…¥")
        lines.append("")
        if analysis.has_images:
            lines.append("å°†é¢˜ç›®å›¾ç‰‡å’Œæ–‡å­—ç»„åˆæˆå®Œæ•´çš„ promptï¼š")
            lines.append("")
            lines.append("```")
            lines.append("[ä¸Šä¼ å›¾ç‰‡]")
            lines.append("")
            lines.append("{é¢˜ç›®æ–‡å­—}")
            lines.append("```")
        else:
            lines.append("å‡†å¤‡å®Œæ•´çš„é¢˜ç›®æ–‡å­—ä½œä¸º promptã€‚")
        lines.append("")

        lines.append(f"### æ­¥éª¤ 2: æ‰§è¡Œæµ‹è¯•ï¼ˆ{test_count}æ¬¡ï¼‰")
        lines.append("")
        for i in range(1, test_count + 1):
            lines.append(f"{i}. **ç¬¬{i}æ¬¡æµ‹è¯•**" + ("ï¼ˆæ–°å¯¹è¯ï¼‰" if i > 1 else ""))
            if i > 1:
                lines.append("   - å¼€å¯æ–°å¯¹è¯")
            lines.append("   - å‘é€ prompt")
            lines.append("   - ç­‰å¾…æ¨¡å‹å›å¤")
            lines.append("   - è®°å½•å®Œæ•´å›ç­”")
            lines.append("   - åˆ¤å®šæ­£ç¡®/é”™è¯¯")
            lines.append("")

        lines.append("### æ­¥éª¤ 3: åˆ¤å®šç»“æœ")
        lines.append("")
        lines.append("| æ­£ç¡®æ¬¡æ•° | åˆ¤å®š | å¤„ç† |")
        lines.append("|----------|------|------|")
        for i in range(test_count + 1):
            if i <= max_correct:
                lines.append(f"| {i} æ¬¡ | âœ… æœ‰æ•ˆ | å¯æäº¤ |")
            else:
                lines.append(f"| {i} æ¬¡ | âŒ æ— æ•ˆ | éœ€ä¿®æ”¹ |")
        lines.append("")

        # Section 4: Recording Template
        lines.append("---")
        lines.append("")
        lines.append("## å››ã€è®°å½•æ¨¡æ¿")
        lines.append("")

        lines.append("### 4.1 å•é¢˜æµ‹è¯•è®°å½•")
        lines.append("")
        lines.append("```markdown")
        lines.append("## é¢˜ç›® ID: [XXX]")
        lines.append("")
        lines.append("### æµ‹è¯•é…ç½®")
        lines.append(f"- æ¨¡å‹: {model_name}")
        lines.append(f"- é…ç½®: {settings}")
        lines.append("- æµ‹è¯•æ—¥æœŸ: YYYY-MM-DD")
        lines.append("")
        lines.append("### æµ‹è¯•ç»“æœ")
        lines.append("")
        for i in range(1, test_count + 1):
            lines.append(f"**ç¬¬ {i} æ¬¡æµ‹è¯•ï¼š**")
            lines.append("- æ¨¡å‹å›ç­”: [å®Œæ•´å›ç­”]")
            lines.append("- åˆ¤å®š: âœ…æ­£ç¡® / âŒé”™è¯¯")
            lines.append("- åŸå› : [ç®€è¦è¯´æ˜]")
            lines.append("")
        lines.append("### æœ€ç»ˆåˆ¤å®š")
        lines.append(f"- æ­£ç¡®æ¬¡æ•°: X/{test_count}")
        lines.append("- æœ‰æ•ˆæ€§: âœ…æœ‰æ•ˆ / âŒæ— æ•ˆ")
        lines.append("```")
        lines.append("")

        lines.append("### 4.2 æ‰¹é‡è®°å½•è¡¨æ ¼")
        lines.append("")
        header = "| é¢˜ç›®ID |"
        separator = "|--------|"
        for i in range(1, test_count + 1):
            header += f" æµ‹è¯•{i} |"
            separator += "-------|"
        header += " æ­£ç¡®æ•° | æœ‰æ•ˆæ€§ |"
        separator += "--------|--------|"
        lines.append(header)
        lines.append(separator)

        # Example rows
        lines.append("| 001 |" + " âŒ |" * test_count + " 0 | âœ… |")
        if max_correct >= 1:
            lines.append("| 002 |" + " âŒ |" * (test_count - 1) + " âœ… | 1 | âœ… |")
        if test_count > 2:
            lines.append("| 003 |" + " âœ… |" * 2 + " âŒ |" * (test_count - 2) + f" 2 | {'âŒ' if max_correct < 2 else 'âœ…'} |")
        lines.append("")

        # Section 5: Handling Invalid Questions
        lines.append("---")
        lines.append("")
        lines.append("## äº”ã€æ— æ•ˆé¢˜ç›®å¤„ç†")
        lines.append("")

        lines.append("### 5.1 å¸¸è§åŸå› ")
        lines.append("")
        lines.append("1. **é¢˜ç›®è¿‡äºç®€å•**ï¼šæ¨ç†æ­¥éª¤å°‘ï¼Œæ¨¡å‹å®¹æ˜“çŒœå¯¹")
        lines.append("2. **è§„åˆ™ä¸å¤Ÿå¤æ‚**ï¼šè§„åˆ™ç®€å•ï¼Œæ¨¡å‹èƒ½è½»æ¾ç†è§£")
        lines.append("3. **ç­”æ¡ˆé€‰é¡¹æœ‰é™**ï¼šç­”æ¡ˆç©ºé—´å°ï¼ŒçŒœä¸­æ¦‚ç‡é«˜")
        lines.append("")

        lines.append("### 5.2 ä¿®æ”¹ç­–ç•¥")
        lines.append("")
        lines.append("| é—®é¢˜ | ä¿®æ”¹æ–¹å‘ |")
        lines.append("|------|----------|")
        lines.append("| æ¨ç†æ­¥éª¤å°‘ | å¢åŠ ä¸­é—´æ­¥éª¤ï¼ŒåµŒå¥—æ›´å¤šè§„åˆ™ |")
        lines.append("| è§„åˆ™ç®€å• | æ·»åŠ ä¾‹å¤–æ¡ä»¶ã€ç‰¹æ®Šæƒ…å†µ |")
        lines.append("| ç­”æ¡ˆç©ºé—´å° | è®¾è®¡å¼€æ”¾å¼é—®é¢˜ï¼Œå¢åŠ è®¡ç®—é‡ |")
        if analysis.has_images:
            lines.append("| å›¾æ–‡ä¿¡æ¯å°‘ | å¢åŠ å›¾ä¸­ä¿¡æ¯é‡ï¼Œå‡å°‘æ–‡å­—æç¤º |")
        lines.append("")

        lines.append("### 5.3 ä¿®æ”¹åé‡æ–°éªŒè¯")
        lines.append("")
        lines.append(f"ä¿®æ”¹åå¿…é¡»é‡æ–°æ‰§è¡Œå®Œæ•´çš„ {test_count} æ¬¡æµ‹è¯•æµç¨‹ã€‚")
        lines.append("")

        # Section 6: Notes
        lines.append("---")
        lines.append("")
        lines.append("## å…­ã€æ³¨æ„äº‹é¡¹")
        lines.append("")
        lines.append("1. **æ¯æ¬¡æµ‹è¯•ä½¿ç”¨æ–°å¯¹è¯**ï¼šé¿å…ä¸Šä¸‹æ–‡å½±å“")
        lines.append("2. **ä¿æŒ prompt ä¸€è‡´**ï¼šä¸‰æ¬¡æµ‹è¯•ä½¿ç”¨å®Œå…¨ç›¸åŒçš„è¾“å…¥")
        lines.append("3. **å®¢è§‚åˆ¤å®š**ï¼šæ ¹æ®è¯„åˆ†æ ‡å‡†åˆ¤å®šï¼Œä¸ä¸»è§‚æ”¾å®½")
        lines.append("4. **ä¿ç•™è®°å½•**ï¼šæ‰€æœ‰æµ‹è¯•è®°å½•éœ€ä¿ç•™ç”¨äºäº¤ä»˜")
        lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> æœ¬æµç¨‹ç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ")

        path = os.path.join(output_dir, subdirs["guide"], "DIFFICULTY_VALIDATION.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(f"{subdirs['guide']}/DIFFICULTY_VALIDATION.md")

    def _generate_validation_guide(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
    ):
        """Generate validation guides for all validation strategies.

        For model_test, delegates to the existing _generate_difficulty_validation.
        For other strategy types, generates a corresponding guide document.
        """
        strategies = analysis.parsed_validation_strategies
        if not strategies:
            return

        for strategy in strategies:
            if strategy.strategy_type == "model_test":
                self._generate_difficulty_validation(analysis, output_dir, subdirs, result)
            else:
                lines = []
                lines.append(f"# {analysis.project_name} éªŒè¯æµç¨‹ - {strategy.strategy_type}")
                lines.append("")
                lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
                lines.append(f"> ç­–ç•¥ç±»å‹: {strategy.strategy_type}")
                lines.append("")
                lines.append("---")
                lines.append("")
                lines.append("## éªŒè¯è¯´æ˜")
                lines.append("")
                lines.append(f"{strategy.description or 'è¯·æŒ‰ä»¥ä¸‹æµç¨‹æ‰§è¡ŒéªŒè¯ã€‚'}")
                lines.append("")

                if strategy.config:
                    lines.append("## é…ç½®å‚æ•°")
                    lines.append("")
                    lines.append("| å‚æ•° | å€¼ |")
                    lines.append("|------|-----|")
                    for k, v in strategy.config.items():
                        lines.append(f"| {k} | {v} |")
                    lines.append("")

                if strategy.strategy_type == "human_review":
                    lines.append("## æµç¨‹")
                    lines.append("")
                    lines.append("1. éšæœºæŠ½å–æŒ‡å®šæ¯”ä¾‹çš„æ•°æ®")
                    lines.append("2. ç”±å®¡æ ¸å‘˜æŒ‰æ ‡æ³¨è§„èŒƒé€æ¡æ£€æŸ¥")
                    lines.append("3. è®°å½•é—®é¢˜å¹¶åé¦ˆä¿®æ”¹")
                    lines.append("4. è¾¾åˆ°é€šè¿‡ç‡é˜ˆå€¼åæ”¾è¡Œ")
                    lines.append("")
                elif strategy.strategy_type == "format_check":
                    lines.append("## æµç¨‹")
                    lines.append("")
                    lines.append("1. ä½¿ç”¨ DATA_SCHEMA.json æ ¡éªŒæ¯æ¡æ•°æ®æ ¼å¼")
                    lines.append("2. æ£€æŸ¥å¿…å¡«å­—æ®µã€ç±»å‹ã€é•¿åº¦ç­‰çº¦æŸ")
                    lines.append("3. è‡ªåŠ¨è¿‡æ»¤ä¸ç¬¦åˆæ ¼å¼çš„æ•°æ®")
                    lines.append("")
                elif strategy.strategy_type == "cross_validation":
                    lines.append("## æµç¨‹")
                    lines.append("")
                    lines.append("1. åŒä¸€æ•°æ®ç”±å¤šåæ ‡æ³¨å‘˜ç‹¬ç«‹æ ‡æ³¨")
                    lines.append("2. è®¡ç®—æ ‡æ³¨è€…é—´ä¸€è‡´æ€§ (Cohen's Kappa)")
                    lines.append("3. å¯¹ä½ä¸€è‡´æ€§æ•°æ®è¿›è¡Œä»²è£")
                    lines.append("")
                elif strategy.strategy_type == "auto_scoring":
                    lines.append("## æµç¨‹")
                    lines.append("")
                    lines.append("1. ä½¿ç”¨é¢„å®šä¹‰è¯„åˆ†å‡½æ•°è‡ªåŠ¨æ‰“åˆ†")
                    lines.append("2. ä½äºé˜ˆå€¼çš„æ•°æ®æ ‡è®°ä¸ºå¾…å®¡æ ¸")
                    lines.append("3. äººå·¥å¤æ ¸æ ‡è®°æ•°æ®")
                    lines.append("")

                lines.append("---")
                lines.append("")
                lines.append("> æœ¬éªŒè¯æµç¨‹ç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ")

                safe_type = strategy.strategy_type.upper()
                filename = f"VALIDATION_{safe_type}.md"
                path = os.path.join(output_dir, subdirs["guide"], filename)
                with open(path, "w", encoding="utf-8") as f:
                    f.write("\n".join(lines))
                result.files_generated.append(f"{subdirs['guide']}/{filename}")

    # --- Schema-driven helpers ---

    def _template_placeholder(self, fd: FieldDefinition) -> Any:
        """Generate a template placeholder for a FieldDefinition (mode=template)."""
        return self._generate_value_from_field(fd, mode="template")

    def _sample_placeholder(self, fd: FieldDefinition, context: dict) -> Any:
        """Generate a sample placeholder for a FieldDefinition (mode=sample)."""
        return self._generate_value_from_field(fd, mode="sample", context=context)

    def _generate_value_from_field(
        self, fd: FieldDefinition, mode: str = "template", context: Optional[dict] = None
    ) -> Any:
        """Recursively generate a value from a FieldDefinition.

        Args:
            fd: field definition
            mode: "template" (placeholder text) or "sample" (example data)
            context: optional dict with task_type, sample_index etc.
        """
        from datarecipe.analyzers.spec_analyzer import _map_type
        json_type = _map_type(fd.type)

        # Enum â†’ pick first value for template, vary for sample
        if fd.enum:
            if mode == "template":
                return fd.enum[0] if fd.enum else ""
            else:
                idx = (context or {}).get("sample_index", 0)
                return fd.enum[idx % len(fd.enum)]

        # Object with properties â†’ recurse
        if json_type == "object" and fd.properties:
            obj = {}
            for p in fd.properties:
                obj[p.name] = self._generate_value_from_field(p, mode=mode, context=context)
            return obj

        # Array with items â†’ wrap one example item
        if json_type == "array" and fd.items:
            item_val = self._generate_value_from_field(fd.items, mode=mode, context=context)
            return [item_val]

        # Scalar types
        desc = fd.description or fd.name
        if json_type == "string":
            if mode == "template":
                return f"[è¯·å¡«å†™: {desc}]"
            else:
                return f"[{desc}]"
        elif json_type in ("number", "integer"):
            return 0
        elif json_type == "boolean":
            return True
        elif json_type == "array":
            return []
        elif json_type == "object":
            return {}
        return f"[{fd.type}: {desc}]"

    def _generate_data_template(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
    ):
        """Generate data_template.json â€” schema-driven single data entry template."""
        template: Dict[str, Any] = {"id": "EXAMPLE_001"}

        # Use field_definitions if available, else fall back to profile defaults
        field_defs = analysis.field_definitions
        if not field_defs:
            profile = get_task_profile(analysis.dataset_type)
            field_defs = [FieldDefinition.from_dict(f) for f in profile.default_fields]

        for fd in field_defs:
            if fd.name == "id":
                continue
            template[fd.name] = self._template_placeholder(fd)

        # Add image field if needed and not already present
        if analysis.has_images and "image" not in template:
            template["image"] = {
                "path": "images/example_001.png",
                "type": "software",
                "description": "åŒ…å«è§„åˆ™å®šä¹‰å’Œå¾…è§£å†³é—®é¢˜çš„å›¾ç¤º",
            }

        # Add model test section if difficulty validation is enabled
        if analysis.has_difficulty_validation():
            diff_val = analysis.difficulty_validation or {}
            model_name = diff_val.get("model", "æœªæŒ‡å®šæ¨¡å‹")
            settings = diff_val.get("settings", "é»˜è®¤è®¾ç½®")
            test_count = diff_val.get("test_count", 3)
            template["model_test"] = {
                "model": model_name,
                "settings": settings,
                "results": [
                    {"attempt": i, "response": f"æ¨¡å‹ç¬¬{i}æ¬¡çš„å›ç­”...", "is_correct": i == test_count}
                    for i in range(1, test_count + 1)
                ],
                "valid": True,
            }

        template["metadata"] = {
            "category": analysis.estimated_domain or analysis.dataset_type,
            "difficulty": analysis.estimated_difficulty,
            "created_by": "æ ‡æ³¨å‘˜å§“å",
            "created_at": datetime.now().strftime("%Y-%m-%d"),
            "reviewed_by": "",
            "reviewed_at": "",
        }

        path = os.path.join(output_dir, subdirs["templates"], "data_template.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(template, f, indent=2, ensure_ascii=False)
        result.files_generated.append(f"{subdirs['templates']}/data_template.json")

    def _generate_production_sop(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
    ):
        """Generate PRODUCTION_SOP.md - production standard operating procedure."""
        lines = []
        lines.append(f"# {analysis.project_name} ç”Ÿäº§æ ‡å‡†æ“ä½œæµç¨‹ (SOP)")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> æ•°æ®ç±»å‹: {analysis.dataset_type}")
        lines.append("")
        lines.append("---")
        lines.append("")

        # Phase 1: Preparation
        lines.append("## é˜¶æ®µä¸€ï¼šå‡†å¤‡é˜¶æ®µ")
        lines.append("")
        lines.append("### 1.1 ç¯å¢ƒå‡†å¤‡")
        lines.append("")
        lines.append("- [ ] åˆ›å»ºå·¥ä½œç›®å½•ç»“æ„")
        lines.append("- [ ] å‡†å¤‡æ ‡æ³¨å·¥å…·")
        lines.append("- [ ] é…ç½®è´¨æ£€æµç¨‹")
        lines.append("")

        lines.append("### 1.2 èµ„æ–™å‡†å¤‡")
        lines.append("")
        lines.append("- [ ] é˜…è¯» `03_æ ‡æ³¨è§„èŒƒ/ANNOTATION_SPEC.md`")
        lines.append("- [ ] é˜…è¯» `03_æ ‡æ³¨è§„èŒƒ/TRAINING_GUIDE.md`")
        if analysis.has_difficulty_validation():
            lines.append("- [ ] é˜…è¯» `04_å¤åˆ»æŒ‡å—/DIFFICULTY_VALIDATION.md`")
        lines.append("- [ ] å‡†å¤‡ `07_æ¨¡æ¿/data_template.json` æ¨¡æ¿")
        lines.append("")

        # Phase 2: Content Creation
        lines.append("---")
        lines.append("")
        lines.append("## é˜¶æ®µäºŒï¼šå†…å®¹åˆ›ä½œ")
        lines.append("")

        if analysis.has_images:
            lines.append("### 2.1 å›¾ç‰‡åˆ¶ä½œ")
            lines.append("")
            lines.append("**è¦æ±‚**ï¼š")
            for item in analysis.forbidden_items:
                if "AI" in item or "å›¾" in item:
                    lines.append(f"- âŒ {item}")
            lines.append("- âœ… ä½¿ç”¨æ‰‹ç»˜ã€è½¯ä»¶ç»˜å›¾æˆ–ç…§ç‰‡")
            lines.append("- âœ… ç¡®ä¿åˆ†è¾¨ç‡ â‰¥ 800x600")
            lines.append("")

        lines.append("### 2.2 é¢˜ç›®è®¾è®¡")
        lines.append("")
        lines.append("**è¦æ±‚**ï¼š")
        lines.append("- é¢˜æ„æ¸…æ™°ï¼Œæ— æ­§ä¹‰")
        lines.append("- ä¸å›¾ç‰‡å¼ºç›¸å…³ï¼ˆæ— å›¾æ— æ³•è§£é¢˜ï¼‰")
        lines.append("- æ˜ç¡®è¾“å‡ºæ ¼å¼è¦æ±‚")
        lines.append("")

        lines.append("### 2.3 ç­”æ¡ˆç¼–å†™")
        lines.append("")
        lines.append("**è¦æ±‚**ï¼š")
        lines.append("- ç­”æ¡ˆæ­£ç¡®ä¸”å¯éªŒè¯")
        lines.append("- å¦‚æœ‰å¤šè§£ï¼Œå…¨éƒ¨åˆ—å‡º")
        lines.append("- æ ¼å¼ç¬¦åˆé¢˜ç›®è¦æ±‚")
        lines.append("")

        lines.append("### 2.4 è§£æç¼–å†™")
        lines.append("")
        lines.append("**è¦æ±‚**ï¼š")
        lines.append("- æ­¥éª¤å®Œæ•´ï¼Œä»é¢˜ç›®åˆ°ç­”æ¡ˆ")
        lines.append("- é€»è¾‘æ¸…æ™°ï¼Œæ˜“äºç†è§£")
        lines.append("- é AI ç”Ÿæˆ")
        lines.append("")

        # Phase 3: Difficulty Validation (if enabled)
        if analysis.has_difficulty_validation():
            diff_val = analysis.difficulty_validation
            model_name = diff_val.get("model", "æœªæŒ‡å®šæ¨¡å‹")
            settings = diff_val.get("settings", "é»˜è®¤è®¾ç½®")
            test_count = diff_val.get("test_count", 3)
            max_correct = diff_val.get("max_correct", 1)

            lines.append("---")
            lines.append("")
            lines.append("## é˜¶æ®µä¸‰ï¼šéš¾åº¦éªŒè¯")
            lines.append("")
            lines.append(f"ä½¿ç”¨ **{model_name}** ({settings}) è¿›è¡ŒéªŒè¯ã€‚")
            lines.append("")
            lines.append("### éªŒè¯æ­¥éª¤")
            lines.append("")
            lines.append(f"1. å°†é¢˜ç›®è¾“å…¥ {model_name}ï¼ˆ{settings}ï¼‰")
            lines.append(f"2. è®°å½•æ¨¡å‹å›ç­”")
            lines.append(f"3. åˆ¤å®šæ­£ç¡®/é”™è¯¯")
            lines.append(f"4. é‡å¤ {test_count} æ¬¡ï¼ˆæ¯æ¬¡æ–°å¯¹è¯ï¼‰")
            lines.append("")
            lines.append("### åˆ¤å®šæ ‡å‡†")
            lines.append("")
            lines.append(f"- âœ… æœ‰æ•ˆï¼šæ­£ç¡®æ¬¡æ•° â‰¤ {max_correct}")
            lines.append(f"- âŒ æ— æ•ˆï¼šæ­£ç¡®æ¬¡æ•° > {max_correct}ï¼Œéœ€å¢åŠ éš¾åº¦åé‡æ–°éªŒè¯")
            lines.append("")
            phase_num = 4
        else:
            phase_num = 3

        # Phase 4/3: Quality Check
        lines.append("---")
        lines.append("")
        lines.append(f"## é˜¶æ®µ{phase_num}ï¼šè´¨é‡æ£€æŸ¥")
        lines.append("")
        lines.append("### è‡ªæ£€æ¸…å•")
        lines.append("")
        lines.append("å¯¹ç…§ `03_æ ‡æ³¨è§„èŒƒ/QA_CHECKLIST.md` é€é¡¹æ£€æŸ¥ï¼š")
        lines.append("")
        if analysis.has_images:
            lines.append("- [ ] å›¾ç‰‡ï¼šåŸåˆ›ã€æ¸…æ™°ã€ç›¸å…³")
        lines.append("- [ ] é¢˜ç›®ï¼šæ¸…æ™°ã€å®Œæ•´ã€æ— æ­§ä¹‰")
        lines.append("- [ ] ç­”æ¡ˆï¼šæ­£ç¡®ã€å®Œæ•´ã€æ ¼å¼è§„èŒƒ")
        lines.append("- [ ] è§£æï¼šå®Œæ•´ã€é€»è¾‘æ¸…æ™°")
        lines.append("- [ ] è¯„åˆ†æ ‡å‡†ï¼šå…·ä½“ã€å¯æ“ä½œ")
        if analysis.has_difficulty_validation():
            lines.append("- [ ] éš¾åº¦éªŒè¯ï¼šå·²é€šè¿‡")
        lines.append("")

        # Phase 5/4: Submission
        phase_num += 1
        lines.append("---")
        lines.append("")
        lines.append(f"## é˜¶æ®µ{phase_num}ï¼šæäº¤")
        lines.append("")
        lines.append("### æäº¤æ ¼å¼")
        lines.append("")
        lines.append("æŒ‰ç…§ `04_å¤åˆ»æŒ‡å—/DATA_SCHEMA.json` æ ¼å¼æäº¤æ•°æ®ã€‚")
        lines.append("")
        lines.append("### æäº¤æ£€æŸ¥")
        lines.append("")
        lines.append("- [ ] JSON æ ¼å¼æ­£ç¡®")
        lines.append("- [ ] æ‰€æœ‰å¿…å¡«å­—æ®µå·²å¡«å†™")
        if analysis.has_images:
            lines.append("- [ ] å›¾ç‰‡æ–‡ä»¶å·²ä¸Šä¼ ")
        if analysis.has_difficulty_validation():
            lines.append("- [ ] éš¾åº¦éªŒè¯è®°å½•å·²é™„ä¸Š")
        lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> æœ¬ SOP ç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ")

        path = os.path.join(output_dir, subdirs["guide"], "PRODUCTION_SOP.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(f"{subdirs['guide']}/PRODUCTION_SOP.md")

    def _generate_data_schema(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
    ):
        """Generate DATA_SCHEMA.json - JSON schema for data format."""
        if analysis.fields and len(analysis.fields) > 0:
            # Build schema from actual fields via FieldDefinition
            schema = self._build_schema_from_fields(analysis)
        else:
            # Fall back to profile default fields, then generic
            profile = get_task_profile(analysis.dataset_type)
            if profile.default_fields:
                # Temporarily inject profile defaults
                from copy import deepcopy
                tmp = deepcopy(analysis)
                tmp.fields = profile.default_fields
                # Invalidate field_definitions cache
                if hasattr(tmp, "_cached_field_definitions"):
                    object.__setattr__(tmp, "_cached_field_definitions", None)
                schema = self._build_schema_from_fields(tmp)
            else:
                schema = self._build_generic_schema(analysis)

        # Add image field if needed
        if analysis.has_images:
            schema["required"].insert(1, "image")
            schema["properties"]["image"] = {
                "type": "object",
                "properties": {
                    "path": {"type": "string", "description": "å›¾ç‰‡è·¯å¾„"},
                    "type": {
                        "type": "string",
                        "enum": ["hand_drawn", "software", "photo"],
                        "description": "å›¾ç‰‡ç±»å‹",
                    },
                    "description": {"type": "string", "description": "å›¾ç‰‡æè¿°"},
                },
                "required": ["path", "type"],
            }

        # Add model_test field if difficulty validation is enabled
        if analysis.has_difficulty_validation():
            diff_val = analysis.difficulty_validation
            test_count = diff_val.get("test_count", 3)

            schema["required"].append("model_test")
            schema["properties"]["model_test"] = {
                "type": "object",
                "properties": {
                    "model": {"type": "string", "description": "æµ‹è¯•ä½¿ç”¨çš„æ¨¡å‹"},
                    "settings": {"type": "string", "description": "æ¨¡å‹é…ç½®"},
                    "results": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "attempt": {"type": "integer"},
                                "response": {"type": "string"},
                                "is_correct": {"type": "boolean"},
                            },
                            "required": ["attempt", "response", "is_correct"],
                        },
                        "minItems": test_count,
                        "maxItems": test_count,
                    },
                    "valid": {"type": "boolean", "description": "æ˜¯å¦é€šè¿‡éš¾åº¦éªŒè¯"},
                },
                "required": ["model", "results", "valid"],
            }

        path = os.path.join(output_dir, subdirs["guide"], "DATA_SCHEMA.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(schema, f, indent=2, ensure_ascii=False)
        result.files_generated.append(f"{subdirs['guide']}/DATA_SCHEMA.json")

    def _build_schema_from_fields(self, analysis: SpecificationAnalysis) -> dict:
        """Build JSON schema from analysis.field_definitions (FieldDefinition objects)."""
        properties = {}
        required = []

        # Always add id field
        properties["id"] = {
            "type": "string",
            "description": "å”¯ä¸€æ ‡è¯†ç¬¦",
        }
        required.append("id")

        # Convert each FieldDefinition to JSON Schema via to_json_schema()
        for fd in analysis.field_definitions:
            if not fd.name or fd.name == "id":
                continue

            properties[fd.name] = fd.to_json_schema()

            if fd.required:
                required.append(fd.name)

        # Add metadata field
        properties["metadata"] = {
            "type": "object",
            "properties": {
                "difficulty": {"type": "string", "enum": ["easy", "medium", "hard", "expert"]},
                "domain": {"type": "string"},
                "created_at": {"type": "string", "format": "date-time"},
            },
        }

        return {
            "$schema": "https://json-schema.org/draft/2020-12/schema",
            "title": f"{analysis.project_name} æ•°æ®æ ¼å¼",
            "type": "object",
            "required": required,
            "properties": properties,
            # Include field definitions for reference
            "x-field-definitions": [fd.to_dict() for fd in analysis.field_definitions],
        }

    def _build_generic_schema(self, analysis: SpecificationAnalysis) -> dict:
        """Build generic fallback schema when no fields are defined."""
        return {
            "$schema": "https://json-schema.org/draft/2020-12/schema",
            "title": f"{analysis.project_name} æ•°æ®æ ¼å¼",
            "type": "object",
            "required": ["id", "question", "answer", "explanation", "scoring_rubric", "metadata"],
            "properties": {
                "id": {
                    "type": "string",
                    "description": "å”¯ä¸€æ ‡è¯†ç¬¦",
                    "pattern": "^[A-Z]+_[0-9]+$",
                },
                "question": {
                    "type": "string",
                    "description": "é¢˜ç›®æ–‡å­—",
                    "minLength": 10,
                },
                "answer": {
                    "type": "object",
                    "properties": {
                        "value": {"type": "string", "description": "æ ‡å‡†ç­”æ¡ˆ"},
                        "is_unique": {"type": "boolean", "description": "ç­”æ¡ˆæ˜¯å¦å”¯ä¸€"},
                        "alternatives": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "å…¶ä»–å¯æ¥å—çš„ç­”æ¡ˆ",
                        },
                    },
                    "required": ["value", "is_unique"],
                },
                "explanation": {
                    "type": "string",
                    "description": "è§£é¢˜è¿‡ç¨‹",
                    "minLength": 20,
                },
                "scoring_rubric": {
                    "type": "object",
                    "properties": {
                        "full_score": {"type": "string", "description": "æ»¡åˆ†æ¡ä»¶"},
                        "partial_score": {"type": "string", "description": "éƒ¨åˆ†å¾—åˆ†æ¡ä»¶"},
                        "zero_score": {"type": "string", "description": "é›¶åˆ†æ¡ä»¶"},
                    },
                    "required": ["full_score", "zero_score"],
                },
                "metadata": {
                    "type": "object",
                    "properties": {
                        "category": {"type": "string"},
                        "difficulty": {"type": "string", "enum": ["easy", "medium", "hard", "expert"]},
                        "created_by": {"type": "string"},
                        "created_at": {"type": "string", "format": "date"},
                        "reviewed_by": {"type": "string"},
                        "reviewed_at": {"type": "string"},
                    },
                    "required": ["category", "difficulty", "created_by", "created_at"],
                },
            },
        }

    def _estimate_cost_per_item(self, analysis: SpecificationAnalysis, region: str) -> float:
        """Estimate cost per item based on analysis."""
        # Base cost by difficulty
        base_costs = {
            "easy": 5,
            "medium": 10,
            "hard": 20,
            "expert": 40,
        }
        base = base_costs.get(analysis.estimated_difficulty, 15)

        # Multipliers
        multiplier = 1.0

        # Image multiplier
        if analysis.has_images:
            multiplier *= 1.5

        # Complexity multiplier (based on reasoning chain length)
        if len(analysis.reasoning_chain) > 3:
            multiplier *= 1.3

        # Forbidden items multiplier (all human = more expensive)
        if analysis.forbidden_items:
            multiplier *= 1.2

        # Region adjustment
        if region == "china":
            multiplier *= 0.6  # China is cheaper
        elif region == "us":
            multiplier *= 1.0

        return base * multiplier

    # ========== AI Agent Layer ==========

    def _generate_ai_agent_context(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        target_size: int,
        region: str,
        result: SpecOutputResult,
    ):
        """Generate agent_context.json - aggregated entry point for AI agents."""
        cost_per_item = self._estimate_cost_per_item(analysis, region)
        total_cost = cost_per_item * target_size

        context = {
            "_meta": {
                "version": "1.0",
                "generated_at": datetime.now().isoformat(),
                "generator": "DataRecipe",
                "purpose": "AI Agent èšåˆå…¥å£ï¼Œå¼•ç”¨å…¶ä»–æ–‡ä»¶è€Œéå¤åˆ¶"
            },
            "project": {
                "name": analysis.project_name,
                "type": analysis.dataset_type,
                "description": analysis.description or analysis.task_description,
                "difficulty": analysis.estimated_difficulty,
                "domain": analysis.estimated_domain,
            },
            "summary": {
                "target_size": target_size,
                "total_cost": round(total_cost, 2),
                "cost_per_item": round(cost_per_item, 2),
                "human_percentage": analysis.estimated_human_percentage,
                "has_images": analysis.has_images,
                "image_count": analysis.image_count,
                "field_count": len(analysis.fields),
                "has_difficulty_validation": analysis.has_difficulty_validation(),
            },
            "key_decisions": [
                {
                    "decision": "difficulty_level",
                    "value": analysis.estimated_difficulty,
                    "reasoning_ref": "#/reasoning/difficulty"
                },
                {
                    "decision": "human_percentage",
                    "value": analysis.estimated_human_percentage,
                    "reasoning_ref": "#/reasoning/human_percentage"
                },
                {
                    "decision": "cost_estimate",
                    "value": round(total_cost, 2),
                    "reasoning_ref": "#/reasoning/cost"
                }
            ],
            "validation": None,
            "file_references": {
                "executive_summary": f"../{subdirs['decision']}/EXECUTIVE_SUMMARY.md",
                "milestone_plan": f"../{subdirs['project']}/MILESTONE_PLAN.md",
                "annotation_spec": f"../{subdirs['annotation']}/ANNOTATION_SPEC.md",
                "training_guide": f"../{subdirs['annotation']}/TRAINING_GUIDE.md",
                "qa_checklist": f"../{subdirs['annotation']}/QA_CHECKLIST.md",
                "production_sop": f"../{subdirs['guide']}/PRODUCTION_SOP.md",
                "data_schema": f"../{subdirs['guide']}/DATA_SCHEMA.json",
                "cost_breakdown": f"../{subdirs['cost']}/COST_BREAKDOWN.md",
                "data_template": f"../{subdirs['templates']}/data_template.json",
                "raw_analysis": f"../{subdirs['data']}/spec_analysis.json",
            },
            "quick_actions": [
                {
                    "action": "review_spec",
                    "description": "å®¡æ ¸æ ‡æ³¨è§„èŒƒ",
                    "file": f"../{subdirs['annotation']}/ANNOTATION_SPEC.md",
                    "assignee": "human"
                },
                {
                    "action": "setup_tool",
                    "description": "é…ç½®æ ‡æ³¨å·¥å…·",
                    "config": f"../{subdirs['guide']}/DATA_SCHEMA.json",
                    "assignee": "agent"
                },
                {
                    "action": "create_sample",
                    "description": "åˆ›å»ºæ ·æœ¬æ•°æ®",
                    "template": f"../{subdirs['templates']}/data_template.json",
                    "assignee": "human"
                }
            ]
        }

        # Add validation config if present
        if analysis.has_difficulty_validation():
            diff_val = analysis.difficulty_validation
            context["validation"] = {
                "enabled": True,
                "model": diff_val.get("model"),
                "settings": diff_val.get("settings"),
                "test_count": diff_val.get("test_count", 3),
                "max_correct": diff_val.get("max_correct", 1),
                "guide_ref": f"../{subdirs['guide']}/DIFFICULTY_VALIDATION.md"
            }
            context["file_references"]["difficulty_validation"] = f"../{subdirs['guide']}/DIFFICULTY_VALIDATION.md"

        path = os.path.join(output_dir, subdirs["ai_agent"], "agent_context.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(context, f, indent=2, ensure_ascii=False)
        result.files_generated.append(f"{subdirs['ai_agent']}/agent_context.json")

    def _generate_ai_workflow_state(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
    ):
        """Generate workflow_state.json - workflow state tracking."""
        state = {
            "_meta": {
                "version": "1.0",
                "generated_at": datetime.now().isoformat(),
                "purpose": "å·¥ä½œæµçŠ¶æ€è¿½è¸ªï¼Œä¾› AI Agent äº†è§£å½“å‰è¿›åº¦å’Œä¸‹ä¸€æ­¥"
            },
            "current_phase": "ready_for_review",
            "phases": {
                "analysis": {
                    "status": "completed",
                    "description": "éœ€æ±‚æ–‡æ¡£åˆ†æ",
                    "outputs": [
                        f"../{subdirs['data']}/spec_analysis.json"
                    ]
                },
                "planning": {
                    "status": "completed",
                    "description": "é¡¹ç›®è§„åˆ’ä¸æˆæœ¬ä¼°ç®—",
                    "outputs": [
                        f"../{subdirs['decision']}/EXECUTIVE_SUMMARY.md",
                        f"../{subdirs['project']}/MILESTONE_PLAN.md",
                        f"../{subdirs['cost']}/COST_BREAKDOWN.md"
                    ]
                },
                "spec_generation": {
                    "status": "completed",
                    "description": "æ ‡æ³¨è§„èŒƒä¸åŸ¹è®­ææ–™ç”Ÿæˆ",
                    "outputs": [
                        f"../{subdirs['annotation']}/ANNOTATION_SPEC.md",
                        f"../{subdirs['annotation']}/TRAINING_GUIDE.md",
                        f"../{subdirs['annotation']}/QA_CHECKLIST.md"
                    ]
                },
                "review": {
                    "status": "pending",
                    "description": "äººå·¥å®¡æ ¸æ ‡æ³¨è§„èŒƒ",
                    "blocked_by": [],
                    "assignee": "human"
                },
                "pilot": {
                    "status": "pending",
                    "description": "è¯•ç‚¹æ ‡æ³¨",
                    "blocked_by": ["review"],
                    "assignee": "human"
                },
                "production": {
                    "status": "pending",
                    "description": "ä¸»ä½“æ ‡æ³¨",
                    "blocked_by": ["pilot"],
                    "assignee": "human"
                },
                "quality_check": {
                    "status": "pending",
                    "description": "è´¨é‡å®¡æ ¸",
                    "blocked_by": ["production"],
                    "assignee": "human"
                }
            },
            "next_actions": [
                {
                    "action": "review_annotation_spec",
                    "description": "å®¡æ ¸æ ‡æ³¨è§„èŒƒæ˜¯å¦ç¬¦åˆéœ€æ±‚",
                    "file": f"../{subdirs['annotation']}/ANNOTATION_SPEC.md",
                    "assignee": "human",
                    "priority": "high"
                },
                {
                    "action": "review_training_guide",
                    "description": "å®¡æ ¸åŸ¹è®­æ‰‹å†Œæ˜¯å¦æ¸…æ™°",
                    "file": f"../{subdirs['annotation']}/TRAINING_GUIDE.md",
                    "assignee": "human",
                    "priority": "high"
                },
                {
                    "action": "approve_cost_estimate",
                    "description": "ç¡®è®¤æˆæœ¬ä¼°ç®—å¹¶æ‰¹å‡†é¢„ç®—",
                    "file": f"../{subdirs['cost']}/COST_BREAKDOWN.md",
                    "assignee": "human",
                    "priority": "medium"
                }
            ],
            "blockers": [],
            "decisions_needed": [
                {
                    "question": "æ ‡æ³¨è§„èŒƒæ˜¯å¦éœ€è¦ä¿®æ”¹ï¼Ÿ",
                    "options": ["approved", "needs_revision"],
                    "impact": "å½±å“åç»­è¯•ç‚¹é˜¶æ®µå¯åŠ¨"
                },
                {
                    "question": "æˆæœ¬é¢„ç®—æ˜¯å¦æ‰¹å‡†ï¼Ÿ",
                    "options": ["approved", "needs_adjustment", "rejected"],
                    "impact": "å½±å“é¡¹ç›®æ˜¯å¦ç»§ç»­"
                }
            ]
        }

        path = os.path.join(output_dir, subdirs["ai_agent"], "workflow_state.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        result.files_generated.append(f"{subdirs['ai_agent']}/workflow_state.json")

    def _generate_ai_reasoning_traces(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        target_size: int,
        region: str,
        result: SpecOutputResult,
    ):
        """Generate reasoning_traces.json - reasoning chains for all conclusions."""
        cost_per_item = self._estimate_cost_per_item(analysis, region)

        traces = {
            "_meta": {
                "version": "1.0",
                "generated_at": datetime.now().isoformat(),
                "purpose": "æ‰€æœ‰ç»“è®ºçš„æ¨ç†é“¾ï¼Œä¾›äººç±»ç†è§£å’Œ AI éªŒè¯"
            },
            "reasoning": {
                "difficulty": {
                    "conclusion": {
                        "value": analysis.estimated_difficulty,
                        "display": f"éš¾åº¦: {analysis.estimated_difficulty}"
                    },
                    "chain": [],
                    "confidence": 0.0,
                    "assumptions": [
                        "å‡è®¾æ ‡æ³¨å‘˜æ— ç›¸å…³é¢†åŸŸèƒŒæ™¯",
                        "å‡è®¾æŒ‰ç…§æ ‡å‡†åŸ¹è®­æµç¨‹"
                    ],
                    "human_explanation": ""
                },
                "human_percentage": {
                    "conclusion": {
                        "value": analysis.estimated_human_percentage,
                        "display": f"äººå·¥æ¯”ä¾‹: {analysis.estimated_human_percentage}%"
                    },
                    "chain": [],
                    "confidence": 0.0,
                    "assumptions": [],
                    "human_explanation": ""
                },
                "cost": {
                    "conclusion": {
                        "value": round(cost_per_item * target_size, 2),
                        "display": f"æ€»æˆæœ¬: ${cost_per_item * target_size:,.0f}"
                    },
                    "chain": [],
                    "confidence": 0.0,
                    "range": {
                        "low": round(cost_per_item * target_size * 0.7, 2),
                        "high": round(cost_per_item * target_size * 1.4, 2)
                    },
                    "assumptions": [],
                    "human_explanation": ""
                }
            }
        }

        # Build difficulty reasoning chain
        difficulty_chain = []
        confidence = 0.5  # Base confidence

        if analysis.reasoning_chain:
            chain_len = len(analysis.reasoning_chain)
            difficulty_chain.append({
                "step": "åˆ†ææ¨ç†é“¾é•¿åº¦",
                "evidence": f"æ¨ç†é“¾æœ‰ {chain_len} æ­¥",
                "impact": "hard" if chain_len > 3 else "medium" if chain_len > 2 else "easy"
            })
            confidence += 0.1

        if analysis.cognitive_requirements:
            req_count = len(analysis.cognitive_requirements)
            difficulty_chain.append({
                "step": "è¯„ä¼°è®¤çŸ¥è¦æ±‚",
                "evidence": f"éœ€è¦ {req_count} é¡¹è®¤çŸ¥èƒ½åŠ›",
                "impact": "expert" if req_count > 3 else "hard" if req_count > 2 else "medium"
            })
            confidence += 0.1

        if analysis.has_images:
            difficulty_chain.append({
                "step": "æ£€æµ‹å¤šæ¨¡æ€è¦æ±‚",
                "evidence": f"åŒ…å« {analysis.image_count} å¼ å›¾ç‰‡",
                "impact": "å¢åŠ éš¾åº¦ - éœ€è¦è§†è§‰ç†è§£èƒ½åŠ›"
            })
            confidence += 0.1

        if analysis.forbidden_items:
            difficulty_chain.append({
                "step": "æ£€æŸ¥ç¦æ­¢é¡¹",
                "evidence": f"æœ‰ {len(analysis.forbidden_items)} é¡¹ç¦æ­¢å†…å®¹",
                "impact": "å¢åŠ éš¾åº¦ - éœ€è¦æ›´ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶"
            })
            confidence += 0.05

        traces["reasoning"]["difficulty"]["chain"] = difficulty_chain
        traces["reasoning"]["difficulty"]["confidence"] = min(confidence, 0.95)
        traces["reasoning"]["difficulty"]["human_explanation"] = self._build_difficulty_explanation(analysis)

        # Build human percentage reasoning chain
        human_chain = []
        human_confidence = 0.7

        if analysis.forbidden_items:
            has_ai_restriction = any("AI" in item or "ai" in item.lower() for item in analysis.forbidden_items)
            if has_ai_restriction:
                human_chain.append({
                    "step": "æ£€æµ‹ AI å†…å®¹é™åˆ¶",
                    "evidence": "ç¦æ­¢ä½¿ç”¨ AI ç”Ÿæˆå†…å®¹",
                    "impact": "äººå·¥æ¯”ä¾‹ 100%"
                })
                human_confidence = 0.95
            else:
                human_chain.append({
                    "step": "æ£€æµ‹å†…å®¹é™åˆ¶",
                    "evidence": f"æœ‰ {len(analysis.forbidden_items)} é¡¹é™åˆ¶",
                    "impact": "äººå·¥æ¯”ä¾‹ > 80%"
                })

        traces["reasoning"]["human_percentage"]["chain"] = human_chain
        traces["reasoning"]["human_percentage"]["confidence"] = human_confidence
        traces["reasoning"]["human_percentage"]["human_explanation"] = (
            f"ç”±äº{'ç¦æ­¢ä½¿ç”¨ AI ç”Ÿæˆå†…å®¹ï¼Œ' if analysis.forbidden_items else ''}"
            f"é¢„ä¼°äººå·¥æ¯”ä¾‹ä¸º {analysis.estimated_human_percentage}%ã€‚"
        )

        # Build cost reasoning chain
        cost_chain = [
            {
                "step": "ç¡®å®šåŸºç¡€æˆæœ¬",
                "evidence": f"éš¾åº¦ {analysis.estimated_difficulty} å¯¹åº”åŸºç¡€æˆæœ¬",
                "value": {"easy": 5, "medium": 10, "hard": 20, "expert": 40}.get(analysis.estimated_difficulty, 15)
            }
        ]

        if analysis.has_images:
            cost_chain.append({
                "step": "åº”ç”¨å›¾ç‰‡ä¹˜æ•°",
                "evidence": "åŒ…å«å›¾ç‰‡ï¼Œæˆæœ¬ Ã—1.5",
                "multiplier": 1.5
            })

        if len(analysis.reasoning_chain) > 3:
            cost_chain.append({
                "step": "åº”ç”¨å¤æ‚åº¦ä¹˜æ•°",
                "evidence": "æ¨ç†é“¾ > 3 æ­¥ï¼Œæˆæœ¬ Ã—1.3",
                "multiplier": 1.3
            })

        if analysis.forbidden_items:
            cost_chain.append({
                "step": "åº”ç”¨äººå·¥ä¹˜æ•°",
                "evidence": "æœ‰å†…å®¹é™åˆ¶ï¼Œéœ€å…¨äººå·¥ï¼Œæˆæœ¬ Ã—1.2",
                "multiplier": 1.2
            })

        if region == "china":
            cost_chain.append({
                "step": "åº”ç”¨åŒºåŸŸè°ƒæ•´",
                "evidence": "ä¸­å›½åŒºåŸŸï¼Œæˆæœ¬ Ã—0.6",
                "multiplier": 0.6
            })

        cost_chain.append({
            "step": "è®¡ç®—æ€»æˆæœ¬",
            "evidence": f"å•æ¡ ${cost_per_item:.2f} Ã— {target_size} æ¡",
            "result": round(cost_per_item * target_size, 2)
        })

        traces["reasoning"]["cost"]["chain"] = cost_chain
        traces["reasoning"]["cost"]["confidence"] = 0.75
        traces["reasoning"]["cost"]["assumptions"] = [
            "å‡è®¾æ ‡æ³¨æ•ˆç‡ç¨³å®š",
            "å‡è®¾è¿”å·¥ç‡ < 10%",
            "å‡è®¾æ— çªå‘äººåŠ›çŸ­ç¼º"
        ]
        traces["reasoning"]["cost"]["human_explanation"] = (
            f"åŸºäºéš¾åº¦({analysis.estimated_difficulty})ã€å›¾ç‰‡({analysis.has_images})ã€"
            f"å¤æ‚åº¦({len(analysis.reasoning_chain)}æ­¥æ¨ç†)å’ŒåŒºåŸŸ({region})è®¡ç®—ï¼Œ"
            f"é¢„ä¼°æ€»æˆæœ¬ ${cost_per_item * target_size:,.0f}ï¼Œ"
            f"ç½®ä¿¡åŒºé—´ ${cost_per_item * target_size * 0.7:,.0f} - ${cost_per_item * target_size * 1.4:,.0f}ã€‚"
        )

        path = os.path.join(output_dir, subdirs["ai_agent"], "reasoning_traces.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(traces, f, indent=2, ensure_ascii=False)
        result.files_generated.append(f"{subdirs['ai_agent']}/reasoning_traces.json")

    def _build_difficulty_explanation(self, analysis: SpecificationAnalysis) -> str:
        """Build human-readable difficulty explanation."""
        parts = []

        if analysis.cognitive_requirements:
            parts.append(f"éœ€è¦ {len(analysis.cognitive_requirements)} é¡¹è®¤çŸ¥èƒ½åŠ›")

        if analysis.reasoning_chain:
            parts.append(f"æ¨ç†é“¾é•¿è¾¾ {len(analysis.reasoning_chain)} æ­¥")

        if analysis.has_images:
            parts.append("éœ€è¦è§†è§‰ç†è§£èƒ½åŠ›")

        if parts:
            return f"è¯¥ä»»åŠ¡{', '.join(parts)}ï¼Œå±äº{analysis.estimated_difficulty}çº§éš¾åº¦ã€‚"
        return f"åŸºäºç»¼åˆè¯„ä¼°ï¼Œéš¾åº¦ä¸º{analysis.estimated_difficulty}çº§ã€‚"

    def _generate_ai_pipeline(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
    ):
        """Generate pipeline.yaml - executable pipeline for AI agents."""
        lines = []
        lines.append("# æ•°æ®ç”Ÿäº§æµæ°´çº¿")
        lines.append("# ä¾› AI Agent æ‰§è¡Œçš„å¯æ“ä½œæ­¥éª¤")
        lines.append("")
        lines.append("name: æ•°æ®ç”Ÿäº§æµæ°´çº¿")
        lines.append("version: '1.0'")
        lines.append(f"project: {analysis.project_name}")
        lines.append(f"generated_at: {datetime.now().isoformat()}")
        lines.append("")

        # Variables section
        lines.append("variables:")
        lines.append(f"  project_name: \"{analysis.project_name}\"")
        lines.append(f"  target_size: 100  # å¯è°ƒæ•´")
        lines.append(f"  difficulty: \"{analysis.estimated_difficulty}\"")
        if analysis.has_difficulty_validation():
            diff_val = analysis.difficulty_validation
            lines.append(f"  validation_model: \"{diff_val.get('model', '')}\"")
            lines.append(f"  validation_settings: \"{diff_val.get('settings', '')}\"")
            lines.append(f"  validation_test_count: {diff_val.get('test_count', 3)}")
            lines.append(f"  validation_max_correct: {diff_val.get('max_correct', 1)}")
        lines.append("")

        # Phases
        lines.append("phases:")
        lines.append("")

        # Phase 1: Setup
        lines.append("  - name: setup")
        lines.append("    description: ç¯å¢ƒå‡†å¤‡")
        lines.append("    steps:")
        lines.append("      - action: validate_schema")
        lines.append("        description: éªŒè¯æ•°æ®æ ¼å¼å®šä¹‰")
        lines.append(f"        input: ../{subdirs['guide']}/DATA_SCHEMA.json")
        lines.append("        assignee: agent")
        lines.append("")
        lines.append("      - action: prepare_template")
        lines.append("        description: å‡†å¤‡æ•°æ®æ¨¡æ¿")
        lines.append(f"        input: ../{subdirs['templates']}/data_template.json")
        lines.append("        assignee: agent")
        lines.append("")
        lines.append("      - action: review_training_guide")
        lines.append("        description: å®¡æ ¸åŸ¹è®­æ‰‹å†Œ")
        lines.append(f"        input: ../{subdirs['annotation']}/TRAINING_GUIDE.md")
        lines.append("        assignee: human")
        lines.append("        required: true")
        lines.append("")

        # Phase 2: Pilot
        lines.append("  - name: pilot")
        lines.append("    description: è¯•ç‚¹æ ‡æ³¨")
        lines.append("    depends_on: [setup]")
        lines.append("    steps:")
        lines.append("      - action: create_pilot_samples")
        lines.append("        description: åˆ›å»ºè¯•ç‚¹æ ·æœ¬ (5-10 æ¡)")
        lines.append(f"        template: ../{subdirs['templates']}/data_template.json")
        lines.append(f"        spec: ../{subdirs['annotation']}/ANNOTATION_SPEC.md")
        lines.append("        count: 10")
        lines.append("        assignee: human")
        lines.append("")
        lines.append("      - action: quality_review_pilot")
        lines.append("        description: è¯•ç‚¹è´¨é‡å®¡æ ¸")
        lines.append(f"        checklist: ../{subdirs['annotation']}/QA_CHECKLIST.md")
        lines.append("        assignee: human")
        lines.append("")

        # Phase 3+: Validation phases for each strategy
        last_validation_phase = "pilot"
        for strategy in analysis.parsed_validation_strategies:
            phase_name = f"validation_{strategy.strategy_type}"
            strategy_desc = {
                "model_test": "éš¾åº¦éªŒè¯",
                "human_review": "äººå·¥å®¡æ ¸",
                "format_check": "æ ¼å¼æ ¡éªŒ",
                "cross_validation": "äº¤å‰éªŒè¯",
                "auto_scoring": "è‡ªåŠ¨è¯„åˆ†",
            }.get(strategy.strategy_type, strategy.strategy_type)

            lines.append(f"  - name: {phase_name}")
            lines.append(f"    description: {strategy_desc}")
            lines.append(f"    depends_on: [{last_validation_phase}]")
            lines.append("    steps:")

            if strategy.strategy_type == "model_test":
                cfg = strategy.config
                lines.append("      - action: run_model_test")
                lines.append("        description: æ‰§è¡Œæ¨¡å‹æµ‹è¯•")
                lines.append("        config:")
                lines.append(f"          model: \"{cfg.get('model', '')}\"")
                lines.append(f"          settings: \"{cfg.get('settings', '')}\"")
                lines.append(f"          test_count: {cfg.get('test_count', 3)}")
                lines.append(f"          max_correct: {cfg.get('max_correct', 1)}")
                lines.append(f"        reference: ../{subdirs['guide']}/DIFFICULTY_VALIDATION.md")
                lines.append("        assignee: human")
                lines.append("")
                lines.append("      - action: validate_difficulty_result")
                lines.append("        description: éªŒè¯éš¾åº¦æµ‹è¯•ç»“æœ")
                lines.append("        on_failure: revise_question")
                lines.append("        assignee: agent")
            elif strategy.strategy_type == "human_review":
                lines.append("      - action: human_review")
                lines.append(f"        description: {strategy.description or 'äººå·¥å®¡æ ¸æŠ½æ£€'}")
                lines.append(f"        checklist: ../{subdirs['annotation']}/QA_CHECKLIST.md")
                lines.append(f"        sample_rate: {strategy.config.get('sample_rate', 0.2)}")
                lines.append("        assignee: human")
            elif strategy.strategy_type == "format_check":
                lines.append("      - action: format_check")
                lines.append("        description: è‡ªåŠ¨æ ¼å¼æ ¡éªŒ")
                lines.append(f"        schema: ../{subdirs['guide']}/DATA_SCHEMA.json")
                lines.append("        assignee: agent")
            elif strategy.strategy_type == "cross_validation":
                lines.append("      - action: cross_validation")
                lines.append(f"        description: {strategy.description or 'äº¤å‰éªŒè¯'}")
                lines.append(f"        min_annotators: {strategy.config.get('min_annotators', 2)}")
                lines.append(f"        min_kappa: {strategy.config.get('min_kappa', 0.7)}")
                lines.append("        assignee: human")
            elif strategy.strategy_type == "auto_scoring":
                lines.append("      - action: auto_scoring")
                lines.append(f"        description: {strategy.description or 'è‡ªåŠ¨è¯„åˆ†'}")
                lines.append(f"        threshold: {strategy.config.get('threshold', 0.6)}")
                lines.append("        assignee: agent")
            else:
                lines.append(f"      - action: {strategy.strategy_type}")
                lines.append(f"        description: {strategy.description}")
                lines.append("        assignee: human")

            lines.append("")
            last_validation_phase = phase_name

        # Production phase
        lines.append("  - name: production")
        lines.append("    description: ä¸»ä½“æ ‡æ³¨")
        lines.append(f"    depends_on: [{last_validation_phase}]")
        lines.append("    steps:")
        lines.append("      - action: batch_annotation")
        lines.append("        description: æ‰¹é‡æ ‡æ³¨")
        lines.append(f"        template: ../{subdirs['templates']}/data_template.json")
        lines.append(f"        spec: ../{subdirs['annotation']}/ANNOTATION_SPEC.md")
        lines.append("        count: \"{{ target_size }}\"")
        lines.append("        assignee: human")
        lines.append("")
        lines.append("      - action: incremental_qa")
        lines.append("        description: å¢é‡è´¨æ£€")
        lines.append(f"        checklist: ../{subdirs['annotation']}/QA_CHECKLIST.md")
        lines.append("        sample_rate: 0.2")
        lines.append("        assignee: human")
        lines.append("")

        # Phase 5: Final QA
        lines.append("  - name: final_qa")
        lines.append("    description: æœ€ç»ˆè´¨é‡å®¡æ ¸")
        lines.append("    depends_on: [production]")
        lines.append("    steps:")
        lines.append("      - action: full_qa_review")
        lines.append("        description: å…¨é‡è´¨æ£€")
        lines.append(f"        checklist: ../{subdirs['annotation']}/QA_CHECKLIST.md")
        lines.append("        assignee: human")
        lines.append("")
        lines.append("      - action: generate_qa_report")
        lines.append("        description: ç”Ÿæˆè´¨æ£€æŠ¥å‘Š")
        lines.append("        assignee: agent")
        lines.append("")
        lines.append("      - action: final_approval")
        lines.append("        description: æœ€ç»ˆå®¡æ‰¹")
        lines.append("        assignee: human")
        lines.append("        required: true")
        lines.append("")

        # Error handling
        lines.append("error_handling:")
        lines.append("  on_qa_failure:")
        lines.append("    action: flag_for_revision")
        lines.append("    notify: human")
        lines.append("  on_validation_failure:")
        lines.append("    action: increase_difficulty")
        lines.append("    retry: true")
        lines.append("    max_retries: 3")

        path = os.path.join(output_dir, subdirs["ai_agent"], "pipeline.yaml")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(f"{subdirs['ai_agent']}/pipeline.yaml")

    def _generate_ai_readme(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        result: SpecOutputResult,
    ):
        """Generate README.md for AI Agent directory."""
        lines = []
        lines.append(f"# {analysis.project_name} - AI Agent å…¥å£")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append("")
        lines.append("æœ¬ç›®å½•åŒ…å«ä¾› AI Agent æ¶ˆè´¹çš„ç»“æ„åŒ–æ•°æ®ï¼Œä¸äººç±»å¯è¯»çš„ Markdown æ–‡æ¡£äº’è¡¥ã€‚")
        lines.append("")
        lines.append("---")
        lines.append("")

        lines.append("## æ–‡ä»¶è¯´æ˜")
        lines.append("")
        lines.append("| æ–‡ä»¶ | ç”¨é€” | æ¶ˆè´¹è€… |")
        lines.append("|------|------|--------|")
        lines.append("| `agent_context.json` | èšåˆå…¥å£ï¼Œå¼•ç”¨å…¶ä»–æ–‡ä»¶ | AI Agent |")
        lines.append("| `workflow_state.json` | å·¥ä½œæµçŠ¶æ€ï¼Œå½“å‰é˜¶æ®µå’Œä¸‹ä¸€æ­¥ | AI Agent |")
        lines.append("| `reasoning_traces.json` | æ¨ç†é“¾ï¼Œè§£é‡Šæ¯ä¸ªç»“è®ºçš„åŸå›  | AI Agent + äººç±» |")
        lines.append("| `pipeline.yaml` | å¯æ‰§è¡Œæµæ°´çº¿ï¼Œå®šä¹‰æ ‡å‡†æ“ä½œæ­¥éª¤ | AI Agent |")
        lines.append("")

        lines.append("## å¿«é€Ÿå¼€å§‹")
        lines.append("")
        lines.append("### 1. è·å–é¡¹ç›®ä¸Šä¸‹æ–‡")
        lines.append("")
        lines.append("```python")
        lines.append("import json")
        lines.append("")
        lines.append("with open('agent_context.json') as f:")
        lines.append("    context = json.load(f)")
        lines.append("")
        lines.append("print(f\"é¡¹ç›®: {context['project']['name']}\")")
        lines.append("print(f\"éš¾åº¦: {context['project']['difficulty']}\")")
        lines.append("print(f\"æ€»æˆæœ¬: ${context['summary']['total_cost']}\")")
        lines.append("```")
        lines.append("")

        lines.append("### 2. æ£€æŸ¥å·¥ä½œæµçŠ¶æ€")
        lines.append("")
        lines.append("```python")
        lines.append("with open('workflow_state.json') as f:")
        lines.append("    state = json.load(f)")
        lines.append("")
        lines.append("print(f\"å½“å‰é˜¶æ®µ: {state['current_phase']}\")")
        lines.append("for action in state['next_actions']:")
        lines.append("    print(f\"ä¸‹ä¸€æ­¥: {action['description']} ({action['assignee']})\")")
        lines.append("```")
        lines.append("")

        lines.append("### 3. ç†è§£å†³ç­–æ¨ç†")
        lines.append("")
        lines.append("```python")
        lines.append("with open('reasoning_traces.json') as f:")
        lines.append("    traces = json.load(f)")
        lines.append("")
        lines.append("difficulty = traces['reasoning']['difficulty']")
        lines.append("print(f\"éš¾åº¦: {difficulty['conclusion']['value']}\")")
        lines.append("print(f\"ç½®ä¿¡åº¦: {difficulty['confidence']}\")")
        lines.append("print(f\"åŸå› : {difficulty['human_explanation']}\")")
        lines.append("```")
        lines.append("")

        lines.append("### 4. æ‰§è¡Œæµæ°´çº¿")
        lines.append("")
        lines.append("```python")
        lines.append("import yaml")
        lines.append("")
        lines.append("with open('pipeline.yaml') as f:")
        lines.append("    pipeline = yaml.safe_load(f)")
        lines.append("")
        lines.append("for phase in pipeline['phases']:")
        lines.append("    print(f\"é˜¶æ®µ: {phase['name']}\")")
        lines.append("    for step in phase['steps']:")
        lines.append("        print(f\"  - {step['action']}: {step['description']}\")")
        lines.append("```")
        lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("## ä¸äººç±»æ–‡æ¡£çš„å…³ç³»")
        lines.append("")
        lines.append("```")
        lines.append("AI Agent æ–‡ä»¶              äººç±»æ–‡æ¡£")
        lines.append("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        lines.append(f"agent_context.json    â†’  ../README.md (å¯¼èˆª)")
        lines.append(f"workflow_state.json   â†’  ../{subdirs['project']}/MILESTONE_PLAN.md")
        lines.append(f"reasoning_traces.json â†’  ../{subdirs['decision']}/EXECUTIVE_SUMMARY.md")
        lines.append(f"pipeline.yaml         â†’  ../{subdirs['guide']}/PRODUCTION_SOP.md")
        lines.append("```")
        lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> æœ¬ç›®å½•ç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ")

        path = os.path.join(output_dir, subdirs["ai_agent"], "README.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(f"{subdirs['ai_agent']}/README.md")

    def _generate_think_po_samples(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        target_size: int,
        result: SpecOutputResult,
    ):
        """Generate sample data with automation analysis.

        Generates up to 50 sample data entries with:
        - Actual sample data for automatable tasks
        - Clear markers for manual steps when automation isn't possible
        - Reasoning traces for each sample
        """
        samples = []
        max_samples = min(50, target_size)

        # Analyze automation feasibility
        automation_analysis = self._analyze_automation_feasibility(analysis)

        # Get task types from analysis
        task_types = self._extract_task_types(analysis)

        # Generate samples for each task type
        samples_per_type = max(1, max_samples // max(len(task_types), 1))

        for task_type in task_types:
            type_automation = automation_analysis.get(task_type, automation_analysis.get("default", {}))

            for i in range(samples_per_type):
                if len(samples) >= max_samples:
                    break

                sample = self._generate_single_sample(
                    analysis=analysis,
                    task_type=task_type,
                    sample_index=len(samples) + 1,
                    automation_info=type_automation,
                )
                samples.append(sample)

        # Build the complete samples document
        samples_doc = {
            "_meta": {
                "version": "1.0",
                "generated_at": datetime.now().isoformat(),
                "generator": "DataRecipe Sample Generator",
                "purpose": "ç”Ÿäº§æ ·ä¾‹æ•°æ®ï¼Œæ”¯æŒäººæœºååŒç†è§£",
                "total_samples": len(samples),
                "target_size": target_size,
            },
            "automation_summary": {
                "overall_automation_rate": automation_analysis.get("overall_rate", 0),
                "fully_automated_tasks": automation_analysis.get("fully_automated", []),
                "partially_automated_tasks": automation_analysis.get("partially_automated", []),
                "manual_tasks": automation_analysis.get("manual_only", []),
                "automation_blockers": automation_analysis.get("blockers", []),
            },
            "samples": samples,
            "production_notes": self._generate_production_notes(analysis, automation_analysis),
        }

        # Write JSON file
        json_path = os.path.join(output_dir, subdirs["samples"], "samples.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(samples_doc, f, indent=2, ensure_ascii=False)
        result.files_generated.append(f"{subdirs['samples']}/samples.json")

        # Generate human-readable guide
        self._generate_samples_guide(analysis, output_dir, subdirs, samples_doc, result)

    def _analyze_automation_feasibility(self, analysis: SpecificationAnalysis) -> dict:
        """Analyze which parts of the pipeline can be automated."""
        result = {
            "overall_rate": 0,
            "fully_automated": [],
            "partially_automated": [],
            "manual_only": [],
            "blockers": [],
            "default": {
                "can_automate": False,
                "automation_rate": 0,
                "manual_steps": [],
                "reasons": [],
            }
        }

        # Check for automation blockers
        blockers = []

        # Check forbidden items
        forbidden_lower = [f.lower() for f in analysis.forbidden_items]
        if any("ai" in f or "æœºå™¨" in f or "è‡ªåŠ¨" in f for f in forbidden_lower):
            blockers.append({
                "type": "forbidden_ai",
                "description": "éœ€æ±‚æ˜ç¡®ç¦æ­¢ä½¿ç”¨AIç”Ÿæˆ",
                "impact": "æ‰€æœ‰å†…å®¹å¿…é¡»äººå·¥åˆ›ä½œ"
            })

        # Check if needs human creativity
        cognitive_lower = " ".join(analysis.cognitive_requirements).lower()
        if "åˆ›æ„" in cognitive_lower or "åˆ›ä½œ" in cognitive_lower or "åŸåˆ›" in cognitive_lower:
            blockers.append({
                "type": "creativity_required",
                "description": "ä»»åŠ¡éœ€è¦äººç±»åˆ›æ„",
                "impact": "æ ¸å¿ƒå†…å®¹å¿…é¡»äººå·¥åˆ›ä½œï¼ŒAIå¯è¾…åŠ©æ ¼å¼åŒ–"
            })

        # Check if needs expert knowledge
        if analysis.estimated_difficulty in ["expert", "hard"]:
            if "ä¸“ä¸š" in cognitive_lower or "é¢†åŸŸ" in cognitive_lower:
                blockers.append({
                    "type": "expert_knowledge",
                    "description": "éœ€è¦ä¸“ä¸šé¢†åŸŸçŸ¥è¯†",
                    "impact": "éœ€è¦é¢†åŸŸä¸“å®¶å‚ä¸å†…å®¹å®¡æ ¸"
                })

        # Check if has difficulty validation
        if analysis.has_difficulty_validation():
            blockers.append({
                "type": "difficulty_validation",
                "description": f"éœ€è¦ä½¿ç”¨ {analysis.difficulty_validation.get('model', 'æŒ‡å®šæ¨¡å‹')} è¿›è¡Œéš¾åº¦éªŒè¯",
                "impact": "æ¯æ¡æ•°æ®éœ€è¦é¢å¤–çš„æ¨¡å‹æµ‹è¯•æ­¥éª¤"
            })

        result["blockers"] = blockers

        # Determine automation rate based on blockers
        if not blockers:
            result["overall_rate"] = 80
            result["default"]["can_automate"] = True
            result["default"]["automation_rate"] = 80
            result["default"]["manual_steps"] = [
                {"step": "quality_review", "reason": "æœ€ç»ˆè´¨é‡æŠŠå…³éœ€è¦äººå·¥ç¡®è®¤"}
            ]
        elif any(b["type"] == "forbidden_ai" for b in blockers):
            result["overall_rate"] = 10
            result["default"]["can_automate"] = False
            result["default"]["automation_rate"] = 10
            result["default"]["manual_steps"] = [
                {"step": "content_creation", "reason": "éœ€æ±‚ç¦æ­¢AIç”Ÿæˆï¼Œå¿…é¡»äººå·¥åˆ›ä½œ"},
                {"step": "quality_review", "reason": "äººå·¥è´¨æ£€"}
            ]
            result["default"]["reasons"] = ["éœ€æ±‚æ˜ç¡®ç¦æ­¢AIå‚ä¸å†…å®¹ç”Ÿæˆ"]
        elif any(b["type"] == "creativity_required" for b in blockers):
            result["overall_rate"] = 30
            result["default"]["can_automate"] = False
            result["default"]["automation_rate"] = 30
            result["default"]["manual_steps"] = [
                {"step": "content_creation", "reason": "éœ€è¦äººç±»åˆ›æ„"},
                {"step": "quality_review", "reason": "åˆ›æ„å†…å®¹éœ€äººå·¥è¯„ä¼°"}
            ]
            result["default"]["reasons"] = ["ä»»åŠ¡éœ€è¦äººç±»åˆ›æ„ï¼ŒAIä»…å¯è¾…åŠ©æ ¼å¼åŒ–"]
        else:
            result["overall_rate"] = 50
            result["default"]["can_automate"] = True
            result["default"]["automation_rate"] = 50
            result["default"]["manual_steps"] = [
                {"step": "expert_review", "reason": "ä¸“ä¸šå†…å®¹éœ€è¦ä¸“å®¶å®¡æ ¸"},
                {"step": "difficulty_validation", "reason": "éœ€è¦è¿›è¡Œéš¾åº¦éªŒè¯æµ‹è¯•"}
            ]

        return result

    def _extract_task_types(self, analysis: SpecificationAnalysis) -> list:
        """Extract task types from analysis."""
        task_types = []

        # Check fields for task_type field
        for field in analysis.fields:
            if field.get("name") == "task_type":
                desc = field.get("description", "")
                # Extract types from description like "understanding/editing/generation"
                if "/" in desc:
                    parts = desc.split("ï¼š")[-1] if "ï¼š" in desc else desc
                    task_types = [t.strip() for t in parts.split("/")]
                    break

        # If no task types found, use examples
        if not task_types and analysis.examples:
            seen = set()
            for ex in analysis.examples:
                if ex.get("task_type") and ex["task_type"] not in seen:
                    task_types.append(ex["task_type"])
                    seen.add(ex["task_type"])

        # Default to generic if nothing found
        if not task_types:
            task_types = ["default"]

        return task_types

    def _generate_single_sample(
        self,
        analysis: SpecificationAnalysis,
        task_type: str,
        sample_index: int,
        automation_info: dict,
    ) -> dict:
        """Generate a single sample entry (schema-driven)."""
        sample_id = f"SAMPLE_{sample_index:03d}"

        # Use field_definitions for schema-driven generation
        field_defs = analysis.field_definitions
        if not field_defs:
            profile = get_task_profile(analysis.dataset_type)
            field_defs = [FieldDefinition.from_dict(f) for f in profile.default_fields]

        context = {"task_type": task_type, "sample_index": sample_index}
        data_fields: Dict[str, Any] = {}
        for fd in field_defs:
            name = fd.name
            if name == "task_type":
                data_fields[name] = task_type
            elif fd.type == "image":
                data_fields[name] = f"[å›¾ç‰‡å ä½ç¬¦: {fd.description}]"
            elif "svg" in name.lower() or "code" in name.lower():
                data_fields[name] = self._generate_sample_svg(task_type, sample_index)
            elif "instruction" in name.lower() or "question" in name.lower():
                data_fields[name] = self._generate_sample_instruction(analysis, task_type, sample_index)
            else:
                data_fields[name] = self._sample_placeholder(fd, context)

        # Determine automation status for this sample
        can_automate = automation_info.get("can_automate", False)
        automation_rate = automation_info.get("automation_rate", 0)

        sample = {
            "id": sample_id,
            "task_type": task_type,
            "data": data_fields,
            "think_process": {
                "step_1_parse_input": f"è§£æè¾“å…¥æ•°æ®ï¼Œè¯†åˆ«ä»»åŠ¡ç±»å‹ä¸º {task_type}",
                "step_2_understand_task": f"ç†è§£ä»»åŠ¡è¦æ±‚: {analysis.task_description[:100] if analysis.task_description else 'æ‰§è¡ŒæŒ‡å®šä»»åŠ¡'}...",
                "step_3_execute": "æ‰§è¡Œä»»åŠ¡é€»è¾‘ï¼Œç”Ÿæˆè¾“å‡º",
                "step_4_validate": "éªŒè¯è¾“å‡ºç¬¦åˆè´¨é‡çº¦æŸ",
                "step_5_format": "æ ¼å¼åŒ–è¾“å‡ºä¸ºç›®æ ‡æ ¼å¼",
            },
            "automation_status": {
                "can_fully_automate": can_automate and automation_rate >= 80,
                "automation_rate": automation_rate,
                "automated_steps": [],
                "manual_steps": [],
            },
            "metadata": {
                "difficulty": analysis.estimated_difficulty,
                "domain": analysis.estimated_domain,
                "generated_at": datetime.now().isoformat(),
            }
        }

        # Fill in automation details
        if can_automate and automation_rate >= 80:
            sample["automation_status"]["automated_steps"] = [
                {"step": "input_parsing", "method": "è§„åˆ™è§£æ"},
                {"step": "task_execution", "method": "è‡ªåŠ¨åŒ–è„šæœ¬"},
                {"step": "output_formatting", "method": "æ¨¡æ¿ç”Ÿæˆ"},
            ]
            sample["automation_status"]["manual_steps"] = [
                {
                    "step": "quality_review",
                    "reason": "æœ€ç»ˆè´¨é‡éœ€äººå·¥ç¡®è®¤",
                    "effort": "ä½ (æŠ½æ£€å³å¯)"
                }
            ]
        else:
            manual_steps = automation_info.get("manual_steps", [])
            sample["automation_status"]["manual_steps"] = [
                {
                    "step": ms.get("step", "unknown"),
                    "reason": ms.get("reason", "éœ€è¦äººå·¥å‚ä¸"),
                    "effort": "ä¸­" if ms.get("step") != "content_creation" else "é«˜"
                }
                for ms in manual_steps
            ]

            # What can still be automated
            sample["automation_status"]["automated_steps"] = [
                {"step": "input_parsing", "method": "è§„åˆ™è§£æ"},
                {"step": "output_formatting", "method": "æ¨¡æ¿ç”Ÿæˆ"},
            ]

        return sample

    def _generate_sample_svg(self, task_type: str, index: int) -> str:
        """Generate a sample SVG code."""
        colors = ["red", "blue", "green", "yellow", "purple", "orange"]
        color = colors[index % len(colors)]

        if "edit" in task_type.lower() or "editing" in task_type.lower():
            return f'''<svg width="100" height="100" xmlns="http://www.w3.org/2000/svg">
  <circle cx="50" cy="50" r="40" fill="{color}" />
</svg>'''
        elif "generation" in task_type.lower() or "ç”Ÿæˆ" in task_type:
            return "[å¾…ç”Ÿæˆçš„SVGä»£ç ]"
        else:
            return f'''<svg width="100" height="100" xmlns="http://www.w3.org/2000/svg">
  <rect x="10" y="10" width="80" height="80" fill="{color}" />
  <circle cx="50" cy="50" r="20" fill="white" />
</svg>'''

    def _generate_sample_instruction(self, analysis: SpecificationAnalysis, task_type: str, index: int) -> str:
        """Generate a sample instruction based on task type."""
        # Try to use examples from analysis
        for ex in analysis.examples:
            if ex.get("task_type") == task_type and ex.get("question"):
                return ex["question"]

        # Generate based on task type
        instructions = {
            "understanding": [
                "åˆ†æä»¥ä¸‹SVGä»£ç ï¼Œæè¿°å…¶ä¸­åŒ…å«çš„å›¾å½¢å…ƒç´ ",
                "è§£é‡Šè¿™æ®µSVGä»£ç å®ç°çš„è§†è§‰æ•ˆæœ",
                "è¯†åˆ«SVGä¸­ä½¿ç”¨çš„é¢œè‰²å’Œå½¢çŠ¶",
            ],
            "editing": [
                "å°†åœ†å½¢çš„é¢œè‰²æ”¹ä¸ºçº¢è‰²",
                "å¢åŠ çŸ©å½¢çš„å®½åº¦ä¸ºåŸæ¥çš„1.5å€",
                "ä¸ºæ‰€æœ‰å½¢çŠ¶æ·»åŠ 2pxçš„é»‘è‰²è¾¹æ¡†",
            ],
            "generation": [
                "åˆ›å»ºä¸€ä¸ªåŒ…å«è“è‰²çŸ©å½¢å’Œçº¢è‰²åœ†å½¢çš„SVG",
                "ç”Ÿæˆä¸€ä¸ªç®€å•çš„ç¬‘è„¸å›¾æ ‡",
                "ç»˜åˆ¶ä¸€ä¸ªä¸‰è§’å½¢è­¦å‘Šæ ‡å¿—",
            ],
            "default": [
                f"æ‰§è¡Œä»»åŠ¡ {index}",
            ]
        }

        task_key = task_type.lower()
        for key in instructions:
            if key in task_key:
                return instructions[key][index % len(instructions[key])]

        return instructions["default"][0]

    def _generate_production_notes(self, analysis: SpecificationAnalysis, automation_analysis: dict) -> dict:
        """Generate production notes based on analysis."""
        overall_rate = automation_analysis.get("overall_rate", 0)
        blockers = automation_analysis.get("blockers", [])

        if overall_rate >= 80:
            recommendation = "å¯ä»¥æ‰¹é‡è‡ªåŠ¨åŒ–ç”Ÿäº§"
            workflow = "è‡ªåŠ¨ç”Ÿæˆ â†’ æŠ½æ£€å®¡æ ¸ â†’ æ‰¹é‡æäº¤"
        elif overall_rate >= 50:
            recommendation = "åŠè‡ªåŠ¨åŒ–ç”Ÿäº§ï¼Œéœ€è¦äººå·¥å‚ä¸å…³é”®ç¯èŠ‚"
            workflow = "è‡ªåŠ¨ç”Ÿæˆåˆç¨¿ â†’ äººå·¥å®¡æ ¸/ä¿®æ”¹ â†’ è´¨æ£€ â†’ æäº¤"
        elif overall_rate >= 30:
            recommendation = "ä»¥äººå·¥ä¸ºä¸»ï¼ŒAIè¾…åŠ©"
            workflow = "äººå·¥åˆ›ä½œ â†’ AIè¾…åŠ©æ ¼å¼åŒ– â†’ è´¨æ£€ â†’ æäº¤"
        else:
            recommendation = "éœ€è¦å…¨äººå·¥ç”Ÿäº§"
            workflow = "äººå·¥åˆ›ä½œ â†’ äº¤å‰å®¡æ ¸ â†’ è´¨æ£€ â†’ æäº¤"

        return {
            "recommendation": recommendation,
            "suggested_workflow": workflow,
            "key_blockers": [b["description"] for b in blockers],
            "optimization_suggestions": self._get_optimization_suggestions(analysis, automation_analysis),
        }

    def _get_optimization_suggestions(self, analysis: SpecificationAnalysis, automation_analysis: dict) -> list:
        """Get suggestions for optimizing the production process."""
        suggestions = []

        if automation_analysis.get("overall_rate", 0) < 50:
            suggestions.append({
                "area": "æ¨¡æ¿åŒ–",
                "suggestion": "åˆ›å»ºæ ‡å‡†æ¨¡æ¿å‡å°‘é‡å¤åŠ³åŠ¨",
                "impact": "å¯æå‡10-20%æ•ˆç‡"
            })

        if analysis.has_difficulty_validation():
            suggestions.append({
                "area": "éš¾åº¦éªŒè¯",
                "suggestion": "æ‰¹é‡è¿è¡Œéš¾åº¦éªŒè¯è€Œéé€æ¡æµ‹è¯•",
                "impact": "å¯èŠ‚çœ50%éªŒè¯æ—¶é—´"
            })

        if len(analysis.fields) > 5:
            suggestions.append({
                "area": "å­—æ®µç®€åŒ–",
                "suggestion": "è€ƒè™‘ä½¿ç”¨é»˜è®¤å€¼å‡å°‘å¿…å¡«é¡¹",
                "impact": "å¯æå‡æ ‡æ³¨æ•ˆç‡"
            })

        suggestions.append({
            "area": "è´¨æ£€æŠ½æ ·",
            "suggestion": "ä½¿ç”¨åˆ†å±‚æŠ½æ ·ä»£æ›¿å…¨é‡æ£€æŸ¥",
            "impact": "è´¨æ£€æ•ˆç‡æå‡60%ä»¥ä¸Š"
        })

        return suggestions

    def _generate_samples_guide(
        self,
        analysis: SpecificationAnalysis,
        output_dir: str,
        subdirs: dict,
        samples_doc: dict,
        result: SpecOutputResult,
    ):
        """Generate human-readable SAMPLE_GUIDE.md."""
        lines = []
        lines.append(f"# {analysis.project_name} æ ·ä¾‹æ•°æ®æŒ‡å—")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> æ ·ä¾‹æ•°é‡: {samples_doc['_meta']['total_samples']} æ¡")
        lines.append(f"> ç›®æ ‡è§„æ¨¡: {samples_doc['_meta']['target_size']} æ¡")
        lines.append("")
        lines.append("---")
        lines.append("")

        # Automation summary
        auto_summary = samples_doc["automation_summary"]
        lines.append("## è‡ªåŠ¨åŒ–è¯„ä¼°")
        lines.append("")

        rate = auto_summary["overall_automation_rate"]
        if rate >= 80:
            status = "ğŸŸ¢ é«˜åº¦è‡ªåŠ¨åŒ–"
        elif rate >= 50:
            status = "ğŸŸ¡ åŠè‡ªåŠ¨åŒ–"
        elif rate >= 30:
            status = "ğŸŸ  ä½è‡ªåŠ¨åŒ–"
        else:
            status = "ğŸ”´ éœ€äººå·¥ç”Ÿäº§"

        lines.append(f"**è‡ªåŠ¨åŒ–ç¨‹åº¦**: {status} ({rate}%)")
        lines.append("")

        # Blockers
        if auto_summary["automation_blockers"]:
            lines.append("### è‡ªåŠ¨åŒ–é˜»å¡å› ç´ ")
            lines.append("")
            for blocker in auto_summary["automation_blockers"]:
                lines.append(f"- **{blocker['type']}**: {blocker['description']}")
                lines.append(f"  - å½±å“: {blocker['impact']}")
            lines.append("")

        # Production notes
        prod_notes = samples_doc["production_notes"]
        lines.append("### ç”Ÿäº§å»ºè®®")
        lines.append("")
        lines.append(f"**å»ºè®®**: {prod_notes['recommendation']}")
        lines.append("")
        lines.append(f"**å·¥ä½œæµ**: `{prod_notes['suggested_workflow']}`")
        lines.append("")

        # Manual steps explanation
        lines.append("---")
        lines.append("")
        lines.append("## äººå·¥å‚ä¸è¯´æ˜")
        lines.append("")
        lines.append("ä»¥ä¸‹æ­¥éª¤éœ€è¦äººå·¥å‚ä¸:")
        lines.append("")

        # Collect all manual steps from samples
        manual_steps_seen = {}
        for sample in samples_doc["samples"][:5]:  # Check first 5 samples
            for ms in sample["automation_status"]["manual_steps"]:
                step = ms["step"]
                if step not in manual_steps_seen:
                    manual_steps_seen[step] = ms

        if manual_steps_seen:
            lines.append("| æ­¥éª¤ | åŸå›  | å·¥ä½œé‡ |")
            lines.append("|------|------|--------|")
            for step, info in manual_steps_seen.items():
                lines.append(f"| {step} | {info['reason']} | {info.get('effort', 'ä¸­')} |")
            lines.append("")
        else:
            lines.append("æ— éœ€äººå·¥å‚ä¸æ ¸å¿ƒç”Ÿäº§æ­¥éª¤ï¼Œä»…éœ€æŠ½æ£€å®¡æ ¸ã€‚")
            lines.append("")

        # Sample examples
        lines.append("---")
        lines.append("")
        lines.append("## æ ·ä¾‹å±•ç¤º")
        lines.append("")
        lines.append("ä»¥ä¸‹æ˜¯ç”Ÿæˆçš„æ ·ä¾‹æ•°æ®ç¤ºä¾‹:")
        lines.append("")

        for i, sample in enumerate(samples_doc["samples"][:3], 1):
            lines.append(f"### æ ·ä¾‹ {i}: {sample['id']}")
            lines.append("")
            lines.append(f"**ä»»åŠ¡ç±»å‹**: `{sample['task_type']}`")
            lines.append("")

            # Show data fields
            lines.append("**æ•°æ®å­—æ®µ**:")
            lines.append("")
            lines.append("```json")
            # Pretty print the data
            import json
            lines.append(json.dumps(sample["data"], indent=2, ensure_ascii=False))
            lines.append("```")
            lines.append("")

            # Show think process
            lines.append("**æ¨ç†æ­¥éª¤**:")
            lines.append("")
            for step_name, step_desc in sample["think_process"].items():
                step_num = step_name.split("_")[1]
                lines.append(f"{step_num}. {step_desc}")
            lines.append("")

            # Automation status
            auto_status = sample["automation_status"]
            if auto_status["can_fully_automate"]:
                lines.append("**è‡ªåŠ¨åŒ–çŠ¶æ€**: âœ… å¯å®Œå…¨è‡ªåŠ¨åŒ–")
            else:
                lines.append(f"**è‡ªåŠ¨åŒ–çŠ¶æ€**: âš ï¸ éƒ¨åˆ†è‡ªåŠ¨åŒ– ({auto_status['automation_rate']}%)")
                if auto_status["manual_steps"]:
                    lines.append("")
                    lines.append("éœ€äººå·¥å¤„ç†:")
                    for ms in auto_status["manual_steps"]:
                        lines.append(f"- **{ms['step']}**: {ms['reason']}")
            lines.append("")
            lines.append("---")
            lines.append("")

        # Optimization suggestions
        if prod_notes.get("optimization_suggestions"):
            lines.append("## ä¼˜åŒ–å»ºè®®")
            lines.append("")
            for sug in prod_notes["optimization_suggestions"]:
                lines.append(f"### {sug['area']}")
                lines.append("")
                lines.append(f"- **å»ºè®®**: {sug['suggestion']}")
                lines.append(f"- **é¢„æœŸæ•ˆæœ**: {sug['impact']}")
                lines.append("")

        # File reference
        lines.append("---")
        lines.append("")
        lines.append("## æ–‡ä»¶è¯´æ˜")
        lines.append("")
        lines.append("| æ–‡ä»¶ | ç”¨é€” | æ¶ˆè´¹è€… |")
        lines.append("|------|------|--------|")
        lines.append("| `samples.json` | æœºå™¨å¯è§£æçš„å®Œæ•´æ ·ä¾‹ | AI Agent |")
        lines.append("| `SAMPLE_GUIDE.md` | äººç±»å¯è¯»çš„æ ·ä¾‹æŒ‡å— | æ ‡æ³¨å›¢é˜Ÿ/é¡¹ç›®ç»ç† |")
        lines.append("")
        lines.append("---")
        lines.append("")
        lines.append("> æœ¬æŒ‡å—ç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ")

        path = os.path.join(output_dir, subdirs["samples"], "SAMPLE_GUIDE.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(f"{subdirs['samples']}/SAMPLE_GUIDE.md")
