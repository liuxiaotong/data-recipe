"""Command-line interface for DataRecipe."""

import sys
from pathlib import Path

import click
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

from datarecipe.analyzer import DatasetAnalyzer
from datarecipe.schema import Recipe


console = Console()


def validate_output_path(output: str, base_dir: Path = None) -> Path:
    """Validate and resolve output path to prevent path traversal attacks.

    Args:
        output: User-provided output path
        base_dir: Optional base directory to restrict outputs to

    Returns:
        Resolved Path object

    Raises:
        ValueError: If path is invalid or attempts traversal outside base_dir
    """
    output_path = Path(output).resolve()

    # If base_dir specified, ensure output is within it
    if base_dir:
        base_resolved = base_dir.resolve()
        try:
            output_path.relative_to(base_resolved)
        except ValueError:
            raise ValueError(
                f"Output path '{output}' is outside allowed directory '{base_dir}'"
            )

    # Block obviously dangerous paths
    dangerous_patterns = ["/etc/", "/usr/", "/bin/", "/var/", "/root/"]
    output_str = str(output_path)
    for pattern in dangerous_patterns:
        if output_str.startswith(pattern):
            raise ValueError(f"Output path '{output}' is in a protected system directory")

    return output_path


def recipe_to_markdown(recipe: Recipe) -> str:
    """Generate a beautiful Markdown document for a recipe in Chinese."""
    lines = []

    # Title
    lines.append(f"# ğŸ“Š æ•°æ®é›†é…æ–¹åˆ†æï¼š{recipe.name}")
    lines.append("")

    # Summary box
    lines.append("> **DataRecipe æ•°æ®é›†æˆåˆ†åˆ†ææŠ¥å‘Š**")
    lines.append("> ")
    lines.append("> æ·±å…¥åˆ†æè¯¥æ•°æ®é›†çš„æ„å»ºæ–¹å¼â€”â€”æ•°æ®æ¥æºã€ç”Ÿæˆæ–¹æ³•ä¸å¯å¤ç°æ€§è¯„ä¼°ã€‚")
    lines.append("")

    # Basic Info
    lines.append("## ğŸ“‹ åŸºæœ¬ä¿¡æ¯")
    lines.append("")
    lines.append("| å±æ€§ | å€¼ |")
    lines.append("|------|-----|")
    lines.append(f"| **æ•°æ®é›†åç§°** | `{recipe.name}` |")
    lines.append(f"| **æ•°æ®æ¥æº** | {recipe.source_type.value.title()} |")
    if recipe.license:
        lines.append(f"| **è®¸å¯è¯** | {recipe.license} |")
    if recipe.languages:
        langs = [l for l in recipe.languages if l]
        if langs:
            lines.append(f"| **è¯­è¨€** | {', '.join(langs)} |")
    if recipe.num_examples:
        lines.append(f"| **æ ·æœ¬æ•°é‡** | {recipe.num_examples:,} |")
    lines.append("")

    # Generation Method
    lines.append("## ğŸ§¬ æ•°æ®ç”Ÿæˆæ–¹å¼")
    lines.append("")

    if recipe.synthetic_ratio is not None or recipe.human_ratio is not None:
        synthetic_pct = (recipe.synthetic_ratio or 0) * 100
        human_pct = (recipe.human_ratio or 0) * 100

        # Progress bar visualization (PDF-safe format)
        synthetic_filled = int(synthetic_pct / 5)
        human_filled = int(human_pct / 5)
        synthetic_bar = "[" + "=" * synthetic_filled + "-" * (20 - synthetic_filled) + "]"
        human_bar = "[" + "=" * human_filled + "-" * (20 - human_filled) + "]"

        lines.append("| ç±»å‹ | å æ¯” | åˆ†å¸ƒ |")
        lines.append("|------|------|------|")
        lines.append(f"| åˆæˆæ•°æ® | {synthetic_pct:.0f}% | `{synthetic_bar}` |")
        lines.append(f"| äººå·¥æ ‡æ³¨ | {human_pct:.0f}% | `{human_bar}` |")
    else:
        lines.append("*æ— æ³•ä»ç°æœ‰å…ƒæ•°æ®ä¸­ç¡®å®šç”Ÿæˆæ–¹å¼ã€‚*")
    lines.append("")

    # Teacher Models
    lines.append("## ğŸ“ æ•™å¸ˆæ¨¡å‹")
    lines.append("")

    if recipe.teacher_models:
        lines.append("æ£€æµ‹åˆ°ä»¥ä¸‹ AI æ¨¡å‹è¢«ç”¨äºæ•°æ®ç”Ÿæˆï¼š")
        lines.append("")
        for model in recipe.teacher_models:
            lines.append(f"- **{model}**")
    else:
        lines.append("*æœªåœ¨æ•°æ®é›†æ–‡æ¡£ä¸­æ£€æµ‹åˆ°æ•™å¸ˆæ¨¡å‹ã€‚*")
    lines.append("")

    # Generation Methods Detail
    if recipe.generation_methods:
        method_type_map = {
            "distillation": "çŸ¥è¯†è’¸é¦",
            "human_annotation": "äººå·¥æ ‡æ³¨",
            "web_scrape": "ç½‘é¡µæŠ“å–",
            "red_teaming": "çº¢é˜Ÿæµ‹è¯•",
        }
        lines.append("### ç”Ÿæˆæµç¨‹")
        lines.append("")
        for i, method in enumerate(recipe.generation_methods, 1):
            method_name = method_type_map.get(method.method_type, method.method_type.replace('_', ' ').title())
            lines.append(f"**æ­¥éª¤ {i}ï¼š{method_name}**")
            if method.teacher_model:
                lines.append(f"- æ•™å¸ˆæ¨¡å‹ï¼š`{method.teacher_model}`")
            if method.platform:
                lines.append(f"- æ ‡æ³¨å¹³å°ï¼š{method.platform}")
            if method.prompt_template_available:
                lines.append(f"- æç¤ºè¯æ¨¡æ¿ï¼šâœ… å¯ç”¨")
            lines.append("")

    # Cost Estimation
    lines.append("## ğŸ’° æˆæœ¬ä¼°ç®—")
    lines.append("")

    if recipe.cost and recipe.cost.estimated_total_usd:
        if recipe.cost.confidence == "low":
            low = recipe.cost.estimated_total_usd * 0.5
            high = recipe.cost.estimated_total_usd * 1.5
            lines.append(f"**é¢„ä¼°æ€»æˆæœ¬ï¼š${low:,.0f} - ${high:,.0f}** *(ä½ç½®ä¿¡åº¦)*")
        elif recipe.cost.confidence == "medium":
            low = recipe.cost.estimated_total_usd * 0.8
            high = recipe.cost.estimated_total_usd * 1.2
            lines.append(f"**é¢„ä¼°æ€»æˆæœ¬ï¼š${low:,.0f} - ${high:,.0f}** *(ä¸­ç½®ä¿¡åº¦)*")
        else:
            lines.append(f"**é¢„ä¼°æ€»æˆæœ¬ï¼š${recipe.cost.estimated_total_usd:,.0f}**")
        lines.append("")

        lines.append("| ç±»åˆ« | æˆæœ¬ |")
        lines.append("|------|------|")
        if recipe.cost.api_calls_usd:
            lines.append(f"| API è°ƒç”¨ | ${recipe.cost.api_calls_usd:,.0f} |")
        if recipe.cost.human_annotation_usd:
            lines.append(f"| äººå·¥æ ‡æ³¨ | ${recipe.cost.human_annotation_usd:,.0f} |")
        if recipe.cost.compute_usd:
            lines.append(f"| è®¡ç®—èµ„æº | ${recipe.cost.compute_usd:,.0f} |")
    else:
        lines.append("*æš‚æ— æˆæœ¬ä¼°ç®—æ•°æ®ã€‚*")
    lines.append("")

    # Reproducibility
    lines.append("## ğŸ”„ å¯å¤ç°æ€§è¯„ä¼°")
    lines.append("")

    if recipe.reproducibility:
        score = recipe.reproducibility.score
        score_bar = "[" + "#" * score + "-" * (10 - score) + "]"
        lines.append(f"### è¯„åˆ†ï¼š{score}/10")
        lines.append("")
        lines.append(f"**{score_bar}**")
        lines.append("")

        # Translation map for reproducibility items
        item_translation = {
            "description": "æ•°æ®é›†æè¿°",
            "detailed_documentation": "è¯¦ç»†æ–‡æ¡£",
            "source_code_reference": "æºä»£ç å¼•ç”¨",
            "teacher_model_names": "æ•™å¸ˆæ¨¡å‹åç§°",
            "teacher_model_info": "æ•™å¸ˆæ¨¡å‹ä¿¡æ¯",
            "prompt_templates": "æç¤ºè¯æ¨¡æ¿",
            "exact_prompts": "ç²¾ç¡®æç¤ºè¯",
            "filtering_criteria": "è¿‡æ»¤æ ‡å‡†",
            "quality_thresholds": "è´¨é‡é˜ˆå€¼",
            "generation_scripts": "ç”Ÿæˆè„šæœ¬",
            "source_data_references": "æºæ•°æ®å¼•ç”¨",
            "general_methodology": "é€šç”¨æ–¹æ³•è®º",
            "dataset_statistics": "æ•°æ®é›†ç»Ÿè®¡",
        }

        if recipe.reproducibility.available:
            lines.append("#### âœ… å·²æä¾›çš„ä¿¡æ¯")
            lines.append("")
            for item in recipe.reproducibility.available:
                translated = item_translation.get(item, item.replace('_', ' ').title())
                lines.append(f"- {translated}")
            lines.append("")

        if recipe.reproducibility.missing:
            lines.append("#### âŒ ç¼ºå¤±çš„ä¿¡æ¯")
            lines.append("")
            for item in recipe.reproducibility.missing:
                translated = item_translation.get(item, item.replace('_', ' ').title())
                lines.append(f"- {translated}")
            lines.append("")

        if recipe.reproducibility.notes:
            lines.append("#### ğŸ“ å¤‡æ³¨")
            lines.append("")
            lines.append(recipe.reproducibility.notes)
            lines.append("")
    else:
        lines.append("*æš‚æ— å¯å¤ç°æ€§è¯„ä¼°ã€‚*")

    # Footer
    lines.append("---")
    lines.append("")
    lines.append("> ç”± [DataRecipe](https://github.com/yourusername/data-recipe) ç”Ÿæˆ â€” AI æ•°æ®é›†æˆåˆ†åˆ†æå™¨")

    return "\n".join(lines)


def display_recipe(recipe: Recipe) -> None:
    """Display a recipe in a formatted panel."""
    # Build the content
    lines = []

    # Header info
    lines.append(f"[bold]Name:[/bold] {recipe.name}")
    lines.append(f"[bold]Source:[/bold] {recipe.source_type.value}")
    lines.append("")

    # Generation Method
    lines.append("[bold cyan]ğŸ“Š Generation Method:[/bold cyan]")
    if recipe.synthetic_ratio is not None:
        lines.append(f"   â€¢ Synthetic: {recipe.synthetic_ratio * 100:.0f}%")
    if recipe.human_ratio is not None:
        lines.append(f"   â€¢ Human: {recipe.human_ratio * 100:.0f}%")
    if recipe.generation_type.value == "unknown":
        lines.append("   â€¢ [dim]Unable to determine[/dim]")
    lines.append("")

    # Teacher Models
    lines.append("[bold cyan]ğŸ¤– Teacher Models:[/bold cyan]")
    if recipe.teacher_models:
        for model in recipe.teacher_models:
            lines.append(f"   â€¢ {model}")
    else:
        lines.append("   â€¢ [dim]None detected[/dim]")
    lines.append("")

    # Cost Estimation
    lines.append("[bold cyan]ğŸ’° Estimated Cost:[/bold cyan]")
    if recipe.cost and recipe.cost.estimated_total_usd:
        # Show as a range for low confidence
        if recipe.cost.confidence == "low":
            low = recipe.cost.estimated_total_usd * 0.5
            high = recipe.cost.estimated_total_usd * 1.5
            lines.append(f"   ${low:,.0f} - ${high:,.0f} [dim](low confidence)[/dim]")
        else:
            lines.append(f"   ${recipe.cost.estimated_total_usd:,.0f}")

        if recipe.cost.api_calls_usd:
            lines.append(f"   [dim]â”œâ”€ API calls: ${recipe.cost.api_calls_usd:,.0f}[/dim]")
        if recipe.cost.human_annotation_usd:
            lines.append(
                f"   [dim]â””â”€ Human annotation: ${recipe.cost.human_annotation_usd:,.0f}[/dim]"
            )
    else:
        lines.append("   [dim]Unable to estimate[/dim]")
    lines.append("")

    # Reproducibility
    lines.append("[bold cyan]ğŸ”„ Reproducibility Score:[/bold cyan]")
    if recipe.reproducibility:
        score = recipe.reproducibility.score
        score_bar = "â–ˆ" * score + "â–‘" * (10 - score)
        lines.append(f"   [{score}/10] {score_bar}")

        if recipe.reproducibility.available:
            lines.append(f"   [green]âœ“ Available:[/green] {', '.join(recipe.reproducibility.available[:3])}")
        if recipe.reproducibility.missing:
            lines.append(f"   [red]âœ— Missing:[/red] {', '.join(recipe.reproducibility.missing[:3])}")
    else:
        lines.append("   [dim]Not assessed[/dim]")

    # Create panel
    content = "\n".join(lines)
    panel = Panel(
        content,
        title="[bold white]Dataset Recipe[/bold white]",
        border_style="cyan",
        padding=(1, 2),
    )
    console.print(panel)


@click.group()
@click.version_option(version="0.2.0", prog_name="datarecipe")
def main():
    """DataRecipe - Analyze AI dataset ingredients, estimate costs, and generate workflows."""
    pass


@main.command()
@click.argument("dataset_id")
@click.option("--output", "-o", type=click.Path(), help="Export recipe to file (auto-detect format by extension)")
@click.option("--json", "as_json", is_flag=True, help="Output as JSON")
@click.option("--yaml", "as_yaml", is_flag=True, help="Output as YAML")
@click.option("--markdown", "--md", "as_markdown", is_flag=True, help="Output as Markdown")
def analyze(dataset_id: str, output: str, as_json: bool, as_yaml: bool, as_markdown: bool):
    """Analyze a dataset and display its recipe.

    DATASET_ID is the identifier of the dataset to analyze.
    For HuggingFace datasets, use the format: org/dataset-name
    """
    analyzer = DatasetAnalyzer()

    with console.status(f"[cyan]Analyzing {dataset_id}...[/cyan]"):
        try:
            recipe = analyzer.analyze(dataset_id)
        except ValueError as e:
            console.print(f"[red]Error:[/red] {e}")
            sys.exit(1)
        except Exception as e:
            console.print(f"[red]Error analyzing dataset:[/red] {e}")
            import traceback
            console.print(f"[dim]{traceback.format_exc()}[/dim]")
            sys.exit(1)

    # Output format
    if as_json:
        import json

        console.print(json.dumps(recipe.to_dict(), indent=2))
    elif as_yaml:
        console.print(recipe.to_yaml())
    elif as_markdown:
        print(recipe_to_markdown(recipe))
    else:
        display_recipe(recipe)

    # Export if requested
    if output:
        output_path = Path(output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        if output.endswith(".md"):
            output_path.write_text(recipe_to_markdown(recipe), encoding="utf-8")
            console.print(f"\n[green]Markdown exported to:[/green] {output}")
        elif output.endswith(".json"):
            import json
            output_path.write_text(json.dumps(recipe.to_dict(), indent=2), encoding="utf-8")
            console.print(f"\n[green]JSON exported to:[/green] {output}")
        else:
            analyzer.export_recipe(recipe, output)
            console.print(f"\n[green]Recipe exported to:[/green] {output}")


@main.command()
@click.argument("recipe_file", type=click.Path(exists=True))
def show(recipe_file: str):
    """Display a recipe from a YAML file.

    RECIPE_FILE is the path to the recipe YAML file.
    """
    analyzer = DatasetAnalyzer()

    try:
        recipe = analyzer.analyze_from_yaml(recipe_file)
        display_recipe(recipe)
    except Exception as e:
        console.print(f"[red]Error loading recipe:[/red] {e}")
        sys.exit(1)


@main.command()
@click.argument("dataset_id")
@click.argument("output_file", type=click.Path())
def export(dataset_id: str, output_file: str):
    """Analyze a dataset and export recipe to YAML.

    DATASET_ID is the identifier of the dataset to analyze.
    OUTPUT_FILE is the path where the YAML recipe will be saved.
    """
    analyzer = DatasetAnalyzer()

    with console.status(f"[cyan]Analyzing {dataset_id}...[/cyan]"):
        try:
            recipe = analyzer.analyze(dataset_id)
        except ValueError as e:
            console.print(f"[red]Error:[/red] {e}")
            sys.exit(1)
        except Exception as e:
            console.print(f"[red]Error analyzing dataset:[/red] {e}")
            import traceback
            console.print(f"[dim]{traceback.format_exc()}[/dim]")
            sys.exit(1)

    analyzer.export_recipe(recipe, output_file)
    console.print(f"[green]Recipe exported to:[/green] {output_file}")


@main.command()
def list_sources():
    """List supported data sources."""
    table = Table(title="Supported Data Sources")
    table.add_column("Source", style="cyan")
    table.add_column("Status", style="green")
    table.add_column("Example Input")

    table.add_row("HuggingFace Hub", "âœ“ Supported", "org/dataset-name æˆ– URL")
    table.add_row("GitHub", "âœ“ Supported", "https://github.com/org/repo")
    table.add_row("Web URL", "âœ“ Supported", "https://example.com/dataset")
    table.add_row("Local files", "âœ“ Supported", "datarecipe create (äº¤äº’å¼)")

    console.print(table)


@main.command()
@click.argument("dataset_id")
@click.option("--output", "-o", type=click.Path(), help="Output file path for production guide")
@click.option("--target-size", "-n", type=int, help="Target dataset size")
def guide(dataset_id: str, output: str, target_size: int):
    """Generate a production guide for recreating a dataset.

    Analyzes a dataset and outputs a step-by-step guide for producing
    similar data, including code snippets, tools, and best practices.

    DATASET_ID can be a HuggingFace ID, GitHub URL, or any web URL.
    """
    from datarecipe.pipeline import get_pipeline_template, pipeline_to_markdown

    analyzer = DatasetAnalyzer()

    with console.status(f"[cyan]Analyzing {dataset_id}...[/cyan]"):
        try:
            recipe = analyzer.analyze(dataset_id)
        except ValueError as e:
            console.print(f"[red]Error:[/red] {e}")
            sys.exit(1)
        except Exception as e:
            console.print(f"[red]Error analyzing dataset:[/red] {e}")
            import traceback
            console.print(f"[dim]{traceback.format_exc()}[/dim]")
            sys.exit(1)

    # Get appropriate pipeline template
    pipeline = get_pipeline_template(
        recipe.generation_type.value if recipe.generation_type else "unknown",
        recipe.synthetic_ratio
    )

    # Customize pipeline with dataset info
    if target_size:
        pipeline.target_size = target_size

    if recipe.cost and recipe.cost.estimated_total_usd:
        pipeline.estimated_total_cost = recipe.cost.estimated_total_usd

    # Generate guide
    guide_content = pipeline_to_markdown(pipeline, recipe.name)

    # Add dataset-specific info at the top
    synthetic_pct = (
        f"{recipe.synthetic_ratio * 100:.0f}%"
        if recipe.synthetic_ratio is not None
        else "N/A"
    )
    human_pct = (
        f"{recipe.human_ratio * 100:.0f}%"
        if recipe.human_ratio is not None
        else "N/A"
    )
    repro_score = (
        f"{recipe.reproducibility.score}/10"
        if recipe.reproducibility
        else "N/A"
    )

    header = f"""# æ•°æ®ç”Ÿäº§æŒ‡å—ï¼š{recipe.name}

## å‚è€ƒæ•°æ®é›†åˆ†æ

| å±æ€§ | å€¼ |
|------|-----|
| **æ•°æ®é›†åç§°** | {recipe.name} |
| **æ¥æº** | {recipe.source_type.value} |
| **åˆæˆæ•°æ®æ¯”ä¾‹** | {synthetic_pct} |
| **äººå·¥æ•°æ®æ¯”ä¾‹** | {human_pct} |
| **æ•™å¸ˆæ¨¡å‹** | {', '.join(recipe.teacher_models) if recipe.teacher_models else 'æ— '} |
| **å¯å¤ç°æ€§è¯„åˆ†** | {repro_score} |

---

"""
    full_guide = header + guide_content.split("# ", 1)[-1]  # Remove duplicate title

    if output:
        output_path = Path(output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(full_guide, encoding="utf-8")
        console.print(f"[green]âœ“ ç”Ÿäº§æŒ‡å—å·²ä¿å­˜åˆ°:[/green] {output}")
    else:
        print(full_guide)

    # Also display summary
    console.print("\n[bold cyan]ç”Ÿäº§æŒ‡å—æ¦‚è¦:[/bold cyan]")
    console.print(f"  æµç¨‹ç±»å‹: {pipeline.name}")
    console.print(f"  æ­¥éª¤æ•°é‡: {len(pipeline.steps)}")
    if pipeline.estimated_total_cost:
        console.print(f"  é¢„ä¼°æˆæœ¬: ${pipeline.estimated_total_cost:,.0f}")


@main.command("deep-guide")
@click.argument("url")
@click.option("--output", "-o", type=click.Path(), help="Output file path for production guide")
@click.option("--llm/--no-llm", default=False, help="Use LLM for enhanced analysis (requires API key)")
@click.option("--provider", type=click.Choice(["anthropic", "openai"]), default="anthropic", help="LLM provider")
def deep_guide(url: str, output: str, llm: bool, provider: str):
    """Generate a customized production guide using deep analysis.

    This command performs deep analysis on a paper or dataset page and
    generates a specialized production guide based on the methodology
    detected in the source.

    URL can be an arXiv paper, dataset page, or any web URL describing
    a dataset's construction methodology.

    Use --llm flag to enable LLM-enhanced analysis for better results.
    Requires ANTHROPIC_API_KEY or OPENAI_API_KEY environment variable.

    Examples:
        datarecipe deep-guide https://arxiv.org/abs/2506.07982
        datarecipe deep-guide https://arcprize.org/arc-agi/2/ --llm
    """
    from datarecipe.deep_analyzer import deep_analysis_to_markdown

    # Try to use LLMAnalyzer with PDF parsing (even without LLM)
    try:
        from datarecipe.llm_analyzer import LLMAnalyzer
        if llm:
            console.print(f"[cyan]ä½¿ç”¨ LLM å¢å¼ºåˆ†æ (provider: {provider})...[/cyan]")
            analyzer = LLMAnalyzer(use_llm=True, llm_provider=provider, parse_pdf=True)
        else:
            console.print("[cyan]ä½¿ç”¨ PDF è§£æå’Œå¤šæºèšåˆåˆ†æ...[/cyan]")
            analyzer = LLMAnalyzer(use_llm=False, parse_pdf=True)
    except ImportError as e:
        if llm:
            console.print(f"[yellow]Warning:[/yellow] {e}")
        console.print("[yellow]ä½¿ç”¨åŸºç¡€æ¨¡å¼åŒ¹é…åˆ†æ...[/yellow]")
        from datarecipe.deep_analyzer import DeepAnalyzer
        analyzer = DeepAnalyzer()

    with console.status(f"[cyan]Performing deep analysis on {url}...[/cyan]"):
        try:
            result = analyzer.analyze(url)
        except ValueError as e:
            console.print(f"[red]Error:[/red] {e}")
            sys.exit(1)
        except Exception as e:
            console.print(f"[red]Error during analysis:[/red] {e}")
            sys.exit(1)

    # Generate customized guide
    guide_content = deep_analysis_to_markdown(result)

    if output:
        output_path = Path(output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(guide_content, encoding="utf-8")
        console.print(f"[green]âœ“ ä¸“é¡¹ç”Ÿäº§æŒ‡å—å·²ä¿å­˜åˆ°:[/green] {output}")
    else:
        print(guide_content)

    # Display summary
    console.print("\n[bold cyan]æ·±åº¦åˆ†ææ¦‚è¦:[/bold cyan]")
    console.print(f"  æ•°æ®é›†åç§°: {result.name}")
    console.print(f"  åˆ†ç±»: {result.category.value}")
    console.print(f"  é¢†åŸŸ: {result.domain or 'é€šç”¨'}")
    if result.methodology:
        console.print(f"  æ–¹æ³•è®º: {result.methodology}")
    if result.key_innovations:
        console.print(f"  æ ¸å¿ƒåˆ›æ–°: {len(result.key_innovations)} é¡¹")
    if result.generation_steps:
        console.print(f"  ç”Ÿäº§æ­¥éª¤: {len(result.generation_steps)} æ­¥")
    if result.code_available:
        console.print(f"  ä»£ç å¯ç”¨: âœ“ {result.code_url or ''}")
    if result.data_available:
        console.print(f"  æ•°æ®å¯ç”¨: âœ“ {result.data_url or ''}")
    if hasattr(result, 'paper_url') and result.paper_url:
        console.print(f"  [green]è‡ªåŠ¨å‘ç°è®ºæ–‡:[/green] {result.paper_url}")


@main.command()
@click.option("--output", "-o", type=click.Path(), help="Output YAML file path")
def create(output: str):
    """Interactively create a dataset recipe.

    This command guides you through creating a recipe file step by step.
    """
    from rich.prompt import Prompt, Confirm, IntPrompt, FloatPrompt

    console.print("\n[bold cyan]ğŸ“ åˆ›å»ºæ•°æ®é›†é…æ–¹ / Create Dataset Recipe[/bold cyan]\n")

    # Basic info
    name = Prompt.ask("æ•°æ®é›†åç§° / Dataset name")
    version = Prompt.ask("ç‰ˆæœ¬ / Version", default="1.0")

    # Source
    console.print("\n[bold]æ•°æ®æ¥æº / Data Source[/bold]")
    source_type = Prompt.ask(
        "æ¥æºç±»å‹ / Source type",
        choices=["huggingface", "github", "web", "local"],
        default="local"
    )
    source_id = Prompt.ask("æ¥æºæ ‡è¯† / Source ID (URL or ID)", default="")

    # Generation
    console.print("\n[bold]ç”Ÿæˆæ–¹å¼ / Generation Method[/bold]")
    synthetic_ratio = FloatPrompt.ask(
        "åˆæˆæ•°æ®æ¯”ä¾‹ / Synthetic ratio (0.0-1.0)",
        default=0.0
    )
    human_ratio = 1.0 - synthetic_ratio

    teacher_models = []
    if synthetic_ratio > 0:
        models_input = Prompt.ask(
            "æ•™å¸ˆæ¨¡å‹ / Teacher models (é€—å·åˆ†éš” / comma-separated)",
            default=""
        )
        if models_input:
            teacher_models = [m.strip() for m in models_input.split(",")]

    # Cost
    console.print("\n[bold]æˆæœ¬ä¼°ç®— / Cost Estimation[/bold]")
    has_cost = Confirm.ask("æ˜¯å¦æ·»åŠ æˆæœ¬ä¿¡æ¯? / Add cost info?", default=False)
    cost_total = None
    cost_confidence = "low"
    if has_cost:
        cost_total = FloatPrompt.ask("é¢„ä¼°æ€»æˆæœ¬ (USD) / Estimated total cost", default=0)
        cost_confidence = Prompt.ask(
            "ç½®ä¿¡åº¦ / Confidence",
            choices=["low", "medium", "high"],
            default="low"
        )

    # Reproducibility
    console.print("\n[bold]å¯å¤ç°æ€§ / Reproducibility[/bold]")
    repro_score = IntPrompt.ask("å¯å¤ç°æ€§è¯„åˆ† (1-10) / Score", default=5)

    available_input = Prompt.ask(
        "å·²æä¾›çš„ä¿¡æ¯ / Available info (é€—å·åˆ†éš” / comma-separated)",
        default="description"
    )
    available = [a.strip() for a in available_input.split(",") if a.strip()]

    missing_input = Prompt.ask(
        "ç¼ºå¤±çš„ä¿¡æ¯ / Missing info (é€—å·åˆ†éš” / comma-separated)",
        default="exact_prompts,filtering_criteria"
    )
    missing = [m.strip() for m in missing_input.split(",") if m.strip()]

    # Metadata
    console.print("\n[bold]å…ƒæ•°æ® / Metadata[/bold]")
    num_examples = IntPrompt.ask("æ ·æœ¬æ•°é‡ / Number of examples", default=0)
    languages_input = Prompt.ask("è¯­è¨€ / Languages (é€—å·åˆ†éš”)", default="en")
    languages = [l.strip() for l in languages_input.split(",") if l.strip()]
    license_str = Prompt.ask("è®¸å¯è¯ / License", default="unknown")

    tags_input = Prompt.ask("æ ‡ç­¾ / Tags (é€—å·åˆ†éš”)", default="")
    tags = [t.strip() for t in tags_input.split(",") if t.strip()]

    # Build YAML content
    yaml_content = f"""# Recipe for {name}
# Generated by DataRecipe

name: {name}
version: "{version}"

source:
  type: {source_type}
  id: {source_id or name}

generation:
  synthetic_ratio: {synthetic_ratio}
  human_ratio: {human_ratio}
  teacher_models: {teacher_models}
  methods:"""

    if teacher_models:
        for model in teacher_models:
            yaml_content += f"""
    - type: distillation
      teacher_model: {model}"""

    if human_ratio > 0:
        yaml_content += """
    - type: human_annotation"""

    yaml_content += f"""

cost:
  estimated_total_usd: {cost_total if cost_total else 'null'}
  confidence: {cost_confidence}

reproducibility:
  score: {repro_score}
  available: {available}
  missing: {missing}

metadata:
  num_examples: {num_examples if num_examples else 'null'}
  languages: {languages}
  license: {license_str}
  tags: {tags}
"""

    # Output
    if output:
        output_path = Path(output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(yaml_content, encoding="utf-8")
        console.print(f"\n[green]âœ“ é…æ–¹å·²ä¿å­˜åˆ° / Recipe saved to:[/green] {output}")
    else:
        # Default output path
        safe_name = name.replace("/", "-").replace(" ", "-").lower()
        output_path = Path(f"recipes/{safe_name}.yaml")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(yaml_content, encoding="utf-8")
        console.print(f"\n[green]âœ“ é…æ–¹å·²ä¿å­˜åˆ° / Recipe saved to:[/green] {output_path}")

    # Show preview
    console.print("\n[bold]é¢„è§ˆ / Preview:[/bold]")
    console.print(yaml_content)


@main.command()
@click.argument("dataset_id")
@click.option("--model", "-m", default="gpt-4o", help="LLM model for cost estimation")
@click.option("--examples", "-n", type=int, help="Target number of examples")
@click.option("--json", "as_json", is_flag=True, help="Output as JSON")
def cost(dataset_id: str, model: str, examples: int, as_json: bool):
    """Calculate production cost estimate for a dataset.

    DATASET_ID is the identifier of the dataset to analyze.
    """
    from datarecipe.cost_calculator import CostCalculator

    analyzer = DatasetAnalyzer()
    calculator = CostCalculator()

    with console.status(f"[cyan]Analyzing {dataset_id}...[/cyan]"):
        try:
            recipe = analyzer.analyze(dataset_id)
        except Exception as e:
            console.print(f"[red]Error:[/red] {e}")
            sys.exit(1)

    target_size = examples or recipe.num_examples or 10000

    with console.status("[cyan]Calculating costs...[/cyan]"):
        cost_breakdown = calculator.estimate_from_recipe(recipe, target_size, model)

    if as_json:
        import json
        console.print(json.dumps(cost_breakdown.to_dict(), indent=2))
    else:
        console.print(f"\n[bold cyan]Cost Estimate for {dataset_id}[/bold cyan]")
        console.print(f"Target size: {target_size:,} examples")
        console.print(f"Model: {model}")
        console.print("")

        table = Table(title="Cost Breakdown")
        table.add_column("Category", style="cyan")
        table.add_column("Low", justify="right")
        table.add_column("Expected", justify="right", style="green")
        table.add_column("High", justify="right")

        table.add_row(
            "API Calls",
            f"${cost_breakdown.api_cost.low:,.0f}",
            f"${cost_breakdown.api_cost.expected:,.0f}",
            f"${cost_breakdown.api_cost.high:,.0f}",
        )
        table.add_row(
            "Human Annotation",
            f"${cost_breakdown.human_annotation_cost.low:,.0f}",
            f"${cost_breakdown.human_annotation_cost.expected:,.0f}",
            f"${cost_breakdown.human_annotation_cost.high:,.0f}",
        )
        table.add_row(
            "Compute",
            f"${cost_breakdown.compute_cost.low:,.0f}",
            f"${cost_breakdown.compute_cost.expected:,.0f}",
            f"${cost_breakdown.compute_cost.high:,.0f}",
        )
        table.add_row(
            "[bold]Total[/bold]",
            f"[bold]${cost_breakdown.total.low:,.0f}[/bold]",
            f"[bold green]${cost_breakdown.total.expected:,.0f}[/bold green]",
            f"[bold]${cost_breakdown.total.high:,.0f}[/bold]",
        )

        console.print(table)

        if cost_breakdown.assumptions:
            console.print("\n[bold]Assumptions:[/bold]")
            for assumption in cost_breakdown.assumptions:
                console.print(f"  - {assumption}")


@main.command()
@click.argument("dataset_id")
@click.option("--sample-size", "-n", type=int, default=1000, help="Number of examples to sample")
@click.option("--text-field", "-f", default="text", help="Field containing text to analyze")
@click.option("--detect-ai", is_flag=True, help="Run AI content detection")
@click.option("--json", "as_json", is_flag=True, help="Output as JSON")
def quality(dataset_id: str, sample_size: int, text_field: str, detect_ai: bool, as_json: bool):
    """Analyze quality metrics for a dataset.

    DATASET_ID is the identifier of the dataset to analyze.
    """
    from datarecipe.quality_metrics import QualityAnalyzer

    quality_analyzer = QualityAnalyzer()

    with console.status(f"[cyan]Analyzing quality of {dataset_id}...[/cyan]"):
        try:
            report = quality_analyzer.analyze_from_huggingface(
                dataset_id,
                text_field=text_field,
                sample_size=sample_size,
                detect_ai=detect_ai,
            )
        except Exception as e:
            console.print(f"[red]Error:[/red] {e}")
            sys.exit(1)

    if as_json:
        import json
        console.print(json.dumps(report.to_dict(), indent=2))
    else:
        console.print(f"\n[bold cyan]Quality Report for {dataset_id}[/bold cyan]")
        console.print(f"Sample size: {report.sample_size:,}")
        console.print("")

        # Overall score
        score = report.overall_score
        score_bar = "[" + "#" * int(score / 10) + "-" * (10 - int(score / 10)) + "]"
        console.print(f"[bold]Overall Score: {score:.0f}/100 {score_bar}[/bold]")
        console.print("")

        # Metrics tables
        table = Table(title="Diversity Metrics")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", justify="right")
        table.add_row("Unique Token Ratio", f"{report.diversity.unique_token_ratio:.4f}")
        table.add_row("Vocabulary Size", f"{report.diversity.vocabulary_size:,}")
        table.add_row("Semantic Diversity", f"{report.diversity.semantic_diversity:.4f}")
        console.print(table)
        console.print("")

        table = Table(title="Consistency Metrics")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", justify="right")
        table.add_row("Format Consistency", f"{report.consistency.format_consistency:.4f}")
        table.add_row("Structure Score", f"{report.consistency.structure_score:.4f}")
        table.add_row("Field Completeness", f"{report.consistency.field_completeness:.4f}")
        console.print(table)
        console.print("")

        table = Table(title="Complexity Metrics")
        table.add_column("Metric", style="cyan")
        table.add_column("Value", justify="right")
        table.add_row("Avg Length", f"{report.complexity.avg_length:.0f} chars")
        table.add_row("Avg Tokens", f"{report.complexity.avg_tokens:.0f}")
        table.add_row("Vocabulary Richness", f"{report.complexity.vocabulary_richness:.4f}")
        table.add_row("Readability Score", f"{report.complexity.readability_score:.0f}")
        console.print(table)

        if detect_ai and report.ai_detection:
            console.print("")
            table = Table(title="AI Detection")
            table.add_column("Metric", style="cyan")
            table.add_column("Value", justify="right")
            table.add_row("AI Probability", f"{report.ai_detection.ai_probability:.2%}")
            table.add_row("Confidence", f"{report.ai_detection.confidence:.2%}")
            if report.ai_detection.indicators:
                table.add_row("Indicators", ", ".join(report.ai_detection.indicators[:3]))
            console.print(table)

        if report.recommendations:
            console.print("\n[bold]Recommendations:[/bold]")
            for rec in report.recommendations:
                console.print(f"  - {rec}")

        if report.warnings:
            console.print("\n[yellow]Warnings:[/yellow]")
            for warning in report.warnings:
                console.print(f"  - {warning}")


@main.command()
@click.argument("dataset_ids", nargs=-1)
@click.option("--file", "-f", type=click.Path(exists=True), help="File with dataset IDs")
@click.option("--parallel", "-p", type=int, default=4, help="Number of parallel workers")
@click.option("--output", "-o", type=click.Path(), help="Output directory for results")
@click.option("--format", "fmt", type=click.Choice(["yaml", "json", "markdown"]), default="yaml", help="Output format")
def batch(dataset_ids: tuple, file: str, parallel: int, output: str, fmt: str):
    """Analyze multiple datasets in parallel.

    DATASET_IDS are the identifiers of datasets to analyze.
    Use -f to read dataset IDs from a file.
    """
    from datarecipe.batch_analyzer import BatchAnalyzer

    # Collect dataset IDs
    ids = list(dataset_ids)
    if file:
        batch_analyzer = BatchAnalyzer(max_workers=parallel)
        result = batch_analyzer.analyze_from_file(file)
    elif ids:
        batch_analyzer = BatchAnalyzer(max_workers=parallel)

        def progress_callback(dataset_id, completed, total):
            console.print(f"  [{completed}/{total}] Analyzed: {dataset_id}")

        batch_analyzer.progress_callback = progress_callback
        result = batch_analyzer.analyze_batch(ids)
    else:
        console.print("[red]Error:[/red] Provide dataset IDs or use -f to specify a file")
        sys.exit(1)

    console.print(f"\n[bold cyan]Batch Analysis Complete[/bold cyan]")
    console.print(f"  Total: {len(result.results)}")
    console.print(f"  [green]Successful: {result.successful}[/green]")
    console.print(f"  [red]Failed: {result.failed}[/red]")
    console.print(f"  Duration: {result.total_duration_seconds:.1f}s")

    if result.failed > 0:
        console.print("\n[yellow]Failed datasets:[/yellow]")
        for r in result.get_failed():
            console.print(f"  - {r.dataset_id}: {r.error}")

    if output:
        created = batch_analyzer.export_results(result, output, fmt)
        console.print(f"\n[green]Results exported to {output}[/green]")
        console.print(f"  Created {len(created)} files")


@main.command()
@click.argument("dataset_ids", nargs=-1, required=True)
@click.option("--format", "fmt", type=click.Choice(["table", "markdown"]), default="table", help="Output format")
@click.option("--include-quality", is_flag=True, help="Include quality analysis (slower)")
@click.option("--output", "-o", type=click.Path(), help="Output file")
def compare(dataset_ids: tuple, fmt: str, include_quality: bool, output: str):
    """Compare multiple datasets side by side.

    DATASET_IDS are 2 or more dataset identifiers to compare.
    """
    from datarecipe.comparator import DatasetComparator

    if len(dataset_ids) < 2:
        console.print("[red]Error:[/red] Please provide at least 2 datasets to compare")
        sys.exit(1)

    comparator = DatasetComparator(include_quality=include_quality)

    with console.status(f"[cyan]Comparing {len(dataset_ids)} datasets...[/cyan]"):
        try:
            report = comparator.compare_by_ids(list(dataset_ids))
        except Exception as e:
            console.print(f"[red]Error:[/red] {e}")
            sys.exit(1)

    if fmt == "markdown":
        content = report.to_markdown()
    else:
        content = report.to_table()

    if output:
        output_path = Path(output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(content, encoding="utf-8")
        console.print(f"[green]Report saved to {output}[/green]")
    else:
        print(content)

    # Show recommendations
    if report.recommendations and fmt == "table":
        console.print("\n[bold cyan]Recommendations:[/bold cyan]")
        for rec in report.recommendations:
            console.print(f"  - {rec}")


@main.command()
@click.argument("dataset_id")
@click.option("--output", "-o", type=click.Path(), help="Output file for profile")
@click.option("--region", "-r", default="china", help="Region for cost estimation (china, us, europe, india, sea)")
@click.option("--json", "as_json", is_flag=True, help="Output as JSON")
@click.option("--markdown", "--md", "as_markdown", is_flag=True, help="Output as Markdown")
def profile(dataset_id: str, output: str, region: str, as_json: bool, as_markdown: bool):
    """Generate annotator profile for a dataset.

    Analyzes a dataset and generates requirements for annotation team,
    including skills, experience level, education, and workload estimation.

    DATASET_ID is the identifier of the dataset to analyze.
    """
    from datarecipe.profiler import AnnotatorProfiler, profile_to_markdown

    analyzer = DatasetAnalyzer()
    profiler = AnnotatorProfiler()

    with console.status(f"[cyan]Analyzing {dataset_id}...[/cyan]"):
        try:
            recipe = analyzer.analyze(dataset_id)
        except Exception as e:
            console.print(f"[red]Error:[/red] {e}")
            sys.exit(1)

    with console.status("[cyan]Generating annotator profile...[/cyan]"):
        annotator_profile = profiler.generate_profile(recipe, region=region)

    if as_json:
        import json
        console.print(json.dumps(annotator_profile.to_dict(), indent=2))
    elif as_markdown:
        md_content = profile_to_markdown(annotator_profile, recipe.name)
        print(md_content)
    else:
        # Display as formatted table
        console.print(f"\n[bold cyan]Annotator Profile for {dataset_id}[/bold cyan]")
        console.print("")

        # Skills table
        table = Table(title="Required Skills")
        table.add_column("Skill", style="cyan")
        table.add_column("Level", justify="center")
        table.add_column("Priority", justify="center")

        for skill in annotator_profile.skill_requirements:
            priority = "required" if skill.required else "preferred"
            priority_color = {"required": "red", "preferred": "yellow"}.get(priority, "white")
            table.add_row(
                skill.name,
                skill.level,
                f"[{priority_color}]{priority}[/{priority_color}]"
            )
        console.print(table)
        console.print("")

        # Requirements summary
        console.print("[bold]Requirements:[/bold]")
        console.print(f"  Experience Level: {annotator_profile.experience_level.value}")
        console.print(f"  Education: {annotator_profile.education_level.value}")
        if annotator_profile.domain_knowledge:
            console.print(f"  Domain Expertise: {', '.join(annotator_profile.domain_knowledge)}")
        if annotator_profile.language_requirements:
            console.print(f"  Languages: {', '.join(annotator_profile.language_requirements)}")
        console.print("")

        # Workload estimation
        hourly_rate = (annotator_profile.hourly_rate_range.get("min", 15) + annotator_profile.hourly_rate_range.get("max", 45)) / 2
        estimated_labor_cost = annotator_profile.estimated_person_days * 8 * hourly_rate
        console.print("[bold]Workload Estimation:[/bold]")
        console.print(f"  Team Size: {annotator_profile.team_size} annotators")
        console.print(f"  Person-Days: {annotator_profile.estimated_person_days:.0f}")
        console.print(f"  Hours per Example: {annotator_profile.estimated_hours_per_example:.2f}")
        console.print(f"  Hourly Rate: ${hourly_rate:.2f}")
        console.print(f"  Estimated Labor Cost: ${estimated_labor_cost:,.0f}")

    # Export if requested
    if output:
        output_path = Path(output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        if output.endswith(".md"):
            md_content = profile_to_markdown(annotator_profile, recipe.name)
            output_path.write_text(md_content, encoding="utf-8")
            console.print(f"\n[green]Profile exported to:[/green] {output}")
        elif output.endswith(".json"):
            import json
            output_path.write_text(json.dumps(annotator_profile.to_dict(), indent=2), encoding="utf-8")
            console.print(f"\n[green]Profile exported to:[/green] {output}")
        else:
            # Default to YAML
            import yaml
            output_path.write_text(yaml.dump(annotator_profile.to_dict(), allow_unicode=True, default_flow_style=False), encoding="utf-8")
            console.print(f"\n[green]Profile exported to:[/green] {output}")


@main.command()
@click.argument("dataset_id")
@click.option("--output", "-o", type=click.Path(), help="Output directory (default: ./projects/<dataset_name>)")
@click.option("--provider", "-p", default="local", help="Deployment provider (local, judgeguild, etc.)")
@click.option("--region", "-r", default="china", help="Region for cost estimation")
@click.option("--submit", is_flag=True, help="Submit to provider after generating config")
def deploy(dataset_id: str, output: str, provider: str, region: str, submit: bool):
    """Generate production deployment for a dataset.

    Creates a complete project structure with annotation guidelines,
    quality rules, acceptance criteria, and timeline for data production.

    DATASET_ID is the identifier of the dataset to analyze.

    If --output is not specified, files are saved to ./projects/<dataset_name>/
    """
    from datarecipe.deployer import ProductionDeployer
    from datarecipe.profiler import AnnotatorProfiler
    from datarecipe.schema import DataRecipe

    # é»˜è®¤è¾“å‡ºç›®å½•
    if not output:
        safe_name = dataset_id.replace("/", "_").replace(" ", "_").lower()
        output = f"./projects/{safe_name}"
        console.print(f"[dim]Output directory: {output}[/dim]")

    analyzer = DatasetAnalyzer()
    deployer = ProductionDeployer()
    profiler = AnnotatorProfiler()

    with console.status(f"[cyan]Analyzing {dataset_id}...[/cyan]"):
        try:
            recipe = analyzer.analyze(dataset_id)
        except Exception as e:
            console.print(f"[red]Error:[/red] {e}")
            sys.exit(1)

    with console.status("[cyan]Generating annotator profile...[/cyan]"):
        profile = profiler.generate_profile(recipe, region=region)

    # Convert Recipe to DataRecipe
    data_recipe = DataRecipe(
        name=recipe.name,
        version=recipe.version,
        source_type=recipe.source_type,
        source_id=recipe.source_id,
        num_examples=recipe.num_examples,
        languages=recipe.languages or [],
        license=recipe.license,
        description=recipe.description,
        generation_type=recipe.generation_type,
        synthetic_ratio=recipe.synthetic_ratio,
        human_ratio=recipe.human_ratio,
        generation_methods=recipe.generation_methods or [],
        teacher_models=recipe.teacher_models or [],
        tags=recipe.tags or [],
    )

    with console.status("[cyan]Generating production config...[/cyan]"):
        config = deployer.generate_config(data_recipe, profile=profile)

    # Deploy to provider
    submit_action = submit or provider == "local"
    status_msg = (
        f"[cyan]Deploying to {provider}...[/cyan]"
        if submit_action
        else f"[cyan]Generating deployment package for {provider} (no auto submission)...[/cyan]"
    )
    with console.status(status_msg):
        result = deployer.deploy(
            data_recipe,
            output,
            provider=provider,
            config=config,
            profile=profile,
            submit=submit,
        )

    if result.success:
        console.print(f"\n[bold green]Deployment successful![/bold green]")
        if result.project_handle:
            console.print(f"  Project ID: {result.project_handle.project_id}")
        console.print(f"  Output: {output}")
        if result.details:
            console.print(f"  Details: {result.details}")

        # Show created files
        output_path = Path(output)
        if output_path.exists():
            files = list(output_path.rglob("*"))
            files = [f for f in files if f.is_file()]
            console.print(f"\n[bold]Created files ({len(files)}):[/bold]")
            for f in files[:10]:
                console.print(f"  - {f.relative_to(output_path)}")
            if len(files) > 10:
                console.print(f"  ... and {len(files) - 10} more")

        console.print(f"\n[bold cyan]Next steps:[/bold cyan]")
        console.print(f"  1. cd {output}")
        console.print(f"  2. Review annotation_guide.md")
        console.print(f"  3. Review quality_rules.yaml")
        console.print(f"  4. See README.md for detailed instructions")
        if provider != "local" and not submit:
            console.print(
                "  5. ä½¿ç”¨ provider å¹³å°æ‰‹åŠ¨æäº¤é¡¹ç›® (æœ¬æ¬¡æœªè‡ªåŠ¨æäº¤ï¼Œéœ€ç¡®è®¤é…ç½®åå†æ‰§è¡Œ)"
            )
    else:
        console.print(f"\n[red]Deployment failed:[/red] {result.error}")
        sys.exit(1)


@main.group()
def providers():
    """Manage deployment providers."""
    pass


@providers.command("list")
def providers_list():
    """List available deployment providers."""
    from datarecipe.providers import list_providers

    provider_list = list_providers()

    table = Table(title="Available Providers")
    table.add_column("Name", style="cyan")
    table.add_column("Description")

    for p in provider_list:
        table.add_row(p["name"], p["description"])

    console.print(table)

    console.print("\n[dim]Install additional providers with: pip install datarecipe-<provider>[/dim]")


@main.command()
@click.argument("dataset_id")
@click.option("--output", "-o", type=click.Path(), required=True, help="Output directory for project")
@click.option("--target-size", "-n", type=int, help="Target number of examples")
@click.option("--format", "fmt", type=click.Choice(["huggingface", "jsonl", "parquet"]), default="huggingface", help="Output format")
def workflow(dataset_id: str, output: str, target_size: int, fmt: str):
    """Generate a production workflow for reproducing a dataset.

    Creates a complete project structure with scripts, configuration,
    and documentation for producing a dataset similar to DATASET_ID.
    """
    from datarecipe.workflow import WorkflowGenerator

    analyzer = DatasetAnalyzer()
    generator = WorkflowGenerator()

    with console.status(f"[cyan]Analyzing {dataset_id}...[/cyan]"):
        try:
            recipe = analyzer.analyze(dataset_id)
        except Exception as e:
            console.print(f"[red]Error:[/red] {e}")
            sys.exit(1)

    with console.status("[cyan]Generating workflow...[/cyan]"):
        wf = generator.generate(recipe, target_size, fmt)

    # Export project
    created_files = wf.export_project(output)

    console.print(f"\n[bold green]Workflow generated successfully![/bold green]")
    console.print(f"  Project: {output}")
    console.print(f"  Target size: {wf.target_size:,} examples")
    console.print(f"  Estimated cost: ${wf.estimated_total_cost:,.0f}")
    console.print(f"  Steps: {len(wf.steps)}")

    console.print(f"\n[bold]Created files ({len(created_files)}):[/bold]")
    for f in created_files[:10]:
        console.print(f"  - {f}")
    if len(created_files) > 10:
        console.print(f"  ... and {len(created_files) - 10} more")

    console.print(f"\n[bold cyan]Next steps:[/bold cyan]")
    console.print(f"  1. cd {output}")
    console.print(f"  2. pip install -r requirements.txt")
    console.print(f"  3. cp .env.example .env && edit .env")
    console.print(f"  4. See README.md for detailed instructions")


# =============================================================================
# New Commands: Pattern Extraction & Generation
# =============================================================================

@main.command("extract-rubrics")
@click.argument("dataset_id")
@click.option("--output", "-o", default=None, help="Output file path (JSON)")
@click.option("--sample-size", "-n", default=1000, help="Number of samples to analyze")
def extract_rubrics(dataset_id: str, output: str, sample_size: int):
    """Extract rubrics/evaluation patterns from a dataset."""
    from datarecipe.extractors import RubricsAnalyzer

    console.print(f"\n[bold]Extracting rubrics patterns from {dataset_id}...[/bold]\n")

    try:
        # Load dataset
        from datasets import load_dataset
        ds = load_dataset(dataset_id, split="train", streaming=True)

        # Collect rubrics
        rubrics = []
        for i, item in enumerate(ds):
            if i >= sample_size:
                break
            # Try common rubrics field names
            for field in ["rubrics", "rubric", "criteria", "evaluation"]:
                if field in item:
                    value = item[field]
                    if isinstance(value, list):
                        rubrics.extend(value)
                    elif isinstance(value, str):
                        rubrics.append(value)

        if not rubrics:
            console.print("[yellow]No rubrics found in dataset.[/yellow]")
            console.print("Tried fields: rubrics, rubric, criteria, evaluation")
            return

        # Analyze
        analyzer = RubricsAnalyzer()
        result = analyzer.analyze(rubrics, task_count=sample_size)

        # Display summary
        console.print(Panel(result.summary(), title="Rubrics Analysis"))
        console.print("\n[bold]Top Structured Templates:[/bold]")
        for entry in result.structured_templates[:5]:
            console.print(
                f"â€¢ [{entry.get('category', 'general')}] {entry.get('action') or ''} â†’ {entry.get('target') or ''}" +
                (f" | æ¡ä»¶: {entry.get('condition')}" if entry.get('condition') else "")
            )

        # Export if requested
        if output:
            import json
            base = output
            if output.endswith(".json"):
                data_path = output
                yaml_path = output.replace(".json", "_templates.yaml")
                md_path = output.replace(".json", "_templates.md")
            else:
                data_path = f"{output}.json"
                yaml_path = f"{output}_templates.yaml"
                md_path = f"{output}_templates.md"

            with open(data_path, "w", encoding="utf-8") as f:
                json.dump(analyzer.to_dict(result), f, indent=2, ensure_ascii=False)
            with open(yaml_path, "w", encoding="utf-8") as f:
                f.write(analyzer.to_yaml_templates(result))
            with open(md_path, "w", encoding="utf-8") as f:
                f.write(analyzer.to_markdown_templates(result))

            console.print(f"\n[green]Exported analysis to {data_path}[/green]")
            console.print(f"[green]Exported templates to {yaml_path} & {md_path}[/green]")

    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")


@main.command("extract-prompts")
@click.argument("dataset_id")
@click.option("--output", "-o", default=None, help="Output file path (JSON)")
@click.option("--sample-size", "-n", default=500, help="Number of samples to analyze")
def extract_prompts(dataset_id: str, output: str, sample_size: int):
    """Extract system prompt templates from a dataset."""
    from datarecipe.extractors import PromptExtractor

    console.print(f"\n[bold]Extracting prompt templates from {dataset_id}...[/bold]\n")

    try:
        from datasets import load_dataset
        ds = load_dataset(dataset_id, split="train", streaming=True)

        # Collect messages with progress
        messages = []
        console.print(f"[dim]Collecting messages from {sample_size} samples...[/dim]")
        for i, item in enumerate(ds):
            if i >= sample_size:
                break
            if i > 0 and i % 100 == 0:
                console.print(f"[dim]  Processed {i}/{sample_size} samples ({len(messages)} messages)[/dim]")
            # Try common message field names
            for field in ["messages", "conversation", "turns"]:
                if field in item and isinstance(item[field], list):
                    messages.extend(item[field])

        if not messages:
            console.print("[yellow]No messages found in dataset.[/yellow]")
            return

        console.print(f"[dim]Collected {len(messages)} messages, deduplicating...[/dim]")

        # Extract
        extractor = PromptExtractor()
        library = extractor.extract(messages)
        console.print(f"[green]âœ“ Deduplication complete[/green]")

        # Display summary
        console.print(Panel(library.summary(), title="Prompt Library"))

        # Export if output specified
        if output:
            import json
            data = extractor.to_dict(library)
            with open(output, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            console.print(f"\n[green]Exported to {output}[/green]")

    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")


@main.command("detect-strategy")
@click.argument("dataset_id")
@click.option("--output", "-o", default=None, help="Output file path (JSON)")
@click.option("--sample-size", "-n", default=100, help="Number of samples to analyze")
def detect_strategy(dataset_id: str, output: str, sample_size: int):
    """Detect context construction strategy in a dataset."""
    from datarecipe.analyzers import ContextStrategyDetector

    console.print(f"\n[bold]Detecting context strategy in {dataset_id}...[/bold]\n")

    try:
        from datasets import load_dataset
        ds = load_dataset(dataset_id, split="train", streaming=True)

        # Collect contexts
        contexts = []
        for i, item in enumerate(ds):
            if i >= sample_size:
                break
            # Try common context field names
            for field in ["context", "input", "text", "content", "document"]:
                if field in item and isinstance(item[field], str):
                    contexts.append(item[field])
                    break
            # Also check messages
            if "messages" in item and isinstance(item["messages"], list):
                for msg in item["messages"]:
                    if isinstance(msg, dict) and msg.get("role") == "user":
                        contexts.append(msg.get("content", ""))

        if not contexts:
            console.print("[yellow]No contexts found in dataset.[/yellow]")
            return

        # Detect
        detector = ContextStrategyDetector()
        result = detector.analyze(contexts)

        # Display summary
        console.print(Panel(result.summary(), title="Context Strategy"))

        # Export if output specified
        if output:
            import json
            data = detector.to_dict(result)
            with open(output, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            console.print(f"\n[green]Exported to {output}[/green]")

    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")


@main.command("allocate")
@click.option("--size", "-s", default=10000, help="Target dataset size")
@click.option("--region", "-r", default="china", help="Region for cost calculation")
@click.option("--output", "-o", default=None, help="Output file path (JSON/Markdown)")
@click.option("--format", "fmt", type=click.Choice(["table", "json", "markdown"]), default="table")
def allocate(size: int, region: str, output: str, fmt: str):
    """Generate human-machine task allocation."""
    from datarecipe.generators import HumanMachineSplitter, TaskType

    console.print(f"\n[bold]Generating human-machine allocation...[/bold]")
    console.print(f"Target size: {size:,} | Region: {region}\n")

    splitter = HumanMachineSplitter(region=region)
    result = splitter.analyze(
        dataset_size=size,
        task_types=[
            TaskType.CONTEXT_CREATION,
            TaskType.TASK_DESIGN,
            TaskType.RUBRICS_WRITING,
            TaskType.DATA_GENERATION,
            TaskType.QUALITY_REVIEW,
        ]
    )

    if fmt == "table":
        console.print(Panel(result.summary(), title="Allocation Summary"))
        console.print("\n" + result.to_markdown_table())
    elif fmt == "markdown":
        console.print(result.summary())
        console.print("\n" + result.to_markdown_table())
    else:
        import json
        data = splitter.to_dict(result)
        console.print(json.dumps(data, indent=2))

    if output:
        import json
        with open(output, "w", encoding="utf-8") as f:
            if output.endswith(".json"):
                json.dump(splitter.to_dict(result), f, indent=2, ensure_ascii=False)
            else:
                f.write(result.summary() + "\n\n" + result.to_markdown_table())
        console.print(f"\n[green]Exported to {output}[/green]")


@main.command("enhanced-guide")
@click.argument("dataset_id")
@click.option("--output", "-o", default=None, help="Output file path")
@click.option("--size", "-s", default=10000, help="Target dataset size")
@click.option("--region", "-r", default="china", help="Region for cost calculation")
def enhanced_guide(dataset_id: str, output: str, size: int, region: str):
    """Generate enhanced production guide with patterns and allocation."""
    from datarecipe.generators import EnhancedGuideGenerator, HumanMachineSplitter, TaskType
    from datarecipe.extractors import RubricsAnalyzer, PromptExtractor
    from datarecipe.analyzers import ContextStrategyDetector

    console.print(f"\n[bold]Generating enhanced guide for {dataset_id}...[/bold]\n")

    try:
        # Try to load and analyze the dataset
        rubrics_result = None
        prompt_library = None
        strategy_result = None

        try:
            from datasets import load_dataset
            ds = load_dataset(dataset_id, split="train", streaming=True)

            rubrics = []
            messages = []
            contexts = []

            for i, item in enumerate(ds):
                if i >= 500:
                    break
                # Collect rubrics
                for field in ["rubrics", "rubric", "criteria"]:
                    if field in item:
                        value = item[field]
                        if isinstance(value, list):
                            rubrics.extend(value)
                        elif isinstance(value, str):
                            rubrics.append(value)
                # Collect messages
                if "messages" in item and isinstance(item["messages"], list):
                    messages.extend(item["messages"])
                # Collect contexts
                for field in ["context", "input", "text"]:
                    if field in item and isinstance(item[field], str):
                        contexts.append(item[field])
                        break

            if rubrics:
                analyzer = RubricsAnalyzer()
                rubrics_result = analyzer.analyze(rubrics)
                console.print(f"[green]âœ“ Analyzed {len(rubrics)} rubrics[/green]")

            if messages:
                console.print(f"[dim]  Deduplicating {len(messages)} messages...[/dim]")
                extractor = PromptExtractor()
                prompt_library = extractor.extract(messages)
                console.print(f"[green]âœ“ Extracted {prompt_library.unique_count} unique prompts[/green]")

            if contexts:
                detector = ContextStrategyDetector()
                strategy_result = detector.analyze(contexts[:100])
                console.print(f"[green]âœ“ Detected strategy: {strategy_result.primary_strategy.value}[/green]")

        except Exception as e:
            console.print(f"[yellow]Could not analyze dataset: {e}[/yellow]")

        # Generate allocation
        splitter = HumanMachineSplitter(region=region)
        allocation = splitter.analyze(
            dataset_size=size,
            task_types=[
                TaskType.CONTEXT_CREATION,
                TaskType.TASK_DESIGN,
                TaskType.RUBRICS_WRITING,
                TaskType.QUALITY_REVIEW,
            ]
        )

        # Generate guide
        generator = EnhancedGuideGenerator()
        guide = generator.generate(
            dataset_name=dataset_id,
            target_size=size,
            rubrics_analysis=rubrics_result,
            prompt_library=prompt_library,
            context_strategy=strategy_result,
            allocation=allocation,
            region=region,
        )

        # Output
        markdown = generator.to_markdown(guide)

        if output:
            with open(output, "w", encoding="utf-8") as f:
                f.write(markdown)
            console.print(f"\n[green]Guide saved to {output}[/green]")
        else:
            console.print("\n" + markdown)

    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        import traceback
        traceback.print_exc()


@main.command("generate")
@click.option("--type", "gen_type", type=click.Choice(["rubrics", "prompts", "contexts"]), default="rubrics")
@click.option("--count", "-n", default=10, help="Number of items to generate")
@click.option("--context", "-c", default="the topic", help="Context/topic for generation")
@click.option("--output", "-o", default=None, help="Output file path (JSONL)")
def generate(gen_type: str, count: int, context: str, output: str):
    """Generate data based on patterns."""
    from datarecipe.generators import PatternGenerator

    console.print(f"\n[bold]Generating {count} {gen_type}...[/bold]\n")

    generator = PatternGenerator()

    if gen_type == "rubrics":
        result = generator.generate_rubrics(context=context, count=count)
    elif gen_type == "prompts":
        result = generator.generate_prompts(domain=context, count=count)
    elif gen_type == "contexts":
        result = generator.generate_contexts(count=count)
    else:
        console.print(f"[red]Unknown type: {gen_type}[/red]")
        return

    # Display
    console.print(Panel(result.summary(), title="Generation Result"))
    console.print("")

    for item in result.items[:5]:
        console.print(f"[cyan]{item.data_type}[/cyan]: {item.content[:100]}...")
        console.print("")

    if len(result.items) > 5:
        console.print(f"... and {len(result.items) - 5} more")

    # Export
    if output:
        generator.export_jsonl(result, output)
        console.print(f"\n[green]Exported to {output}[/green]")


@main.command("deep-analyze")
@click.argument("dataset_id")
@click.option("--output-dir", "-o", default="./analysis_output", help="Output directory")
@click.option("--sample-size", "-n", default=500, help="Number of samples to analyze")
@click.option("--size", "-s", default=None, type=int, help="Target dataset size (for cost estimation)")
@click.option("--region", "-r", default="china", help="Region for cost calculation")
@click.option("--split", default=None, help="Dataset split (auto-detect if not specified)")
@click.option("--use-llm", is_flag=True, default=False, help="Use LLM for intelligent analysis of unknown dataset types")
@click.option("--llm-provider", default="anthropic", type=click.Choice(["anthropic", "openai"]), help="LLM provider for intelligent analysis")
@click.option("--enhance-mode", default="auto", type=click.Choice(["auto", "interactive", "api"]), help="LLM enhancement mode: auto (detect), interactive (Claude Code/App), api (standalone)")
@click.option("--force", "-f", is_flag=True, help="Force re-analysis, ignore cache")
@click.option("--no-cache", is_flag=True, help="Don't use or update cache")
def deep_analyze(dataset_id: str, output_dir: str, sample_size: int, size: int, region: str, split: str, use_llm: bool, llm_provider: str, enhance_mode: str, force: bool, no_cache: bool):
    """
    Run comprehensive deep analysis on a dataset.

    Generates both JSON data files and a human-readable Markdown report.

    Example:
        datarecipe deep-analyze tencent/CL-bench -o ./output
    """
    import os
    from datarecipe.cache import AnalysisCache
    from datarecipe.core.deep_analyzer import DeepAnalyzerCore

    # Create output directory with dataset subdirectory
    safe_dataset_name = dataset_id.replace("/", "_").replace("\\", "_").replace(":", "_")
    dataset_output_dir = os.path.join(output_dir, safe_dataset_name)

    # Check cache first (unless --force or --no-cache)
    cache = AnalysisCache() if not no_cache else None
    if cache and not force:
        cached = cache.get(dataset_id, check_freshness=True)
        if cached:
            console.print(f"\n[bold cyan]{'='*60}[/bold cyan]")
            console.print(f"[bold cyan]  DataRecipe æ·±åº¦é€†å‘åˆ†æ (ç¼“å­˜å‘½ä¸­)[/bold cyan]")
            console.print(f"[bold cyan]{'='*60}[/bold cyan]\n")
            console.print(f"æ•°æ®é›†: [bold]{dataset_id}[/bold]")
            console.print(f"[green]âœ“ ä½¿ç”¨ç¼“å­˜ç»“æœ (åˆ›å»ºäº {cached.created_at[:10]})[/green]")
            console.print(f"  ç±»å‹: {cached.dataset_type or 'unknown'}")
            console.print(f"  æ ·æœ¬: {cached.sample_count}")

            if cached.output_dir != dataset_output_dir:
                os.makedirs(dataset_output_dir, exist_ok=True)
                cache.copy_to_output(dataset_id, dataset_output_dir)
                console.print(f"  è¾“å‡º: {dataset_output_dir}")
            else:
                console.print(f"  è¾“å‡º: {cached.output_dir}")

            console.print(f"\n[dim]ä½¿ç”¨ --force å¼ºåˆ¶é‡æ–°åˆ†æ[/dim]")
            return

    # Display header
    console.print(f"\n[bold cyan]{'='*60}[/bold cyan]")
    console.print(f"[bold cyan]  DataRecipe æ·±åº¦é€†å‘åˆ†æ[/bold cyan]")
    console.print(f"[bold cyan]{'='*60}[/bold cyan]\n")
    console.print(f"æ•°æ®é›†: [bold]{dataset_id}[/bold]")
    console.print(f"è¾“å‡ºç›®å½•: [bold]{dataset_output_dir}[/bold]\n")

    try:
        # Use shared DeepAnalyzerCore
        analyzer = DeepAnalyzerCore(
            output_dir=output_dir,
            region=region,
            use_llm=use_llm,
            llm_provider=llm_provider,
            enhance_mode=enhance_mode,
        )

        console.print("[dim]ğŸ“¥ åŠ è½½æ•°æ®é›†...[/dim]")
        result = analyzer.analyze(
            dataset_id=dataset_id,
            sample_size=sample_size,
            split=split,
            target_size=size,
        )

        if not result.success:
            console.print(f"[red]é”™è¯¯: {result.error}[/red]")
            return

        console.print(f"[green]âœ“ åŠ è½½å®Œæˆ: {result.sample_count} æ ·æœ¬[/green]")

        # Display analysis results
        if result.dataset_type == "preference":
            console.print(f"\n[dim]ğŸ”„ åˆ†æåå¥½æ¨¡å¼...[/dim]")
            console.print(f"[green]âœ“ åå¥½åˆ†æ: {result.sample_count} å¯¹[/green]")
        elif result.dataset_type == "swe_bench":
            console.print(f"\n[dim]ğŸ”§ åˆ†æ SWE ä»»åŠ¡...[/dim]")
            console.print(f"[green]âœ“ SWE åˆ†æå®Œæˆ[/green]")
        elif result.rubric_patterns > 0:
            console.print(f"\n[dim]ğŸ“Š åˆ†æè¯„åˆ†æ ‡å‡†...[/dim]")
            console.print(f"[green]âœ“ è¯„åˆ†æ ‡å‡†: {result.rubric_patterns} ç§æ¨¡å¼[/green]")

        if result.prompt_templates > 0:
            console.print(f"[dim]ğŸ“ æå– Prompt æ¨¡æ¿...[/dim]")
            console.print(f"[green]âœ“ Promptæ¨¡æ¿: {result.prompt_templates} ä¸ª[/green]")

        console.print(f"[dim]âš™ï¸ è®¡ç®—äººæœºåˆ†é…...[/dim]")
        console.print(f"[green]âœ“ äººæœºåˆ†é…: äººå·¥ {result.human_percentage:.0f}%, æœºå™¨ {100-result.human_percentage:.0f}%[/green]")

        console.print(f"\n[dim]ğŸ“„ ç”Ÿæˆç»¼åˆæŠ¥å‘Š...[/dim]")
        console.print(f"[green]âœ“ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜[/green]")
        console.print(f"[dim]ğŸ“‹ ç”Ÿæˆå¤åˆ»æŒ‡å—...[/dim]")
        console.print(f"[green]âœ“ å¤åˆ»æŒ‡å—å·²ä¿å­˜[/green]")
        console.print(f"[dim]ğŸ“¦ ç”Ÿæˆæ ‡å‡†åŒ–æ‘˜è¦...[/dim]")
        console.print(f"[green]âœ“ æ ‡å‡†åŒ–æ‘˜è¦å·²ä¿å­˜ (Radar å…¼å®¹)[/green]")
        console.print(f"[dim]ğŸ“š æ›´æ–°çŸ¥è¯†åº“...[/dim]")
        console.print(f"[green]âœ“ çŸ¥è¯†åº“å·²æ›´æ–°[/green]")
        console.print(f"[dim]ğŸ’¾ æ›´æ–°ç¼“å­˜...[/dim]")
        console.print(f"[green]âœ“ ç¼“å­˜å·²æ›´æ–°[/green]")

        # Display summary
        console.print(f"\n[bold cyan]{'='*60}[/bold cyan]")
        console.print(f"[bold cyan]  åˆ†æå®Œæˆ[/bold cyan]")
        console.print(f"[bold cyan]{'='*60}[/bold cyan]\n")

        console.print(f"[bold]ç”Ÿæˆçš„æ–‡ä»¶:[/bold]")
        for fname in result.files_generated:
            fpath = os.path.join(result.output_dir, fname)
            if os.path.exists(fpath):
                fsize = os.path.getsize(fpath)
                if fsize > 1024:
                    size_str = f"{fsize / 1024:.1f}KB"
                else:
                    size_str = f"{fsize}B"
                icon = "ğŸ“Š" if fname.endswith(".json") else "ğŸ“„" if fname.endswith(".md") else "ğŸ“‘"
                console.print(f"  {icon} {fname} ({size_str})")

        report_path = os.path.join(result.output_dir, "ANALYSIS_REPORT.md")
        guide_path = os.path.join(result.output_dir, "REPRODUCTION_GUIDE.md")
        console.print(f"\n[bold]æ ¸å¿ƒäº§å‡º:[/bold]")
        console.print(f"  ğŸ“„ åˆ†ææŠ¥å‘Š: [cyan]{report_path}[/cyan]")
        console.print(f"  ğŸ“‹ å¤åˆ»æŒ‡å—: [cyan]{guide_path}[/cyan]")

        # Display warnings if any
        if hasattr(result, "warnings") and result.warnings:
            console.print(f"\n[yellow]âš  éƒ¨åˆ†æ­¥éª¤è·³è¿‡ ({len(result.warnings)} é¡¹):[/yellow]")
            for w in result.warnings:
                console.print(f"  [dim]Â· {w}[/dim]")

    except Exception as e:
        console.print(f"[red]é”™è¯¯: {e}[/red]")
        import traceback
        traceback.print_exc()

def _generate_analysis_report(
    dataset_id: str,
    sample_count: int,
    actual_size: int,
    rubrics_result,
    prompt_library,
    strategy_result,
    allocation,
    region: str,
) -> str:
    """Generate a comprehensive Markdown analysis report."""
    from datetime import datetime

    lines = []
    lines.append(f"# ğŸ”¬ {dataset_id} æ·±åº¦é€†å‘åˆ†ææŠ¥å‘Š")
    lines.append("")
    lines.append(f"> **åˆ†ææ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    lines.append(f"> **æ•°æ®é›†**: {dataset_id}")
    lines.append(f"> **åˆ†ææ ·æœ¬**: {sample_count} æ¡")
    lines.append(f"> **ç›®æ ‡è§„æ¨¡**: {actual_size:,} æ¡")
    lines.append("")
    lines.append("---")
    lines.append("")

    # Executive Summary
    lines.append("## ğŸ“Š æ‰§è¡Œæ‘˜è¦")
    lines.append("")
    lines.append("| ç»´åº¦ | å‘ç° |")
    lines.append("|------|------|")

    if rubrics_result:
        lines.append(f"| **è¯„åˆ†æ ‡å‡†** | {rubrics_result.total_rubrics:,} æ¡ï¼Œ{rubrics_result.unique_patterns:,} ç§ç‹¬ç‰¹æ¨¡å¼ |")
    if prompt_library:
        lines.append(f"| **Promptæ¨¡æ¿** | {prompt_library.unique_count} ä¸ªå»é‡åçš„ç³»ç»Ÿæç¤ºæ¨¡æ¿ |")
    if strategy_result:
        lines.append(f"| **æ•°æ®æ¥æº** | æ··åˆç­–ç•¥ï¼ˆåˆæˆ {strategy_result.synthetic_score*100:.0f}% + æ”¹ç¼– {strategy_result.modified_score*100:.0f}% + ä¸“ä¸š {strategy_result.niche_score*100:.0f}%ï¼‰ |")

    lines.append(f"| **å¤ç°æˆæœ¬** | çº¦ ${allocation.total_cost:,.0f}ï¼ˆäººå·¥ ${allocation.total_human_cost:,.0f} + API ${allocation.total_machine_cost:,.0f}ï¼‰ |")
    lines.append(f"| **äººæœºåˆ†é…** | äººå·¥ {allocation.human_work_percentage:.0f}%ï¼Œæœºå™¨ {allocation.machine_work_percentage:.0f}% |")
    lines.append("")
    lines.append("---")
    lines.append("")

    # Rubrics Analysis
    if rubrics_result:
        lines.append("## 1ï¸âƒ£ è¯„åˆ†æ ‡å‡†ï¼ˆRubricsï¼‰æ¨¡å¼åˆ†æ")
        lines.append("")
        lines.append("### 1.1 æ€»ä½“ç»Ÿè®¡")
        lines.append("")
        lines.append(f"- **æ€»æ•°**: {rubrics_result.total_rubrics:,} æ¡è¯„åˆ†æ ‡å‡†")
        lines.append(f"- **ç‹¬ç‰¹æ¨¡å¼**: {rubrics_result.unique_patterns:,} ç§")
        lines.append(f"- **å¹³å‡æ¯ä»»åŠ¡**: {rubrics_result.avg_rubrics_per_task:.1f} æ¡")
        lines.append("")

        lines.append("### 1.2 é«˜é¢‘åŠ¨è¯åˆ†å¸ƒ")
        lines.append("")
        lines.append("| æ’å | åŠ¨è¯ | å‡ºç°æ¬¡æ•° | å æ¯” |")
        lines.append("|------|------|----------|------|")

        sorted_verbs = sorted(rubrics_result.verb_distribution.items(), key=lambda x: -x[1])[:10]
        for i, (verb, count) in enumerate(sorted_verbs, 1):
            pct = count / rubrics_result.total_rubrics * 100
            lines.append(f"| {i} | **{verb}** | {count:,} | {pct:.1f}% |")
        lines.append("")

        lines.append("### 1.3 è¯„åˆ†ç±»åˆ«åˆ†å¸ƒ")
        lines.append("")
        sorted_cats = sorted(rubrics_result.category_distribution.items(), key=lambda x: -x[1])
        for cat, count in sorted_cats[:5]:
            pct = count / rubrics_result.total_rubrics * 100
            bar_len = int(pct / 2.5)
            bar = "â–ˆ" * bar_len
            lines.append(f"- **{cat}**: {bar} {pct:.1f}% ({count:,})")
        lines.append("")

        if rubrics_result.structured_templates:
            lines.append("### 1.4 æ¨¡æ¿åŒ–ç»“æ„ï¼ˆTop 5ï¼‰")
            lines.append("")
            lines.append("| ç±»åˆ« | åŠ¨ä½œ | ç›®æ ‡ | æ¡ä»¶ | é¢‘æ¬¡ |")
            lines.append("|------|------|------|------|------|")
            for entry in rubrics_result.structured_templates[:5]:
                action = entry.get("action") or "N/A"
                target = entry.get("target") or "N/A"
                condition = entry.get("condition") or "â€”"
                freq = entry.get("frequency", 0)
                lines.append(
                    f"| {entry.get('category', 'general')} | {action} | {target} | {condition} | {freq} |"
                )
            lines.append("")
        lines.append("---")
        lines.append("")

    # Prompt Templates
    if prompt_library:
        lines.append("## 2ï¸âƒ£ ç³»ç»Ÿæç¤ºï¼ˆSystem Promptï¼‰æ¨¡æ¿åˆ†æ")
        lines.append("")
        lines.append("### 2.1 æå–ç»Ÿè®¡")
        lines.append("")
        lines.append(f"- **åŸå§‹æ•°é‡**: {prompt_library.total_extracted} æ¡")
        lines.append(f"- **å»é‡å**: {prompt_library.unique_count} ä¸ªç‹¬ç‰¹æ¨¡æ¿")
        lines.append(f"- **å»é‡ç‡**: {prompt_library.deduplication_ratio:.1%}")
        lines.append(f"- **å¹³å‡é•¿åº¦**: {prompt_library.avg_length:,.0f} å­—ç¬¦")
        lines.append("")

        lines.append("### 2.2 æ¨¡æ¿åˆ†ç±»")
        lines.append("")
        lines.append("| ç±»åˆ« | æ•°é‡ | è¯´æ˜ |")
        lines.append("|------|------|------|")
        category_desc = {
            "system": "ç³»ç»Ÿè§’è‰²è®¾å®š",
            "constraint": "çº¦æŸæ¡ä»¶",
            "task": "ä»»åŠ¡è¯´æ˜",
            "format": "æ ¼å¼è¦æ±‚",
            "example": "ç¤ºä¾‹è¯´æ˜",
            "other": "å…¶ä»–ç±»å‹",
        }
        for cat, count in sorted(prompt_library.category_counts.items(), key=lambda x: -x[1]):
            desc = category_desc.get(cat, cat)
            lines.append(f"| **{cat}** | {count} | {desc} |")
        lines.append("")

        if prompt_library.domain_counts:
            lines.append("### 2.3 é¢†åŸŸåˆ†å¸ƒ")
            lines.append("")
            for domain, count in sorted(prompt_library.domain_counts.items(), key=lambda x: -x[1])[:5]:
                pct = count / prompt_library.unique_count * 100
                lines.append(f"- **{domain}**: {count} ({pct:.0f}%)")
            lines.append("")
        lines.append("---")
        lines.append("")

    # Context Strategy
    if strategy_result:
        lines.append("## 3ï¸âƒ£ ä¸Šä¸‹æ–‡æ„é€ ç­–ç•¥åˆ†æ")
        lines.append("")
        lines.append("### 3.1 ç­–ç•¥è¯†åˆ«")
        lines.append("")
        lines.append(f"**ä¸»è¦ç­–ç•¥**: {strategy_result.primary_strategy.value}")
        lines.append(f"**ç½®ä¿¡åº¦**: {strategy_result.confidence:.1%}")
        lines.append("")

        lines.append("### 3.2 ç­–ç•¥å¾—åˆ†")
        lines.append("")
        lines.append("| ç­–ç•¥ | å¾—åˆ† | è¯´æ˜ |")
        lines.append("|------|------|------|")
        lines.append(f"| ğŸ”§ åˆæˆç”Ÿæˆ | {strategy_result.synthetic_score*100:.1f}% | ä½¿ç”¨ AI æ¨¡å‹ç”Ÿæˆè™šæ„å†…å®¹ |")
        lines.append(f"| ğŸ“ æ”¹ç¼–ä¿®æ”¹ | {strategy_result.modified_score*100:.1f}% | åŸºäºçœŸå®æ¥æºæ”¹ç¼– |")
        lines.append(f"| ğŸ”¬ ä¸“ä¸šé¢†åŸŸ | {strategy_result.niche_score*100:.1f}% | ä¸“ä¸š/å°ä¼—é¢†åŸŸå†…å®¹ |")
        lines.append("")

        lines.append("### 3.3 æ£€æµ‹åˆ°çš„æŒ‡æ ‡")
        lines.append("")
        if strategy_result.synthetic_indicators:
            lines.append("**ğŸ”§ åˆæˆç”Ÿæˆ**")
            for ind in strategy_result.synthetic_indicators[:5]:
                lines.append(f"- `{ind}`")
            lines.append("")
        if strategy_result.modified_indicators:
            lines.append("**ğŸ“ æ”¹ç¼–ä¿®æ”¹**")
            for ind in strategy_result.modified_indicators[:5]:
                lines.append(f"- `{ind}`")
            lines.append("")
        if strategy_result.niche_indicators:
            lines.append("**ğŸ”¬ ä¸“ä¸šé¢†åŸŸ**")
            for ind in strategy_result.niche_indicators[:5]:
                lines.append(f"- `{ind}`")
            lines.append("")

        if strategy_result.recommendations:
            lines.append("### 3.4 å¤ç°å»ºè®®")
            lines.append("")
            for rec in strategy_result.recommendations:
                lines.append(f"- {rec}")
            lines.append("")
        lines.append("---")
        lines.append("")

    # Human-Machine Allocation
    lines.append("## 4ï¸âƒ£ äººæœºä»»åŠ¡åˆ†é…")
    lines.append("")
    lines.append("### 4.1 åˆ†é…æ€»è§ˆ")
    lines.append("")
    human_pct = allocation.human_work_percentage
    machine_pct = allocation.machine_work_percentage
    human_bar = "â–ˆ" * int(human_pct / 2.5)
    machine_bar = "â–ˆ" * int(machine_pct / 2.5)
    lines.append(f"- äººå·¥å·¥ä½œ: {human_bar} **{human_pct:.0f}%**")
    lines.append(f"- æœºå™¨å·¥ä½œ: {machine_bar} **{machine_pct:.0f}%**")
    lines.append("")

    lines.append("### 4.2 ä»»åŠ¡æ˜ç»†")
    lines.append("")
    lines.append("| ä»»åŠ¡ | åˆ†é…æ–¹å¼ | äººå·¥å æ¯” | äººå·¥æ—¶é•¿ | äººå·¥æˆæœ¬ | æœºå™¨æˆæœ¬ |")
    lines.append("|------|----------|----------|----------|----------|----------|")

    decision_zh = {
        "human_only": "çº¯äººå·¥",
        "machine_only": "çº¯æœºå™¨",
        "human_primary": "äººå·¥ä¸ºä¸»",
        "machine_primary": "æœºå™¨ä¸ºä¸»",
        "balanced": "å‡è¡¡",
    }
    for task in allocation.tasks:
        dec = decision_zh.get(task.decision.value, task.decision.value)
        lines.append(f"| **{task.task_name}** | {dec} | {task.human_percentage:.0f}% | {task.human_hours:.1f}h | ${task.human_cost:,.0f} | ${task.machine_cost:.1f} |")
    lines.append("")

    lines.append("### 4.3 æˆæœ¬ä¼°ç®—")
    lines.append("")
    lines.append("| é¡¹ç›® | é‡‘é¢ |")
    lines.append("|------|------|")
    lines.append(f"| äººå·¥æˆæœ¬ | ${allocation.total_human_cost:,.0f} |")
    lines.append(f"| API/æœºå™¨æˆæœ¬ | ${allocation.total_machine_cost:,.0f} |")
    lines.append(f"| **æ€»è®¡** | **${allocation.total_cost:,.0f}** |")
    lines.append(f"| é¢„ä¼°èŠ‚çœ | ${allocation.estimated_savings_vs_all_human:,.0f}ï¼ˆç›¸æ¯”å…¨äººå·¥ï¼‰ |")
    lines.append("")
    lines.append("---")
    lines.append("")

    # Recommendations
    lines.append("## 5ï¸âƒ£ å¤ç°å»ºè®®")
    lines.append("")
    lines.append("### 5.1 å›¢é˜Ÿé…ç½®")
    lines.append("")
    lines.append("| è§’è‰² | äººæ•° | èŒè´£ |")
    lines.append("|------|------|------|")
    lines.append("| é¢†åŸŸä¸“å®¶ | 4 | åˆ›å»ºå’Œå®¡æ ¸ä¸Šä¸‹æ–‡å†…å®¹ |")
    lines.append("| ä»»åŠ¡è®¾è®¡å¸ˆ | 2 | è®¾è®¡è¯„ä¼°ä»»åŠ¡å’Œé—®é¢˜ |")
    lines.append("| æ ‡æ³¨å‘˜ | 4 | ç¼–å†™è¯„åˆ†æ ‡å‡†å’Œæ ‡æ³¨ |")
    lines.append("| QAå®¡æ ¸å‘˜ | 2 | è´¨é‡ä¿è¯å’ŒéªŒè¯ |")
    lines.append("| é¡¹ç›®ç»ç† | 1 | åè°ƒå›¢é˜Ÿå’Œè¿›åº¦è·Ÿè¸ª |")
    lines.append("")

    lines.append("### 5.2 è´¨é‡æ£€æŸ¥ç‚¹")
    lines.append("")
    lines.append("- [ ] ä¸Šä¸‹æ–‡å†…å®¹æ˜¯åŸåˆ›çš„ï¼ˆä¸åœ¨è®­ç»ƒæ•°æ®ä¸­ï¼‰")
    lines.append("- [ ] ä»»åŠ¡éœ€è¦ä¸Šä¸‹æ–‡æ‰èƒ½å›ç­”")
    lines.append("- [ ] è¯„åˆ†æ ‡å‡†éµå¾ªå·²å‘ç°çš„æ¨¡å¼")
    lines.append("- [ ] é€šè¿‡äº¤å‰éªŒè¯å®¡æ ¸")
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("> æŠ¥å‘Šç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ")

    return "\n".join(lines)


def _generate_reproduction_guide(
    dataset_id: str,
    schema_info: dict,
    category_set: set,
    sub_category_set: set,
    system_prompts_by_domain: dict,
    rubrics_examples: list,
    sample_items: list,
    rubrics_result,
    prompt_library,
    allocation,
    # RLHF preference dataset support
    is_preference_dataset: bool = False,
    preference_pairs: list = None,
    preference_topics: dict = None,
    preference_patterns: dict = None,
    # SWE-bench dataset support
    is_swe_dataset: bool = False,
    swe_stats: dict = None,
    # LLM analysis for unknown types
    llm_analysis=None,
) -> str:
    """Generate a practical reproduction guide for recreating a similar dataset."""
    import json
    from datarecipe.analyzers.llm_dataset_analyzer import generate_llm_guide_section

    preference_pairs = preference_pairs or []
    preference_topics = preference_topics or {}
    preference_patterns = preference_patterns or {}
    swe_stats = swe_stats or {}

    lines = []
    lines.append(f"# ğŸ“‹ {dataset_id} å¤åˆ»æŒ‡å—")
    lines.append("")

    if is_swe_dataset:
        lines.append("> **è¿™æ˜¯ä¸€ä¸ªè½¯ä»¶å·¥ç¨‹è¯„æµ‹æ•°æ®é›† (SWE-bench é£æ ¼)ã€‚æœ¬æŒ‡å—æä¾›ä»»åŠ¡æ„å»ºè§„èŒƒï¼Œå¸®åŠ©ä½ æ„å»ºç±»ä¼¼çš„ä»£ç ä¿®å¤/åŠŸèƒ½å®ç°è¯„æµ‹é›†ã€‚**")
    elif is_preference_dataset:
        lines.append("> **è¿™æ˜¯ä¸€ä¸ª RLHF åå¥½æ•°æ®é›†ã€‚æœ¬æŒ‡å—æä¾›åå¥½æ ‡æ³¨è§„èŒƒï¼Œå¸®åŠ©ä½ æ„å»ºç±»ä¼¼çš„äººç±»åå¥½æ•°æ®ã€‚**")
    elif llm_analysis and llm_analysis.dataset_type != "unknown":
        lines.append(f"> **æ•°æ®é›†ç±»å‹: {llm_analysis.dataset_type}ã€‚{llm_analysis.purpose}**")
    else:
        lines.append("> **æœ¬æŒ‡å—æä¾›å¯ç›´æ¥æ“ä½œçš„æ¨¡æ¿å’Œè§„èŒƒï¼Œå¸®åŠ©ä½ ä»é›¶å¼€å§‹æ„å»ºç±»ä¼¼é£æ ¼çš„æ•°æ®é›†ã€‚**")
    lines.append("")
    lines.append("---")
    lines.append("")

    # ==================== LLM Analysis Section (if available) ====================
    if llm_analysis and llm_analysis.dataset_type != "unknown":
        lines.append(generate_llm_guide_section(llm_analysis))
        lines.append("")

    # ==================== Section 1: Data Schema ====================
    lines.append("## 1ï¸âƒ£ æ•°æ®ç»“æ„è§„èŒƒ (Schema)")
    lines.append("")
    lines.append("### 1.1 å­—æ®µå®šä¹‰")
    lines.append("")
    lines.append("| å­—æ®µå | ç±»å‹ | å­ç±»å‹ | è¯´æ˜ |")
    lines.append("|--------|------|--------|------|")

    field_descriptions = {
        "messages": "å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å« system/user/assistant è§’è‰²",
        "rubrics": "è¯„åˆ†æ ‡å‡†åˆ—è¡¨ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹å›ç­”è´¨é‡",
        "metadata": "å…ƒæ•°æ®å­—å…¸ï¼ŒåŒ…å«ä»»åŠ¡åˆ†ç±»ç­‰ä¿¡æ¯",
        "input": "ç”¨æˆ·è¾“å…¥/ä¸Šä¸‹æ–‡",
        "output": "æœŸæœ›çš„æ¨¡å‹è¾“å‡º",
        "instruction": "ä»»åŠ¡æŒ‡ä»¤",
        "context": "ä¸Šä¸‹æ–‡ä¿¡æ¯",
        "question": "é—®é¢˜å†…å®¹",
        "answer": "å‚è€ƒç­”æ¡ˆ",
    }

    for field, info in schema_info.items():
        ftype = info["type"]
        nested = info.get("nested_type", "")
        if isinstance(nested, list):
            nested = f"keys: {', '.join(nested[:3])}"
        desc = field_descriptions.get(field, "â€”")
        lines.append(f"| `{field}` | `{ftype}` | `{nested or 'â€”'}` | {desc} |")
    lines.append("")

    # JSON Schema
    lines.append("### 1.2 JSON Schema")
    lines.append("")
    lines.append("```json")
    lines.append("{")
    for i, (field, info) in enumerate(schema_info.items()):
        comma = "," if i < len(schema_info) - 1 else ""
        if info["type"] == "list":
            if info.get("nested_type") == "dict":
                lines.append(f'  "{field}": [{{...}}]{comma}')
            elif info.get("nested_type") == "str":
                lines.append(f'  "{field}": ["..."]' + comma)
            else:
                lines.append(f'  "{field}": []{comma}')
        elif info["type"] == "dict":
            lines.append(f'  "{field}": {{...}}{comma}')
        elif info["type"] == "str":
            lines.append(f'  "{field}": "..."{comma}')
        else:
            lines.append(f'  "{field}": ...{comma}')
    lines.append("}")
    lines.append("```")
    lines.append("")

    # ==================== Section 2: Category System ====================
    lines.append("## 2ï¸âƒ£ ä»»åŠ¡åˆ†ç±»ä½“ç³»")
    lines.append("")

    if category_set:
        lines.append("### 2.1 ä¸»åˆ†ç±» (context_category)")
        lines.append("")
        for cat in sorted(category_set):
            lines.append(f"- `{cat}`")
        lines.append("")

    if sub_category_set:
        lines.append("### 2.2 å­åˆ†ç±» (sub_category)")
        lines.append("")
        for sub in sorted(sub_category_set):
            lines.append(f"- `{sub}`")
        lines.append("")

    if not category_set and not sub_category_set and not is_preference_dataset:
        lines.append("*æœªæ£€æµ‹åˆ°åˆ†ç±»ä½“ç³»*")
        lines.append("")

    # For preference datasets, show topic distribution
    if is_preference_dataset and preference_topics:
        lines.append("### è¯é¢˜åˆ†å¸ƒ")
        lines.append("")
        lines.append("| è¯é¢˜ | æ•°é‡ | å æ¯” |")
        lines.append("|------|------|------|")
        total = sum(preference_topics.values())
        for topic, count in sorted(preference_topics.items(), key=lambda x: -x[1]):
            pct = count / total * 100 if total > 0 else 0
            lines.append(f"| {topic} | {count} | {pct:.1f}% |")
        lines.append("")

    lines.append("---")
    lines.append("")

    # ==================== Section 2.5: Preference Dataset Guide (if applicable) ====================
    if is_preference_dataset:
        lines.append("## ğŸ”„ åå¥½æ•°æ®é›†ä¸“ç”¨æŒ‡å—")
        lines.append("")
        lines.append("è¿™æ˜¯ä¸€ä¸ª RLHF (Reinforcement Learning from Human Feedback) åå¥½æ•°æ®é›†ã€‚")
        lines.append("æ¯æ¡æ•°æ®åŒ…å«ä¸€å¯¹å›å¤ï¼š`chosen`ï¼ˆè¢«é€‰ä¸­çš„æ›´å¥½å›å¤ï¼‰å’Œ `rejected`ï¼ˆè¢«æ‹’ç»çš„è¾ƒå·®å›å¤ï¼‰ã€‚")
        lines.append("")

        # Preference patterns analysis
        lines.append("### åå¥½æ¨¡å¼åˆ†æ")
        lines.append("")
        if preference_patterns:
            total_patterns = sum(preference_patterns.values())
            if total_patterns > 0:
                lines.append("| æ¨¡å¼ | æ•°é‡ | å æ¯” | è¯´æ˜ |")
                lines.append("|------|------|------|------|")
                pattern_desc = {
                    "chosen_longer": "è¢«é€‰ä¸­å›å¤æ›´é•¿",
                    "rejected_longer": "è¢«æ‹’ç»å›å¤æ›´é•¿",
                    "same_length": "é•¿åº¦ç›¸è¿‘",
                    "chosen_safer": "è¢«é€‰ä¸­å›å¤æ›´å®‰å…¨ï¼ˆrejected å«æ‹’ç»è¯ï¼‰",
                }
                for pattern, count in sorted(preference_patterns.items(), key=lambda x: -x[1]):
                    if count > 0:
                        pct = count / total_patterns * 100
                        desc = pattern_desc.get(pattern, pattern)
                        lines.append(f"| {pattern} | {count} | {pct:.1f}% | {desc} |")
                lines.append("")

        # Preference labeling guidelines
        lines.append("### åå¥½æ ‡æ³¨è§„èŒƒ")
        lines.append("")
        lines.append("æ ‡æ³¨å‘˜éœ€è¦æ¯”è¾ƒä¸¤ä¸ªå›å¤ï¼Œé€‰æ‹©ã€Œæ›´å¥½ã€çš„é‚£ä¸ªã€‚åˆ¤æ–­æ ‡å‡†ï¼š")
        lines.append("")
        lines.append("| ç»´åº¦ | é€‰æ‹© chosen çš„æ¡ä»¶ |")
        lines.append("|------|-------------------|")
        lines.append("| **æœ‰ç”¨æ€§** | æ›´ç›´æ¥åœ°å›ç­”äº†é—®é¢˜ï¼Œæä¾›äº†æ›´å®ç”¨çš„ä¿¡æ¯ |")
        lines.append("| **å‡†ç¡®æ€§** | ä¿¡æ¯æ›´å‡†ç¡®ï¼Œæ²¡æœ‰äº‹å®é”™è¯¯ |")
        lines.append("| **å®‰å…¨æ€§** | ä¸åŒ…å«æœ‰å®³ã€è¿æ³•ã€æ­§è§†æ€§å†…å®¹ |")
        lines.append("| **å®Œæ•´æ€§** | è¦†ç›–äº†é—®é¢˜çš„å„ä¸ªæ–¹é¢ï¼Œä¸é—æ¼å…³é”®ä¿¡æ¯ |")
        lines.append("| **æ¸…æ™°åº¦** | è¡¨è¾¾æ›´æ¸…æ™°ï¼Œç»“æ„æ›´å¥½ï¼Œæ˜“äºç†è§£ |")
        lines.append("| **è¯šå®æ€§** | æ‰¿è®¤ä¸ç¡®å®šæ€§ï¼Œä¸ç¼–é€ ä¿¡æ¯ |")
        lines.append("")

        # Preference pair examples
        if preference_pairs:
            lines.append("### åå¥½å¯¹ç¤ºä¾‹")
            lines.append("")
            for i, pair in enumerate(preference_pairs[:3], 1):
                lines.append(f"**ç¤ºä¾‹ {i}** (è¯é¢˜: `{pair.get('topic', 'unknown')}`)")
                lines.append("")
                lines.append("**Human:**")
                lines.append("```")
                lines.append(pair.get("human_query", "")[:300] or "(æ— )")
                lines.append("```")
                lines.append("")
                lines.append("**Chosen (è¢«é€‰ä¸­):**")
                lines.append("```")
                chosen_resp = pair.get("chosen_response", "")[:400]
                lines.append(chosen_resp if chosen_resp else "(æ— )")
                lines.append("```")
                lines.append("")
                lines.append("**Rejected (è¢«æ‹’ç»):**")
                lines.append("```")
                rejected_resp = pair.get("rejected_response", "")[:400]
                lines.append(rejected_resp if rejected_resp else "(æ— )")
                lines.append("```")
                lines.append("")

        # SOP for preference dataset
        lines.append("### åå¥½æ•°æ®ç”Ÿäº§ SOP")
        lines.append("")
        lines.append("```")
        lines.append("Phase 1: å‡†å¤‡é˜¶æ®µ")
        lines.append("â”œâ”€ æ­¥éª¤ 1.1: æ”¶é›†ç”¨æˆ·é—®é¢˜ï¼ˆå¤šæ ·åŒ–è¯é¢˜ï¼‰")
        lines.append("â”œâ”€ æ­¥éª¤ 1.2: ä½¿ç”¨ LLM ç”Ÿæˆå¤šä¸ªå€™é€‰å›å¤ï¼ˆé€šå¸¸ 2-4 ä¸ªï¼‰")
        lines.append("â””â”€ æ­¥éª¤ 1.3: å‡†å¤‡æ ‡æ³¨ç•Œé¢å’Œæ ‡æ³¨æŒ‡å—")
        lines.append("")
        lines.append("Phase 2: æ ‡æ³¨é˜¶æ®µ")
        lines.append("â”œâ”€ æ­¥éª¤ 2.1: æ ‡æ³¨å‘˜é˜…è¯»é—®é¢˜å’Œæ‰€æœ‰å€™é€‰å›å¤")
        lines.append("â”œâ”€ æ­¥éª¤ 2.2: æ ¹æ®æ ‡æ³¨è§„èŒƒé€‰æ‹©æœ€ä½³å›å¤ (chosen)")
        lines.append("â”œâ”€ æ­¥éª¤ 2.3: é€‰æ‹©æœ€å·®å›å¤ (rejected)")
        lines.append("â””â”€ æ­¥éª¤ 2.4: è®°å½•é€‰æ‹©ç†ç”±ï¼ˆå¯é€‰ï¼Œç”¨äºè´¨æ£€ï¼‰")
        lines.append("")
        lines.append("Phase 3: è´¨é‡æ§åˆ¶")
        lines.append("â”œâ”€ æ­¥éª¤ 3.1: åŒäººæ ‡æ³¨ï¼Œè®¡ç®—ä¸€è‡´æ€§ (Cohen's Kappa)")
        lines.append("â”œâ”€ æ­¥éª¤ 3.2: ä¸ä¸€è‡´æ ·æœ¬ç”±ç¬¬ä¸‰äººä»²è£")
        lines.append("â””â”€ æ­¥éª¤ 3.3: æŠ½æ ·å®¡æ ¸ï¼Œç¡®ä¿æ ‡æ³¨è´¨é‡")
        lines.append("```")
        lines.append("")
        lines.append("---")
        lines.append("")

    # ==================== Section 2.6: SWE-bench Dataset Guide (if applicable) ====================
    if is_swe_dataset and swe_stats:
        lines.append("## ğŸ”§ è½¯ä»¶å·¥ç¨‹è¯„æµ‹æ•°æ®é›†ä¸“ç”¨æŒ‡å—")
        lines.append("")
        lines.append("è¿™æ˜¯ä¸€ä¸ª SWE-bench é£æ ¼çš„è½¯ä»¶å·¥ç¨‹è¯„æµ‹æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼° AI ä»£ç ä¿®å¤å’ŒåŠŸèƒ½å®ç°èƒ½åŠ›ã€‚")
        lines.append("")

        # Language distribution
        if swe_stats.get("languages"):
            lines.append("### ç¼–ç¨‹è¯­è¨€åˆ†å¸ƒ")
            lines.append("")
            lines.append("| è¯­è¨€ | æ•°é‡ | å æ¯” |")
            lines.append("|------|------|------|")
            total = sum(swe_stats["languages"].values())
            for lang, count in sorted(swe_stats["languages"].items(), key=lambda x: -x[1]):
                pct = count / total * 100 if total > 0 else 0
                lines.append(f"| {lang} | {count} | {pct:.1f}% |")
            lines.append("")

        # Repository distribution
        if swe_stats.get("repos"):
            lines.append("### ä»“åº“åˆ†å¸ƒ (Top 10)")
            lines.append("")
            lines.append("| ä»“åº“ | ä»»åŠ¡æ•° |")
            lines.append("|------|--------|")
            for repo, count in sorted(swe_stats["repos"].items(), key=lambda x: -x[1])[:10]:
                lines.append(f"| `{repo}` | {count} |")
            lines.append("")

        # Issue types
        if swe_stats.get("issue_types"):
            lines.append("### é—®é¢˜ç±»å‹åˆ†å¸ƒ")
            lines.append("")
            lines.append("| ç±»å‹ | æ•°é‡ |")
            lines.append("|------|------|")
            for itype, count in sorted(swe_stats["issue_types"].items(), key=lambda x: -x[1]):
                lines.append(f"| `{itype}` | {count} |")
            lines.append("")

        # Issue categories
        if swe_stats.get("issue_categories"):
            lines.append("### æ‰€éœ€çŸ¥è¯†é¢†åŸŸ")
            lines.append("")
            lines.append("| é¢†åŸŸ | æ•°é‡ |")
            lines.append("|------|------|")
            for cat, count in sorted(swe_stats["issue_categories"].items(), key=lambda x: -x[1]):
                lines.append(f"| `{cat}` | {count} |")
            lines.append("")

        # Patch complexity
        if swe_stats.get("patch_lines"):
            avg_lines = sum(swe_stats["patch_lines"]) / len(swe_stats["patch_lines"])
            max_lines = max(swe_stats["patch_lines"])
            min_lines = min(swe_stats["patch_lines"])
            lines.append("### ä»£ç ä¿®æ”¹å¤æ‚åº¦")
            lines.append("")
            lines.append(f"- **å¹³å‡ä¿®æ”¹è¡Œæ•°**: {avg_lines:.1f} è¡Œ")
            lines.append(f"- **æœ€å¤§ä¿®æ”¹**: {max_lines} è¡Œ")
            lines.append(f"- **æœ€å°ä¿®æ”¹**: {min_lines} è¡Œ")
            lines.append("")

        # Problem statement examples
        if swe_stats.get("examples"):
            lines.append("### é—®é¢˜æè¿°ç¤ºä¾‹")
            lines.append("")
            for i, ex in enumerate(swe_stats["examples"][:2], 1):
                lines.append(f"**ç¤ºä¾‹ {i}** (`{ex.get('repo', 'unknown')}` - {ex.get('language', 'unknown')})")
                lines.append("")
                lines.append("**Problem Statement:**")
                lines.append("```")
                lines.append(ex.get("problem_statement", "")[:600])
                lines.append("```")
                lines.append("")
                if ex.get("requirements"):
                    lines.append("**Requirements:**")
                    lines.append("```")
                    lines.append(ex.get("requirements", "")[:400])
                    lines.append("```")
                    lines.append("")

        # SOP for SWE-bench dataset
        lines.append("### SWE-bench æ•°æ®ç”Ÿäº§ SOP")
        lines.append("")
        lines.append("```")
        lines.append("Phase 1: ä»“åº“ç­›é€‰")
        lines.append("â”œâ”€ æ­¥éª¤ 1.1: é€‰æ‹©æ´»è·ƒçš„å¼€æºä»“åº“ï¼ˆGPL ç­‰å¼º copyleft è®¸å¯ä¼˜å…ˆï¼‰")
        lines.append("â”œâ”€ æ­¥éª¤ 1.2: ç¡®ä¿æœ‰å®Œå–„çš„æµ‹è¯•å¥—ä»¶")
        lines.append("â””â”€ æ­¥éª¤ 1.3: ç­›é€‰æœ‰æ¸…æ™° issue/PR å†å²çš„ä»“åº“")
        lines.append("")
        lines.append("Phase 2: ä»»åŠ¡æŒ–æ˜")
        lines.append("â”œâ”€ æ­¥éª¤ 2.1: ä»å·²åˆå¹¶çš„ PR ä¸­æå– bug fix / feature")
        lines.append("â”œâ”€ æ­¥éª¤ 2.2: æå– base_commit (ä¿®å¤å‰) å’Œ patch (ä¿®å¤å†…å®¹)")
        lines.append("â”œâ”€ æ­¥éª¤ 2.3: è¯†åˆ« fail-to-pass æµ‹è¯•ï¼ˆä¿®å¤ååº”é€šè¿‡ï¼‰")
        lines.append("â””â”€ æ­¥éª¤ 2.4: è¯†åˆ« pass-to-pass æµ‹è¯•ï¼ˆç¡®ä¿æ— å›å½’ï¼‰")
        lines.append("")
        lines.append("Phase 3: ä»»åŠ¡å¢å¼º")
        lines.append("â”œâ”€ æ­¥éª¤ 3.1: æ’°å†™ problem_statementï¼ˆé—®é¢˜æè¿°ï¼‰")
        lines.append("â”œâ”€ æ­¥éª¤ 3.2: æ’°å†™ requirementsï¼ˆåŠŸèƒ½éœ€æ±‚ï¼‰")
        lines.append("â”œâ”€ æ­¥éª¤ 3.3: æ ‡æ³¨ interfaceï¼ˆæ¶‰åŠçš„ API/å‡½æ•°ï¼‰")
        lines.append("â””â”€ æ­¥éª¤ 3.4: åˆ†ç±» issue_categoriesï¼ˆæ‰€éœ€çŸ¥è¯†é¢†åŸŸï¼‰")
        lines.append("")
        lines.append("Phase 4: è´¨é‡éªŒè¯")
        lines.append("â”œâ”€ æ­¥éª¤ 4.1: éªŒè¯ patch èƒ½é€šè¿‡æ‰€æœ‰æµ‹è¯•")
        lines.append("â”œâ”€ æ­¥éª¤ 4.2: ç¡®ä¿ problem_statement ä¸æ³„éœ²è§£å†³æ–¹æ¡ˆ")
        lines.append("â””â”€ æ­¥éª¤ 4.3: éªŒè¯ä»»åŠ¡å¯ç”±äººç±»å·¥ç¨‹å¸ˆç‹¬ç«‹å®Œæˆ")
        lines.append("```")
        lines.append("")

        # Quality criteria
        lines.append("### æ•°æ®è´¨é‡æ ‡å‡†")
        lines.append("")
        lines.append("| ç»´åº¦ | è¦æ±‚ |")
        lines.append("|------|------|")
        lines.append("| **é—®é¢˜æè¿°** | æ¸…æ™°æè¿° bug ç°è±¡æˆ–åŠŸèƒ½éœ€æ±‚ï¼Œä¸æ³„éœ²è§£å†³æ–¹æ¡ˆ |")
        lines.append("| **æµ‹è¯•è¦†ç›–** | è‡³å°‘æœ‰ 1 ä¸ª fail-to-pass æµ‹è¯•éªŒè¯ä¿®å¤æ­£ç¡®æ€§ |")
        lines.append("| **æ— å›å½’** | pass-to-pass æµ‹è¯•ç¡®ä¿ä¸å¼•å…¥æ–° bug |")
        lines.append("| **å¯å¤ç°** | æä¾›å®Œæ•´çš„ç¯å¢ƒè®¾ç½®å‘½ä»¤ |")
        lines.append("| **åˆç†å¤æ‚åº¦** | ä¿®æ”¹è¡Œæ•°é€‚ä¸­ï¼Œä¸è¿‡äºç®€å•ä¹Ÿä¸è¿‡äºå¤æ‚ |")
        lines.append("")
        lines.append("---")
        lines.append("")

    # ==================== Section 3: System Prompt Templates ====================
    lines.append("## 3ï¸âƒ£ System Prompt æ¨¡æ¿åº“")
    lines.append("")
    lines.append("> ä»¥ä¸‹æ˜¯ä»æ•°æ®é›†ä¸­æå–çš„çœŸå® System Prompt ç¤ºä¾‹ï¼Œå¯ç›´æ¥å¤ç”¨æˆ–æ”¹ç¼–ã€‚")
    lines.append("")

    if system_prompts_by_domain:
        for domain, prompts in list(system_prompts_by_domain.items())[:5]:
            lines.append(f"### 3.{list(system_prompts_by_domain.keys()).index(domain)+1} {domain}")
            lines.append("")
            for i, p in enumerate(prompts[:2], 1):
                content = p["content"]
                # Truncate if too long
                if len(content) > 1500:
                    content = content[:1500] + "\n\n... (æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è§ prompt_templates.json)"
                lines.append(f"**ç¤ºä¾‹ {i}:**")
                lines.append("")
                lines.append("```")
                lines.append(content)
                lines.append("```")
                lines.append("")
    else:
        lines.append("*æœªæå–åˆ° System Prompt*")
        lines.append("")

    lines.append("---")
    lines.append("")

    # ==================== Section 4: Rubric Writing Guide ====================
    lines.append("## 4ï¸âƒ£ è¯„åˆ†æ ‡å‡† (Rubric) ç¼–å†™è§„èŒƒ")
    lines.append("")

    if rubrics_result:
        lines.append("### 4.1 å¥å¼æ¨¡å¼")
        lines.append("")
        lines.append("ä»æ•°æ®é›†ä¸­å‘ç°çš„é«˜é¢‘å¥å¼æ¨¡å¼ï¼š")
        lines.append("")

        # Top verbs
        sorted_verbs = sorted(rubrics_result.verb_distribution.items(), key=lambda x: -x[1])[:8]
        lines.append("| æ ¸å¿ƒåŠ¨è¯ | é¢‘æ¬¡ | ç¤ºä¾‹å¥å¼ |")
        lines.append("|----------|------|----------|")
        verb_examples = {
            "include": "The response should include [å…·ä½“å†…å®¹]",
            "state": "The response should state [å…·ä½“äº‹å®]",
            "explain": "The response should explain [æ¦‚å¿µ/åŸå› ]",
            "provide": "The response should provide [ä¿¡æ¯/ç¤ºä¾‹]",
            "not": "The response should not [ç¦æ­¢è¡Œä¸º]",
            "identify": "The response should identify [ç›®æ ‡å¯¹è±¡]",
            "use": "The response should use [æŒ‡å®šæ–¹æ³•/æ ¼å¼]",
            "define": "The response should define [æœ¯è¯­/æ¦‚å¿µ]",
            "list": "The response should list [æ¡ç›®/æ­¥éª¤]",
            "describe": "The response should describe [æè¿°å¯¹è±¡]",
        }
        for verb, count in sorted_verbs:
            example = verb_examples.get(verb, f"... should {verb} ...")
            lines.append(f"| **{verb}** | {count} | `{example}` |")
        lines.append("")

        lines.append("### 4.2 è¯„åˆ†æ ‡å‡†ç»“æ„")
        lines.append("")
        lines.append("æ¨èé‡‡ç”¨ä»¥ä¸‹ç»“æ„ç¼–å†™è¯„åˆ†æ ‡å‡†ï¼š")
        lines.append("")
        lines.append("```")
        lines.append("[ä¸»è¯­] should [åŠ¨ä½œ] [ç›®æ ‡]. [æ¡ä»¶/ä¾‹å¤–]. Fail if [å¤±è´¥æ¡ä»¶].")
        lines.append("```")
        lines.append("")
        lines.append("**ç»“æ„è¯´æ˜ï¼š**")
        lines.append("")
        lines.append("| ç»„æˆéƒ¨åˆ† | è¯´æ˜ | ç¤ºä¾‹ |")
        lines.append("|----------|------|------|")
        lines.append("| ä¸»è¯­ | è¢«è¯„ä¼°å¯¹è±¡ | The response / The model / The answer |")
        lines.append("| åŠ¨ä½œ | æœŸæœ›è¡Œä¸º | should include / should explain / should not |")
        lines.append("| ç›®æ ‡ | å…·ä½“å†…å®¹ | the definition of X / at least 3 examples |")
        lines.append("| æ¡ä»¶ | é€‚ç”¨èŒƒå›´ | For example, ... / When X, ... |")
        lines.append("| å¤±è´¥æ¡ä»¶ | æ‰£åˆ†æ ‡å‡† | Fail if X is missing / Fail if incorrect |")
        lines.append("")

    # Real rubric examples
    if rubrics_examples:
        lines.append("### 4.3 å®Œæ•´ç¤ºä¾‹")
        lines.append("")
        lines.append("> ä»¥ä¸‹æ˜¯ä»æ•°æ®é›†ä¸­æå–çš„çœŸå®è¯„åˆ†æ ‡å‡†ç¤ºä¾‹ï¼š")
        lines.append("")

        for i, ex in enumerate(rubrics_examples[:3], 1):
            meta = ex.get("metadata", {})
            cat = meta.get("context_category", meta.get("category", "unknown"))
            sub = meta.get("sub_category", "")

            lines.append(f"**ç¤ºä¾‹ {i}** (`{cat}` / `{sub}`)")
            lines.append("")
            for j, r in enumerate(ex["rubrics"][:5], 1):
                lines.append(f"{j}. {r}")
            if len(ex["rubrics"]) > 5:
                lines.append(f"   ... (å…± {len(ex['rubrics'])} æ¡)")
            lines.append("")

    lines.append("---")
    lines.append("")

    # ==================== Section 5: Step-by-Step SOP ====================
    lines.append("## 5ï¸âƒ£ å¤åˆ» SOP (æ ‡å‡†æ“ä½œæµç¨‹)")
    lines.append("")
    lines.append("### Phase 1: å‡†å¤‡é˜¶æ®µ")
    lines.append("")
    lines.append("```")
    lines.append("æ­¥éª¤ 1.1: ç¡®å®šç›®æ ‡é¢†åŸŸå’Œåˆ†ç±»ä½“ç³»")
    lines.append("         â”œâ”€ å‚è€ƒä¸Šæ–¹ã€Œä»»åŠ¡åˆ†ç±»ä½“ç³»ã€")
    lines.append("         â””â”€ ç¡®å®šè¦è¦†ç›–çš„ context_category åˆ—è¡¨")
    lines.append("")
    lines.append("æ­¥éª¤ 1.2: æ”¶é›†åŸå§‹ä¸Šä¸‹æ–‡ææ–™")
    lines.append("         â”œâ”€ ä¸“ä¸šæ–‡æ¡£ã€æ‰‹å†Œã€è§„èŒƒ")
    lines.append("         â”œâ”€ ç¡®ä¿ææ–™ä¸åœ¨ LLM è®­ç»ƒæ•°æ®ä¸­")
    lines.append("         â””â”€ æ¯ä¸ªåˆ†ç±»å‡†å¤‡ 10-20 ä»½ææ–™")
    lines.append("")
    lines.append("æ­¥éª¤ 1.3: å‡†å¤‡ System Prompt æ¨¡æ¿")
    lines.append("         â”œâ”€ å‚è€ƒä¸Šæ–¹ã€ŒSystem Prompt æ¨¡æ¿åº“ã€")
    lines.append("         â””â”€ æŒ‰é¢†åŸŸå®šåˆ¶è§’è‰²è®¾å®š")
    lines.append("```")
    lines.append("")

    lines.append("### Phase 2: æ•°æ®ç”Ÿæˆé˜¶æ®µ")
    lines.append("")
    lines.append("```")
    lines.append("æ­¥éª¤ 2.1: ç¼–å†™ System Prompt")
    lines.append("         â”œâ”€ å®šä¹‰ AI è§’è‰²å’Œèƒ½åŠ›è¾¹ç•Œ")
    lines.append("         â”œâ”€ è®¾ç½®è¾“å‡ºæ ¼å¼çº¦æŸ")
    lines.append("         â””â”€ æ·»åŠ é¢†åŸŸç‰¹å®šæŒ‡ä»¤")
    lines.append("")
    lines.append("æ­¥éª¤ 2.2: æ„é€  User Query")
    lines.append("         â”œâ”€ åµŒå…¥ä¸Šä¸‹æ–‡ææ–™")
    lines.append("         â”œâ”€ è®¾è®¡éœ€è¦ç†è§£ä¸Šä¸‹æ–‡æ‰èƒ½å›ç­”çš„é—®é¢˜")
    lines.append("         â””â”€ é—®é¢˜åº”æœ‰æ˜ç¡®çš„è¯„ä¼°æ ‡å‡†")
    lines.append("")
    lines.append("æ­¥éª¤ 2.3: ç¼–å†™è¯„åˆ†æ ‡å‡† (Rubrics)")
    lines.append("         â”œâ”€ éµå¾ªä¸Šæ–¹ã€Œè¯„åˆ†æ ‡å‡†ç¼–å†™è§„èŒƒã€")
    lines.append("         â”œâ”€ æ¯ä¸ªä»»åŠ¡ 8-15 æ¡è¯„åˆ†æ ‡å‡†")
    lines.append("         â”œâ”€ è¦†ç›–ï¼šæ­£ç¡®æ€§ã€å®Œæ•´æ€§ã€æ ¼å¼ã€çº¦æŸ")
    lines.append("         â””â”€ ä½¿ç”¨ Fail if ... æ˜ç¡®å¤±è´¥æ¡ä»¶")
    lines.append("```")
    lines.append("")

    lines.append("### Phase 3: è´¨é‡æ§åˆ¶é˜¶æ®µ")
    lines.append("")
    lines.append("```")
    lines.append("æ­¥éª¤ 3.1: è‡ªæ£€")
    lines.append("         â”œâ”€ [ ] é—®é¢˜å¿…é¡»ä¾èµ–ä¸Šä¸‹æ–‡æ‰èƒ½å›ç­”")
    lines.append("         â”œâ”€ [ ] è¯„åˆ†æ ‡å‡†å¯é‡åŒ–ã€å¯æ‰§è¡Œ")
    lines.append("         â””â”€ [ ] æ•°æ®æ ¼å¼ç¬¦åˆ Schema è§„èŒƒ")
    lines.append("")
    lines.append("æ­¥éª¤ 3.2: äº¤å‰å®¡æ ¸")
    lines.append("         â”œâ”€ å¦ä¸€æ ‡æ³¨å‘˜ç‹¬ç«‹è¯„ä¼°")
    lines.append("         â”œâ”€ æ£€æŸ¥è¯„åˆ†æ ‡å‡†æ˜¯å¦é—æ¼")
    lines.append("         â””â”€ éªŒè¯æ ‡å‡†æ˜¯å¦å­˜åœ¨æ­§ä¹‰")
    lines.append("")
    lines.append("æ­¥éª¤ 3.3: æŠ½æ ·æµ‹è¯•")
    lines.append("         â”œâ”€ ç”¨ LLM ç”Ÿæˆå›ç­”")
    lines.append("         â”œâ”€ æŒ‰ Rubrics è¯„åˆ†")
    lines.append("         â””â”€ éªŒè¯è¯„åˆ†æ ‡å‡†çš„åŒºåˆ†åº¦")
    lines.append("```")
    lines.append("")

    lines.append("---")
    lines.append("")

    # ==================== Section 6: Complete Example ====================
    lines.append("## 6ï¸âƒ£ å®Œæ•´æ•°æ®ç¤ºä¾‹")
    lines.append("")

    if sample_items:
        item = sample_items[0]
        lines.append("```json")
        # Create a clean version for display
        display_item = {}
        for k, v in item.items():
            if k == "messages" and isinstance(v, list):
                display_messages = []
                for msg in v:
                    if isinstance(msg, dict):
                        content = msg.get("content", "")
                        if len(content) > 500:
                            msg = dict(msg)
                            msg["content"] = content[:500] + "... (truncated)"
                        display_messages.append(msg)
                display_item[k] = display_messages
            elif k == "rubrics" and isinstance(v, list):
                display_item[k] = v[:5] + ["... (truncated)"] if len(v) > 5 else v
            elif isinstance(v, str) and len(v) > 300:
                display_item[k] = v[:300] + "... (truncated)"
            else:
                display_item[k] = v
        lines.append(json.dumps(display_item, indent=2, ensure_ascii=False))
        lines.append("```")
    else:
        lines.append("*æ— å¯ç”¨ç¤ºä¾‹*")
    lines.append("")

    lines.append("---")
    lines.append("")

    # ==================== Section 7: Resource Estimation ====================
    lines.append("## 7ï¸âƒ£ èµ„æºä¼°ç®—")
    lines.append("")

    if allocation:
        lines.append("### 7.1 äººåŠ›é…ç½®å»ºè®®")
        lines.append("")
        lines.append("| è§’è‰² | äººæ•° | ä¸»è¦èŒè´£ |")
        lines.append("|------|------|----------|")
        lines.append("| é¢†åŸŸä¸“å®¶ | 2-4 | æä¾›ä¸Šä¸‹æ–‡ææ–™ï¼Œå®¡æ ¸ä¸“ä¸šæ€§ |")
        lines.append("| ä»»åŠ¡è®¾è®¡å¸ˆ | 1-2 | è®¾è®¡é—®é¢˜ï¼Œç¡®ä¿è¯„æµ‹æ•ˆåº¦ |")
        lines.append("| æ ‡æ³¨å‘˜ | 3-5 | ç¼–å†™è¯„åˆ†æ ‡å‡†ï¼Œæ ‡æ³¨æ•°æ® |")
        lines.append("| QA | 1-2 | è´¨é‡æŠ½æ£€ï¼Œä¸€è‡´æ€§æ ¡éªŒ |")
        lines.append("")

        lines.append("### 7.2 æˆæœ¬ä¼°ç®—")
        lines.append("")
        lines.append(f"- **äººå·¥æˆæœ¬**: ${allocation.total_human_cost:,.0f}")
        lines.append(f"- **API æˆæœ¬**: ${allocation.total_machine_cost:,.0f}")
        lines.append(f"- **æ€»è®¡**: ${allocation.total_cost:,.0f}")
        lines.append("")

    lines.append("---")
    lines.append("")

    # ==================== Section 8: Checklist ====================
    lines.append("## 8ï¸âƒ£ å‘å¸ƒå‰æ£€æŸ¥æ¸…å•")
    lines.append("")
    lines.append("### æ•°æ®è´¨é‡")
    lines.append("")
    lines.append("- [ ] æ‰€æœ‰å­—æ®µç¬¦åˆ Schema è§„èŒƒ")
    lines.append("- [ ] æ— ç©ºå€¼æˆ–å¼‚å¸¸å€¼")
    lines.append("- [ ] ä¸Šä¸‹æ–‡ææ–™ä¸åœ¨å…¬å¼€è®­ç»ƒé›†ä¸­")
    lines.append("- [ ] è¯„åˆ†æ ‡å‡†æ— æ­§ä¹‰ï¼Œå¯é‡åŒ–æ‰§è¡Œ")
    lines.append("")
    lines.append("### è¦†ç›–åº¦")
    lines.append("")
    lines.append("- [ ] å„åˆ†ç±»æ•°æ®é‡å‡è¡¡")
    lines.append("- [ ] éš¾åº¦åˆ†å¸ƒåˆç†")
    lines.append("- [ ] é¢†åŸŸè¦†ç›–å®Œæ•´")
    lines.append("")
    lines.append("### åˆè§„æ€§")
    lines.append("")
    lines.append("- [ ] æ— ç‰ˆæƒé—®é¢˜")
    lines.append("- [ ] æ— éšç§ä¿¡æ¯æ³„éœ²")
    lines.append("- [ ] æ ‡æ³¨è®¸å¯è¯æ˜ç¡®")
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("> æŒ‡å—ç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ")

    return "\n".join(lines)


@main.command("batch-from-radar")
@click.argument("radar_report")
@click.option("--output-dir", "-o", default="./analysis_output", help="Output directory")
@click.option("--sample-size", "-n", default=200, help="Number of samples per dataset")
@click.option("--limit", "-l", default=0, type=int, help="Max datasets to analyze (0 = all)")
@click.option("--orgs", help="Filter by orgs (comma-separated)")
@click.option("--categories", help="Filter by categories (comma-separated)")
@click.option("--min-downloads", default=0, type=int, help="Minimum downloads")
@click.option("--use-llm", is_flag=True, help="Use LLM for unknown types")
@click.option("--region", "-r", default="china", help="Region for cost calculation")
@click.option("--sort-by", type=click.Choice(["downloads", "name", "category"]), default="downloads", help="Sort datasets by")
@click.option("--incremental", "-i", is_flag=True, help="Skip already analyzed datasets")
@click.option("--parallel", "-p", default=1, type=int, help="Parallel workers (1=sequential)")
def batch_from_radar(
    radar_report: str,
    output_dir: str,
    sample_size: int,
    limit: int,
    orgs: str,
    categories: str,
    min_downloads: int,
    use_llm: bool,
    region: str,
    sort_by: str,
    incremental: bool,
    parallel: int,
):
    """
    Batch analyze datasets from an ai-dataset-radar report.

    Reads a radar intel_report JSON file and analyzes all (or filtered) datasets.

    Example:
        datarecipe batch-from-radar ./data/reports/intel_report_2024-01-01.json
        datarecipe batch-from-radar ./report.json --orgs Anthropic,OpenAI --limit 5
        datarecipe batch-from-radar ./report.json --incremental --parallel 3
    """
    import json
    import os
    from datarecipe.integrations.radar import RadarIntegration, RecipeSummary

    console.print(f"\n[bold cyan]{'='*60}[/bold cyan]")
    console.print(f"[bold cyan]  DataRecipe æ‰¹é‡åˆ†æ (Radar é›†æˆ)[/bold cyan]")
    console.print(f"[bold cyan]{'='*60}[/bold cyan]\n")

    # Load radar report
    console.print(f"[dim]ğŸ“‚ åŠ è½½ Radar æŠ¥å‘Š: {radar_report}[/dim]")
    try:
        integration = RadarIntegration()
        all_datasets = integration.load_radar_report(radar_report)
        console.print(f"[green]âœ“ åŠ è½½ {len(all_datasets)} ä¸ªæ•°æ®é›†[/green]")
    except Exception as e:
        console.print(f"[red]é”™è¯¯: æ— æ³•åŠ è½½ Radar æŠ¥å‘Š - {e}[/red]")
        return

    # Filter datasets
    org_list = [o.strip() for o in orgs.split(",")] if orgs else None
    cat_list = [c.strip() for c in categories.split(",")] if categories else None

    datasets = integration.filter_datasets(
        orgs=org_list,
        categories=cat_list,
        min_downloads=min_downloads,
        limit=0,  # Apply limit after sorting
    )

    if not datasets:
        console.print("[yellow]âš  æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®é›†[/yellow]")
        return

    # Sort datasets
    if sort_by == "downloads":
        datasets.sort(key=lambda x: x.downloads, reverse=True)
    elif sort_by == "name":
        datasets.sort(key=lambda x: x.id.lower())
    elif sort_by == "category":
        datasets.sort(key=lambda x: (x.category or "zzz", -x.downloads))

    # Incremental mode: skip already analyzed
    skipped_count = 0
    if incremental:
        filtered = []
        for ds in datasets:
            safe_name = ds.id.replace("/", "_").replace("\\", "_")
            summary_path = os.path.join(output_dir, safe_name, "recipe_summary.json")
            if os.path.exists(summary_path):
                skipped_count += 1
            else:
                filtered.append(ds)
        datasets = filtered
        if skipped_count > 0:
            console.print(f"[dim]å¢é‡æ¨¡å¼: è·³è¿‡ {skipped_count} ä¸ªå·²åˆ†ææ•°æ®é›†[/dim]")

    # Apply limit after filtering
    if limit > 0:
        datasets = datasets[:limit]

    if not datasets:
        console.print("[green]âœ“ æ‰€æœ‰æ•°æ®é›†å·²åˆ†æå®Œæˆ[/green]")
        return

    console.print(f"[dim]å¾…åˆ†æ: {len(datasets)} ä¸ªæ•°æ®é›† (æ’åº: {sort_by})[/dim]\n")

    # Show datasets to analyze
    console.print("[bold]å¾…åˆ†ææ•°æ®é›†:[/bold]")
    for i, ds in enumerate(datasets[:10], 1):
        console.print(f"  {i}. {ds.id} ({ds.category}, {ds.downloads:,} downloads)")
    if len(datasets) > 10:
        console.print(f"  ... è¿˜æœ‰ {len(datasets) - 10} ä¸ª")
    console.print("")

    # Save progress file for resume capability
    progress_file = os.path.join(output_dir, ".batch_progress.json")

    # Analyze each dataset
    summaries = []
    success_count = 0
    fail_count = 0

    for i, ds in enumerate(datasets, 1):
        console.print(f"\n[bold]â”â”â” [{i}/{len(datasets)}] {ds.id} â”â”â”[/bold]")

        try:
            # Import here to avoid circular imports
            from datasets import load_dataset
            from datarecipe.extractors import RubricsAnalyzer, PromptExtractor
            from datarecipe.analyzers import ContextStrategyDetector
            from datarecipe.generators import HumanMachineSplitter, TaskType

            # Create output directory
            safe_name = ds.id.replace("/", "_").replace("\\", "_")
            dataset_output_dir = os.path.join(output_dir, safe_name)
            os.makedirs(dataset_output_dir, exist_ok=True)

            # Load dataset
            console.print("[dim]  ğŸ“¥ åŠ è½½æ•°æ®...[/dim]")
            try:
                dataset = load_dataset(ds.id, split="train", streaming=True)
            except ValueError:
                # Try test split
                try:
                    dataset = load_dataset(ds.id, split="test", streaming=True)
                except Exception:
                    raise ValueError("æ— æ³•æ‰¾åˆ°å¯ç”¨çš„ split")

            # Collect samples
            schema_info = {}
            sample_items = []
            rubrics = []
            messages = []

            for j, item in enumerate(dataset):
                if j >= sample_size:
                    break

                # Schema info
                if j < 5:
                    for field, value in item.items():
                        if field not in schema_info:
                            schema_info[field] = {
                                "type": type(value).__name__,
                                "nested_type": None
                            }
                    sample_items.append(item)

                # Collect rubrics/messages
                for field in ["rubrics", "rubric", "criteria"]:
                    if field in item:
                        v = item[field]
                        if isinstance(v, list):
                            rubrics.extend(v)
                        elif isinstance(v, str):
                            rubrics.append(v)

                if "messages" in item:
                    messages.extend(item.get("messages", []))

            sample_count = j + 1
            console.print(f"[dim]  âœ“ åŠ è½½ {sample_count} æ ·æœ¬[/dim]")

            # Detect dataset type
            is_preference = "chosen" in schema_info and "rejected" in schema_info
            is_swe = "repo" in schema_info and "patch" in schema_info

            dataset_type = ds.category or ""
            if is_preference:
                dataset_type = "preference"
            elif is_swe:
                dataset_type = "swe_bench"
            elif rubrics:
                dataset_type = "evaluation"

            # Human-machine allocation
            console.print("[dim]  âš™ï¸ è®¡ç®—æˆæœ¬...[/dim]")
            splitter = HumanMachineSplitter(region=region)
            allocation = splitter.analyze(
                dataset_size=sample_count,
                task_types=[
                    TaskType.CONTEXT_CREATION,
                    TaskType.TASK_DESIGN,
                    TaskType.RUBRICS_WRITING,
                    TaskType.DATA_GENERATION,
                    TaskType.QUALITY_REVIEW,
                ]
            )

            # Rubrics analysis
            rubrics_result = None
            if rubrics:
                analyzer = RubricsAnalyzer()
                rubrics_result = analyzer.analyze(rubrics, task_count=sample_count)

            # Prompt analysis
            prompt_library = None
            if messages:
                extractor = PromptExtractor()
                prompt_library = extractor.extract(messages)

            # LLM analysis for unknown types
            llm_analysis = None
            if use_llm and not dataset_type:
                console.print("[dim]  ğŸ¤– LLM åˆ†æä¸­...[/dim]")
                try:
                    from datarecipe.analyzers.llm_dataset_analyzer import LLMDatasetAnalyzer
                    llm_analyzer = LLMDatasetAnalyzer()
                    llm_analysis = llm_analyzer.analyze(
                        dataset_id=ds.id,
                        schema_info=schema_info,
                        sample_items=sample_items,
                        sample_count=sample_count,
                    )
                    dataset_type = llm_analysis.dataset_type
                except Exception as e:
                    console.print(f"[yellow]  âš  LLM åˆ†æå¤±è´¥: {e}[/yellow]")

            # Create summary
            summary = RadarIntegration.create_summary(
                dataset_id=ds.id,
                dataset_type=dataset_type,
                category=ds.category,
                allocation=allocation,
                rubrics_result=rubrics_result,
                prompt_library=prompt_library,
                schema_info=schema_info,
                sample_count=sample_count,
                llm_analysis=llm_analysis,
                output_dir=dataset_output_dir,
            )

            # Save summary
            RadarIntegration.save_summary(summary, dataset_output_dir)
            summaries.append(summary)
            success_count += 1

            console.print(f"[green]  âœ“ å®Œæˆ: {dataset_type or 'unknown'}, ${allocation.total_cost:,.0f}[/green]")

            # Update progress file
            progress = {
                "total": len(datasets),
                "completed": success_count,
                "failed": fail_count,
                "last_dataset": ds.id,
                "summaries": [s.dataset_id for s in summaries],
            }
            with open(progress_file, "w", encoding="utf-8") as f:
                json.dump(progress, f, indent=2)

        except Exception as e:
            fail_count += 1
            console.print(f"[red]  âœ— å¤±è´¥: {e}[/red]")

            # Log failed dataset
            failed_log = os.path.join(output_dir, ".batch_failed.log")
            with open(failed_log, "a", encoding="utf-8") as f:
                f.write(f"{ds.id}: {e}\n")
            continue

    # Clean up progress file on completion
    if os.path.exists(progress_file):
        os.remove(progress_file)

    # Generate aggregated report
    console.print(f"\n[bold cyan]{'='*60}[/bold cyan]")
    console.print("[bold cyan]  æ‰¹é‡åˆ†æå®Œæˆ[/bold cyan]")
    console.print(f"[bold cyan]{'='*60}[/bold cyan]\n")

    console.print(f"æˆåŠŸ: [green]{success_count}[/green]")
    console.print(f"å¤±è´¥: [red]{fail_count}[/red]")
    if skipped_count > 0:
        console.print(f"è·³è¿‡: [dim]{skipped_count}[/dim] (å·²åˆ†æ)")

    if summaries:
        # Save aggregated summary
        aggregate = RadarIntegration.aggregate_summaries(summaries)
        aggregate_path = os.path.join(output_dir, "batch_summary.json")
        with open(aggregate_path, "w", encoding="utf-8") as f:
            json.dump(aggregate, f, indent=2, ensure_ascii=False)

        console.print(f"\n[bold]æ±‡æ€»ç»Ÿè®¡:[/bold]")
        console.print(f"  æ€»å¤åˆ»æˆæœ¬: ${aggregate['total_reproduction_cost']['total']:,.0f}")
        console.print(f"  å¹³å‡äººå·¥å æ¯”: {aggregate['avg_human_percentage']:.0f}%")
        console.print(f"  ç±»å‹åˆ†å¸ƒ: {aggregate['type_distribution']}")

        console.print(f"\n[bold]è¾“å‡ºæ–‡ä»¶:[/bold]")
        console.print(f"  ğŸ“Š æ±‡æ€»æŠ¥å‘Š: [cyan]{aggregate_path}[/cyan]")
        console.print(f"  ğŸ“ å„æ•°æ®é›†: [cyan]{output_dir}/<dataset>/recipe_summary.json[/cyan]")


@main.command("integrate-report")
@click.option("--radar-report", "-r", help="Path to Radar intel report JSON")
@click.option("--output-dir", "-o", default="./reports", help="Output directory")
@click.option("--recipe-dir", default="./analysis_output", help="Recipe analysis directory")
@click.option("--start-date", help="Period start date (YYYY-MM-DD)")
@click.option("--end-date", help="Period end date (YYYY-MM-DD)")
@click.option("--format", "-f", "formats", multiple=True, default=["md", "json"], help="Output formats")
def integrate_report(
    radar_report: str,
    output_dir: str,
    recipe_dir: str,
    start_date: str,
    end_date: str,
    formats: tuple,
):
    """
    Generate integrated report combining Radar discoveries and Recipe analysis.

    Example:
        datarecipe integrate-report -r ./intel_report.json -o ./reports
        datarecipe integrate-report --recipe-dir ./analysis_output
    """
    from datarecipe.reports import IntegratedReportGenerator

    console.print(f"\n[bold cyan]ç”Ÿæˆæ•´åˆæŠ¥å‘Š[/bold cyan]\n")

    generator = IntegratedReportGenerator(
        recipe_output_dir=recipe_dir,
    )

    # Generate report
    report = generator.generate_weekly_report(
        radar_report_path=radar_report,
        start_date=start_date,
        end_date=end_date,
    )

    # Display summary
    console.print(f"å‘¨æœŸ: {report.period_start} ~ {report.period_end}")
    console.print(f"å‘ç°æ•°æ®é›†: {report.total_discovered}")
    console.print(f"å·²åˆ†æ: {report.total_analyzed}")
    console.print(f"æ€»å¤åˆ»æˆæœ¬: ${report.total_reproduction_cost:,.0f}")
    console.print("")

    if report.insights:
        console.print("[bold]æ´å¯Ÿ:[/bold]")
        for insight in report.insights:
            console.print(f"  â€¢ {insight}")
        console.print("")

    # Save report
    paths = generator.save_report(report, output_dir, list(formats))

    console.print("[bold]ç”Ÿæˆæ–‡ä»¶:[/bold]")
    for fmt, path in paths.items():
        console.print(f"  ğŸ“„ {path}")


@main.command("watch")
@click.argument("watch_dir")
@click.option("--output-dir", "-o", default="./analysis_output", help="Output directory")
@click.option("--interval", "-i", default=60, type=int, help="Check interval in seconds")
@click.option("--config", "-c", help="Path to trigger config YAML")
@click.option("--orgs", help="Filter by orgs (comma-separated)")
@click.option("--categories", help="Filter by categories (comma-separated)")
@click.option("--min-downloads", default=0, type=int, help="Minimum downloads")
@click.option("--limit", "-l", default=10, type=int, help="Max datasets per report")
@click.option("--once", is_flag=True, help="Check once and exit")
def watch_cmd(
    watch_dir: str,
    output_dir: str,
    interval: int,
    config: str,
    orgs: str,
    categories: str,
    min_downloads: int,
    limit: int,
    once: bool,
):
    """
    Watch for new Radar reports and auto-analyze datasets.

    Monitors a directory for new intel_report_*.json files and
    automatically triggers analysis for matching datasets.

    Example:
        datarecipe watch ./radar_reports/
        datarecipe watch ./reports --orgs Anthropic,OpenAI --interval 300
        datarecipe watch ./reports --config ./triggers.yaml --once
    """
    from datarecipe.triggers import RadarWatcher, TriggerConfig

    # Build config
    if config:
        trigger_config = TriggerConfig.from_yaml(config)
    else:
        trigger_config = TriggerConfig(
            orgs=[o.strip() for o in orgs.split(",")] if orgs else [],
            categories=[c.strip() for c in categories.split(",")] if categories else [],
            min_downloads=min_downloads,
            max_datasets_per_report=limit,
        )

    console.print(f"\n[bold cyan]DataRecipe Radar Watcher[/bold cyan]\n")
    console.print(f"ç›‘å¬ç›®å½•: {watch_dir}")
    console.print(f"è¾“å‡ºç›®å½•: {output_dir}")
    console.print(f"æ£€æŸ¥é—´éš”: {interval}s")

    if trigger_config.orgs:
        console.print(f"ç»„ç»‡è¿‡æ»¤: {', '.join(trigger_config.orgs)}")
    if trigger_config.categories:
        console.print(f"ç±»å‹è¿‡æ»¤: {', '.join(trigger_config.categories)}")
    if trigger_config.min_downloads:
        console.print(f"æœ€å°ä¸‹è½½: {trigger_config.min_downloads}")

    console.print("")

    # Create watcher
    def on_complete(dataset_id: str, result: dict):
        if result.get("success"):
            console.print(f"[green]âœ“[/green] {dataset_id}: {result.get('type', 'unknown')}, ${result.get('cost', 0):,.0f}")
        else:
            console.print(f"[red]âœ—[/red] {dataset_id}: {result.get('error', 'Unknown error')}")

    watcher = RadarWatcher(
        watch_dir=watch_dir,
        output_dir=output_dir,
        config=trigger_config,
        callback=on_complete,
    )

    if once:
        console.print("[dim]å•æ¬¡æ£€æŸ¥æ¨¡å¼[/dim]\n")
        results = watcher.check_once()

        if not results:
            console.print("[dim]æ²¡æœ‰å‘ç°æ–°æŠ¥å‘Š[/dim]")
        else:
            for r in results:
                console.print(f"å¤„ç†: {r['report']}")
                console.print(f"  æˆåŠŸ: {r['datasets_analyzed']}, å¤±è´¥: {r['datasets_failed']}")
    else:
        try:
            watcher.watch(interval=interval)
        except KeyboardInterrupt:
            console.print("\n[dim]å·²åœæ­¢[/dim]")


@main.command("cache")
@click.option("--list", "-l", "list_cache", is_flag=True, help="List cached datasets")
@click.option("--stats", "-s", is_flag=True, help="Show cache statistics")
@click.option("--clear", is_flag=True, help="Clear all cache")
@click.option("--clear-expired", is_flag=True, help="Clear only expired entries")
@click.option("--invalidate", help="Invalidate cache for specific dataset")
def cache_cmd(list_cache: bool, stats: bool, clear: bool, clear_expired: bool, invalidate: str):
    """
    Manage the analysis cache.

    Example:
        datarecipe cache --list
        datarecipe cache --stats
        datarecipe cache --clear-expired
        datarecipe cache --invalidate Anthropic/hh-rlhf
    """
    from datarecipe.cache import AnalysisCache

    cache = AnalysisCache()

    if list_cache:
        entries = cache.list_entries()
        if not entries:
            console.print("[dim]ç¼“å­˜ä¸ºç©º[/dim]")
            return

        console.print("\n[bold]ç¼“å­˜çš„æ•°æ®é›†[/bold]\n")
        console.print("| æ•°æ®é›† | ç±»å‹ | æ ·æœ¬ | åˆ›å»ºæ—¶é—´ | çŠ¶æ€ |")
        console.print("|--------|------|------|----------|------|")
        for e in entries:
            status = "[red]è¿‡æœŸ[/red]" if e.is_expired() else "[green]æœ‰æ•ˆ[/green]"
            console.print(
                f"| {e.dataset_id} | {e.dataset_type or '-'} | {e.sample_count} | "
                f"{e.created_at[:10]} | {status} |"
            )
        return

    if stats:
        s = cache.get_stats()
        console.print("\n[bold]ç¼“å­˜ç»Ÿè®¡[/bold]\n")
        console.print(f"æ€»æ¡ç›®: {s['total_entries']}")
        console.print(f"æœ‰æ•ˆ: {s['valid_entries']}")
        console.print(f"è¿‡æœŸ: {s['expired_entries']}")
        console.print(f"æ€»å¤§å°: {s['total_size_mb']} MB")
        console.print(f"ç¼“å­˜ç›®å½•: {s['cache_dir']}")
        return

    if clear:
        cache.clear_all(delete_files=True)
        console.print("[green]âœ“ ç¼“å­˜å·²æ¸…ç©º[/green]")
        return

    if clear_expired:
        count = cache.clear_expired(delete_files=True)
        console.print(f"[green]âœ“ æ¸…ç†äº† {count} ä¸ªè¿‡æœŸæ¡ç›®[/green]")
        return

    if invalidate:
        cache.invalidate(invalidate, delete_files=False)
        console.print(f"[green]âœ“ å·²ä½¿ {invalidate} çš„ç¼“å­˜å¤±æ•ˆ[/green]")
        return

    # Default: show stats
    s = cache.get_stats()
    console.print("\n[bold]ç¼“å­˜æ¦‚è§ˆ[/bold]\n")
    console.print(f"ç¼“å­˜æ¡ç›®: {s['total_entries']} ({s['valid_entries']} æœ‰æ•ˆ, {s['expired_entries']} è¿‡æœŸ)")
    console.print(f"å ç”¨ç©ºé—´: {s['total_size_mb']} MB")
    console.print("\nä½¿ç”¨ --help æŸ¥çœ‹æ›´å¤šé€‰é¡¹")


@main.command("knowledge")
@click.option("--report", "-r", is_flag=True, help="Generate knowledge report")
@click.option("--patterns", "-p", is_flag=True, help="Show top patterns")
@click.option("--benchmarks", "-b", is_flag=True, help="Show cost benchmarks")
@click.option("--trends", "-t", is_flag=True, help="Show recent trends")
@click.option("--recommend", help="Get recommendations for a dataset type")
@click.option("--output", "-o", help="Output path for report")
def knowledge_cmd(report: bool, patterns: bool, benchmarks: bool, trends: bool, recommend: str, output: str):
    """
    Query the knowledge base for patterns, benchmarks, and trends.

    Example:
        datarecipe knowledge --report
        datarecipe knowledge --patterns
        datarecipe knowledge --benchmarks
        datarecipe knowledge --recommend preference
    """
    from datarecipe.knowledge import KnowledgeBase

    kb = KnowledgeBase()

    if report:
        output_path = kb.export_report(output)
        console.print(f"[green]âœ“ çŸ¥è¯†åº“æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}[/green]")
        return

    if patterns:
        console.print("\n[bold]Top æ¨¡å¼[/bold]\n")
        stats = kb.patterns.get_pattern_stats()

        if not stats["top_patterns"]:
            console.print("[dim]æš‚æ— æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ deep-analyze[/dim]")
            return

        console.print("| æ¨¡å¼ | ç±»å‹ | å‡ºç°æ¬¡æ•° |")
        console.print("|------|------|----------|")
        for p in stats["top_patterns"]:
            console.print(f"| {p['key']} | {p['type']} | {p['frequency']} |")

        console.print(f"\næ€»æ¨¡å¼æ•°: {stats['total_patterns']}")
        return

    if benchmarks:
        console.print("\n[bold]æˆæœ¬åŸºå‡†[/bold]\n")
        all_benchmarks = kb.trends.get_all_benchmarks()

        if not all_benchmarks:
            console.print("[dim]æš‚æ— æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ deep-analyze[/dim]")
            return

        console.print("| ç±»å‹ | å¹³å‡æˆæœ¬ | èŒƒå›´ | äººå·¥% | æ•°æ®é›†æ•° |")
        console.print("|------|----------|------|-------|----------|")
        for dtype, bench in all_benchmarks.items():
            console.print(
                f"| {dtype} | ${bench.avg_total_cost:,.0f} | "
                f"${bench.min_cost:,.0f}-${bench.max_cost:,.0f} | "
                f"{bench.avg_human_percentage:.0f}% | {len(bench.datasets)} |"
            )
        return

    if trends:
        console.print("\n[bold]è¿‘æœŸè¶‹åŠ¿ (30å¤©)[/bold]\n")
        summary = kb.trends.get_trend_summary(30)

        if summary.get("datasets_analyzed", 0) == 0:
            console.print("[dim]æš‚æ— æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ deep-analyze[/dim]")
            return

        console.print(f"åˆ†ææ•°æ®é›†: {summary['datasets_analyzed']}")
        console.print(f"æ€»å¤åˆ»æˆæœ¬: ${summary['total_cost']:,.0f}")
        console.print(f"å¹³å‡æˆæœ¬: ${summary['avg_cost_per_dataset']:,.0f}/æ•°æ®é›†")

        if summary.get("type_distribution"):
            console.print("\nç±»å‹åˆ†å¸ƒ:")
            for dtype, count in summary["type_distribution"].items():
                console.print(f"  - {dtype}: {count}")
        return

    if recommend:
        console.print(f"\n[bold]{recommend} ç±»å‹æ¨è[/bold]\n")
        recs = kb.get_recommendations(recommend)

        if recs.get("cost_estimate"):
            ce = recs["cost_estimate"]
            console.print(f"æˆæœ¬ä¼°ç®—: ${ce['avg_total']:,.0f} (èŒƒå›´ ${ce['range'][0]:,.0f}-${ce['range'][1]:,.0f})")
            console.print(f"äººå·¥å æ¯”: {ce['avg_human_percentage']:.0f}%")
            console.print(f"åŸºäº: {ce['based_on']} ä¸ªæ•°æ®é›†")

        if recs.get("common_patterns"):
            console.print("\nå¸¸è§æ¨¡å¼:")
            for p in recs["common_patterns"][:5]:
                console.print(f"  - {p['pattern']} ({p['type']})")

        if recs.get("suggested_fields"):
            console.print(f"\nå»ºè®®å­—æ®µ: {', '.join(recs['suggested_fields'][:5])}")
        return

    # Default: show summary
    console.print("\n[bold]çŸ¥è¯†åº“æ¦‚è§ˆ[/bold]\n")
    stats = kb.patterns.get_pattern_stats()
    console.print(f"æ€»æ¨¡å¼æ•°: {stats['total_patterns']}")

    all_benchmarks = kb.trends.get_all_benchmarks()
    console.print(f"æˆæœ¬åŸºå‡†: {len(all_benchmarks)} ç§ç±»å‹")

    console.print("\nä½¿ç”¨ --help æŸ¥çœ‹æ›´å¤šé€‰é¡¹")


@main.command("analyze-spec")
@click.argument("file_path", type=click.Path(exists=True), required=False)
@click.option("--output-dir", "-o", default="./spec_output", help="Output directory")
@click.option("--size", "-s", default=100, type=int, help="Target dataset size (for cost estimation)")
@click.option("--region", "-r", default="china", help="Region for cost calculation (china/us)")
@click.option("--provider", "-p", default="anthropic", type=click.Choice(["anthropic", "openai"]), help="LLM provider")
@click.option("--interactive", "-i", is_flag=True, help="Interactive mode: output prompt, wait for JSON input from stdin")
@click.option("--from-json", "from_json", type=click.Path(exists=True), help="Load analysis from JSON file instead of using LLM")
def analyze_spec(file_path: str, output_dir: str, size: int, region: str, provider: str, interactive: bool, from_json: str):
    """
    Analyze a specification/requirements document and generate project artifacts.

    Supports PDF, Word (docx), images (png/jpg), and text files.
    Uses LLM to extract structured information and generate:
    - Annotation specification
    - Executive summary
    - Milestone plan
    - Cost breakdown
    - Industry benchmark comparison

    Three modes of operation:

    \b
    1. API mode (default): Uses LLM API to analyze document
       datarecipe analyze-spec requirements.pdf

    \b
    2. Interactive mode: For use within Claude Code/Desktop
       datarecipe analyze-spec requirements.pdf --interactive
       (Outputs prompt, waits for JSON on stdin)

    \b
    3. From JSON: Load pre-computed analysis
       datarecipe analyze-spec requirements.pdf --from-json analysis.json
    """
    import json
    import os
    import sys
    from pathlib import Path

    from datarecipe.analyzers.spec_analyzer import SpecAnalyzer
    from datarecipe.generators.spec_output import SpecOutputGenerator

    # Validate arguments
    if not file_path and not from_json:
        console.print("[red]é”™è¯¯: éœ€è¦æä¾›æ–‡æ¡£è·¯å¾„æˆ– --from-json å‚æ•°[/red]")
        return

    # Display header (to stderr in interactive mode)
    output = console if not interactive else Console(file=sys.stderr)

    output.print(f"\n[bold cyan]{'='*60}[/bold cyan]")
    output.print(f"[bold cyan]  DataRecipe éœ€æ±‚æ–‡æ¡£åˆ†æ[/bold cyan]")
    output.print(f"[bold cyan]{'='*60}[/bold cyan]\n")

    if file_path:
        file_name = Path(file_path).name
        output.print(f"æ–‡æ¡£: [bold]{file_name}[/bold]")
    output.print(f"ç›®æ ‡è§„æ¨¡: [bold]{size}[/bold] æ¡")
    output.print(f"åŒºåŸŸ: [bold]{region}[/bold]")

    if interactive:
        output.print(f"æ¨¡å¼: [bold]äº¤äº’æ¨¡å¼[/bold] (ç­‰å¾… stdin è¾“å…¥)\n")
    elif from_json:
        output.print(f"æ¨¡å¼: [bold]ä» JSON åŠ è½½[/bold]\n")
    else:
        output.print(f"LLM: [bold]{provider}[/bold]\n")

    try:
        analyzer = SpecAnalyzer(provider=provider)
        analysis = None

        # Mode 1: From JSON file
        if from_json:
            output.print("[dim]ğŸ“„ ä» JSON åŠ è½½åˆ†æç»“æœ...[/dim]")
            with open(from_json, "r", encoding="utf-8") as f:
                extracted = json.load(f)

            # Parse document if provided (for metadata)
            doc = None
            if file_path:
                doc = analyzer.parse_document(file_path)
                if doc.has_images():
                    output.print(f"[green]âœ“ æ–‡æ¡£è§£æå®Œæˆ (åŒ…å« {len(doc.images)} å¼ å›¾ç‰‡)[/green]")
                else:
                    output.print(f"[green]âœ“ æ–‡æ¡£è§£æå®Œæˆ[/green]")

            analysis = analyzer.create_analysis_from_json(extracted, doc)
            output.print(f"[green]âœ“ åŠ è½½å®Œæˆ: {analysis.project_name or 'æœªå‘½åé¡¹ç›®'}[/green]")

        # Mode 2: Interactive mode
        elif interactive:
            output.print("[dim]ğŸ“„ è§£ææ–‡æ¡£...[/dim]")
            doc = analyzer.parse_document(file_path)

            if doc.has_images():
                output.print(f"[green]âœ“ æ–‡æ¡£è§£æå®Œæˆ (åŒ…å« {len(doc.images)} å¼ å›¾ç‰‡)[/green]")
            else:
                output.print(f"[green]âœ“ æ–‡æ¡£è§£æå®Œæˆ[/green]")

            # Output prompt to stdout
            prompt = analyzer.get_extraction_prompt(doc)
            output.print("\n[bold yellow]=" * 60 + "[/bold yellow]")
            output.print("[bold yellow]è¯·å°†ä»¥ä¸‹å†…å®¹äº¤ç»™ LLM åˆ†æï¼Œç„¶åè¾“å…¥ JSON ç»“æœï¼š[/bold yellow]")
            output.print("[bold yellow]=" * 60 + "[/bold yellow]\n")

            # Print prompt to stdout (for piping to LLM)
            print(prompt)

            output.print("\n[bold yellow]=" * 60 + "[/bold yellow]")
            output.print("[bold yellow]è¯·è¾“å…¥ LLM è¿”å›çš„ JSON (ä»¥ç©ºè¡Œç»“æŸ)ï¼š[/bold yellow]")
            output.print("[bold yellow]=" * 60 + "[/bold yellow]\n")

            # Read JSON from stdin
            json_lines = []
            try:
                for line in sys.stdin:
                    if line.strip() == "":
                        break
                    json_lines.append(line)
            except EOFError:
                pass

            json_text = "".join(json_lines)
            if not json_text.strip():
                output.print("[red]é”™è¯¯: æœªæ”¶åˆ° JSON è¾“å…¥[/red]")
                return

            # Parse JSON
            try:
                # Try to extract JSON from markdown code block
                import re
                json_match = re.search(r"```json\s*(.*?)\s*```", json_text, re.DOTALL)
                if json_match:
                    json_text = json_match.group(1)
                else:
                    json_match = re.search(r"\{.*\}", json_text, re.DOTALL)
                    if json_match:
                        json_text = json_match.group(0)

                extracted = json.loads(json_text)
                analysis = analyzer.create_analysis_from_json(extracted, doc)
                output.print(f"[green]âœ“ JSON è§£ææˆåŠŸ: {analysis.project_name or 'æœªå‘½åé¡¹ç›®'}[/green]")
            except json.JSONDecodeError as e:
                output.print(f"[red]é”™è¯¯: JSON è§£æå¤±è´¥ - {e}[/red]")
                return

        # Mode 3: API mode (default)
        else:
            output.print("[dim]ğŸ“„ è§£ææ–‡æ¡£...[/dim]")
            analysis = analyzer.analyze(file_path)

            if analysis.has_images:
                output.print(f"[green]âœ“ æ–‡æ¡£è§£æå®Œæˆ (åŒ…å« {analysis.image_count} å¼ å›¾ç‰‡)[/green]")
            else:
                output.print(f"[green]âœ“ æ–‡æ¡£è§£æå®Œæˆ[/green]")

            output.print("[dim]ğŸ¤– ä½¿ç”¨ LLM æå–ç»“æ„åŒ–ä¿¡æ¯...[/dim]")
            if analysis.project_name:
                output.print(f"[green]âœ“ è¯†åˆ«é¡¹ç›®: {analysis.project_name}[/green]")
                output.print(f"  ç±»å‹: {analysis.dataset_type or 'unknown'}")
                output.print(f"  éš¾åº¦: {analysis.estimated_difficulty or 'unknown'}")
                output.print(f"  äººå·¥å æ¯”: {analysis.estimated_human_percentage:.0f}%")
            else:
                output.print("[yellow]âš  LLM æå–ä¿¡æ¯æœ‰é™ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼[/yellow]")

        # Step 3: LLM Enhancement (optional, enriches document quality)
        enhanced_context = None
        try:
            from datarecipe.generators.llm_enhancer import LLMEnhancer
            enhance_mode = "api" if not interactive else "interactive"
            enhancer = LLMEnhancer(mode=enhance_mode, provider=provider)
            enhanced_context = enhancer.enhance(
                dataset_id=analysis.project_name or "spec_analysis",
                dataset_type=analysis.dataset_type or "unknown",
                domain=analysis.estimated_domain or "é€šç”¨",
                difficulty=analysis.estimated_difficulty or "medium",
                human_percentage=analysis.estimated_human_percentage,
                total_cost=0,
            )
            if enhanced_context and enhanced_context.generated:
                output.print("[green]âœ“ LLM å¢å¼ºå®Œæˆ[/green]")
        except Exception:
            pass

        # Step 4: Generate outputs
        output.print("[dim]ğŸ“ ç”Ÿæˆé¡¹ç›®æ–‡æ¡£...[/dim]")
        generator = SpecOutputGenerator(output_dir=output_dir)
        result = generator.generate(
            analysis=analysis,
            target_size=size,
            region=region,
            enhanced_context=enhanced_context,
        )

        if not result.success:
            output.print(f"[red]é”™è¯¯: {result.error}[/red]")
            return

        output.print(f"[green]âœ“ ç”Ÿæˆå®Œæˆ[/green]")

        # Display summary
        output.print(f"\n[bold cyan]{'='*60}[/bold cyan]")
        output.print(f"[bold cyan]  åˆ†æå®Œæˆ[/bold cyan]")
        output.print(f"[bold cyan]{'='*60}[/bold cyan]\n")

        output.print(f"[bold]ç”Ÿæˆçš„æ–‡ä»¶:[/bold]")
        for fname in result.files_generated:
            fpath = os.path.join(result.output_dir, fname)
            if os.path.exists(fpath):
                fsize = os.path.getsize(fpath)
                if fsize > 1024:
                    size_str = f"{fsize / 1024:.1f}KB"
                else:
                    size_str = f"{fsize}B"
                icon = "ğŸ“Š" if fname.endswith(".json") else "ğŸ“„" if fname.endswith(".md") else "ğŸ“‘"
                output.print(f"  {icon} {fname} ({size_str})")

        output.print(f"\n[bold]è¾“å‡ºç›®å½•:[/bold] [cyan]{result.output_dir}[/cyan]")

        # Key files
        output.print(f"\n[bold]æ ¸å¿ƒäº§å‡º:[/bold]")
        output.print(f"  ğŸ“„ æ‰§è¡Œæ‘˜è¦: [cyan]{result.output_dir}/01_å†³ç­–å‚è€ƒ/EXECUTIVE_SUMMARY.md[/cyan]")
        output.print(f"  ğŸ“‹ é‡Œç¨‹ç¢‘è®¡åˆ’: [cyan]{result.output_dir}/02_é¡¹ç›®ç®¡ç†/MILESTONE_PLAN.md[/cyan]")
        output.print(f"  ğŸ“ æ ‡æ³¨è§„èŒƒ: [cyan]{result.output_dir}/03_æ ‡æ³¨è§„èŒƒ/ANNOTATION_SPEC.md[/cyan]")

    except FileNotFoundError as e:
        output.print(f"[red]é”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ° - {e}[/red]")
    except ValueError as e:
        output.print(f"[red]é”™è¯¯: {e}[/red]")
    except ImportError as e:
        output.print(f"[red]é”™è¯¯: ç¼ºå°‘ä¾èµ– - {e}[/red]")
        output.print("[dim]è¯·å®‰è£…æ‰€éœ€ä¾èµ–: pip install anthropic pymupdf python-docx[/dim]")
    except Exception as e:
        output.print(f"[red]é”™è¯¯: {e}[/red]")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
