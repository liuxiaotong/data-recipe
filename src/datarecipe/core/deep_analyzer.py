"""Core deep analysis functionality shared between CLI and MCP."""

import json
import os
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any

# Output directory structure
OUTPUT_SUBDIRS = {
    "decision": "01_å†³ç­–å‚è€ƒ",  # Executive summary
    "project": "02_é¡¹ç›®ç®¡ç†",  # Milestone plan, industry benchmark
    "annotation": "03_æ ‡æ³¨è§„èŒƒ",  # Annotation spec, rubric templates
    "guide": "04_å¤åˆ»æŒ‡å—",  # Reproduction guide, analysis report
    "cost": "05_æˆæœ¬åˆ†æž",  # Cost breakdown, allocation, token analysis
    "data": "06_åŽŸå§‹æ•°æ®",  # Raw analysis data
    "ai_agent": "08_AI_Agent",  # AI Agent layer
}


class OutputManager:
    """Manage organized output directory structure."""

    def __init__(self, base_dir: str):
        self.base_dir = base_dir
        self.subdirs = {}
        self._create_structure()

    def _create_structure(self):
        """Create subdirectory structure."""
        os.makedirs(self.base_dir, exist_ok=True)
        for key, subdir in OUTPUT_SUBDIRS.items():
            path = os.path.join(self.base_dir, subdir)
            os.makedirs(path, exist_ok=True)
            self.subdirs[key] = path

    def get_path(self, category: str, filename: str) -> str:
        """Get full path for a file in a category."""
        if category in self.subdirs:
            return os.path.join(self.subdirs[category], filename)
        return os.path.join(self.base_dir, filename)

    def get_relative_path(self, category: str, filename: str) -> str:
        """Get relative path for display."""
        if category in self.subdirs:
            return f"{OUTPUT_SUBDIRS[category]}/{filename}"
        return filename

    def generate_readme(self, dataset_id: str, dataset_type: str) -> str:
        """Generate README.md explaining directory structure."""
        content = f"""# {dataset_id} åˆ†æžäº§å‡º

> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M")}
> æ•°æ®ç±»åž‹: {dataset_type}

## ç›®å½•ç»“æž„

```
{os.path.basename(self.base_dir)}/
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â”œâ”€â”€ recipe_summary.json          # æ ¸å¿ƒæ‘˜è¦ (Radar å…¼å®¹)
â”‚
â”œâ”€â”€ {OUTPUT_SUBDIRS["decision"]}/           # ðŸ‘” å†³ç­–å±‚
â”‚   â”œâ”€â”€ EXECUTIVE_SUMMARY.md     # æ‰§è¡Œæ‘˜è¦ (ä»·å€¼è¯„åˆ†ã€ROI)
â”‚   â””â”€â”€ executive_summary.json
â”‚
â”œâ”€â”€ {OUTPUT_SUBDIRS["project"]}/           # ðŸ“‹ é¡¹ç›®ç®¡ç†
â”‚   â”œâ”€â”€ MILESTONE_PLAN.md        # é‡Œç¨‹ç¢‘è®¡åˆ’ (éªŒæ”¶æ ‡å‡†)
â”‚   â”œâ”€â”€ milestone_plan.json
â”‚   â”œâ”€â”€ INDUSTRY_BENCHMARK.md    # è¡Œä¸šåŸºå‡†å¯¹æ¯”
â”‚   â””â”€â”€ industry_benchmark.json
â”‚
â”œâ”€â”€ {OUTPUT_SUBDIRS["annotation"]}/           # ðŸ“ æ ‡æ³¨å›¢é˜Ÿ
â”‚   â”œâ”€â”€ ANNOTATION_SPEC.md       # æ ‡æ³¨è§„èŒƒ (å¤–åŒ…äº¤ä»˜ç”¨)
â”‚   â”œâ”€â”€ annotation_spec.json
â”‚   â”œâ”€â”€ rubric_template.md       # è¯„åˆ†æ ‡å‡†æ¨¡æ¿
â”‚   â””â”€â”€ rubric_template.json
â”‚
â”œâ”€â”€ {OUTPUT_SUBDIRS["guide"]}/           # ðŸ”§ æŠ€æœ¯å›¢é˜Ÿ
â”‚   â”œâ”€â”€ REPRODUCTION_GUIDE.md    # å¤åˆ»æŒ‡å—
â”‚   â””â”€â”€ ANALYSIS_REPORT.md       # åˆ†æžæŠ¥å‘Š
â”‚
â”œâ”€â”€ {OUTPUT_SUBDIRS["cost"]}/           # ðŸ’° æˆæœ¬åˆ†æž
â”‚   â”œâ”€â”€ COST_BREAKDOWN.md        # æˆæœ¬æ˜Žç»†
â”‚   â”œâ”€â”€ allocation.json          # äººæœºåˆ†é…
â”‚   â”œâ”€â”€ phased_cost.json         # åˆ†é˜¶æ®µæˆæœ¬
â”‚   â”œâ”€â”€ cost_comparison.json     # æ¨¡åž‹æˆæœ¬å¯¹æ¯”
â”‚   â”œâ”€â”€ cost_calibration.json    # æˆæœ¬æ ¡å‡†
â”‚   â””â”€â”€ token_analysis.json      # Token åˆ†æž
â”‚
â”œâ”€â”€ {OUTPUT_SUBDIRS["data"]}/           # ðŸ“Š åŽŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ complexity_analysis.json # å¤æ‚åº¦åˆ†æž
â”‚   â”œâ”€â”€ prompt_templates.json    # Prompt æ¨¡æ¿
â”‚   â””â”€â”€ ...                      # å…¶ä»–åˆ†æžæ•°æ®
â”‚
â””â”€â”€ {OUTPUT_SUBDIRS["ai_agent"]}/          # ðŸ¤– AI Agent
    â”œâ”€â”€ agent_context.json       # èšåˆå…¥å£
    â”œâ”€â”€ workflow_state.json      # å·¥ä½œæµçŠ¶æ€
    â”œâ”€â”€ reasoning_traces.json    # æŽ¨ç†é“¾
    â”œâ”€â”€ pipeline.yaml            # å¯æ‰§è¡Œæµæ°´çº¿
    â””â”€â”€ README.md                # Agent è¯´æ˜Ž
```

## å¿«é€Ÿå¯¼èˆª

| ç›®æ ‡ | æŸ¥çœ‹æ–‡ä»¶ |
|------|----------|
| **å¿«é€Ÿå†³ç­–** | `{OUTPUT_SUBDIRS["decision"]}/EXECUTIVE_SUMMARY.md` |
| **é¡¹ç›®è§„åˆ’** | `{OUTPUT_SUBDIRS["project"]}/MILESTONE_PLAN.md` |
| **å¤–åŒ…æ ‡æ³¨** | `{OUTPUT_SUBDIRS["annotation"]}/ANNOTATION_SPEC.md` |
| **æŠ€æœ¯å¤åˆ»** | `{OUTPUT_SUBDIRS["guide"]}/REPRODUCTION_GUIDE.md` |
| **æˆæœ¬é¢„ç®—** | `{OUTPUT_SUBDIRS["cost"]}/COST_BREAKDOWN.md` |
| **AI Agent** | `{OUTPUT_SUBDIRS["ai_agent"]}/agent_context.json` |

---

> ç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ
"""
        return content


@dataclass
class AnalysisResult:
    """Result of deep analysis."""

    dataset_id: str
    success: bool = True
    error: str = ""

    # Dataset info
    dataset_type: str = ""
    sample_count: int = 0
    fields: list[str] = field(default_factory=list)

    # Cost info
    reproduction_cost: dict[str, float] = field(default_factory=dict)
    human_percentage: float = 0.0

    # Analysis stats
    rubric_patterns: int = 0
    prompt_templates: int = 0

    # Output paths
    output_dir: str = ""
    files_generated: list[str] = field(default_factory=list)

    # Warnings collected during analysis (non-fatal issues)
    warnings: list[str] = field(default_factory=list)

    def to_dict(self) -> dict:
        return {
            "dataset_id": self.dataset_id,
            "success": self.success,
            "error": self.error,
            "dataset_type": self.dataset_type,
            "sample_count": self.sample_count,
            "fields": self.fields,
            "reproduction_cost": self.reproduction_cost,
            "human_percentage": self.human_percentage,
            "rubric_patterns": self.rubric_patterns,
            "prompt_templates": self.prompt_templates,
            "output_dir": self.output_dir,
            "files_generated": self.files_generated,
            "warnings": self.warnings,
        }


class DeepAnalyzerCore:
    """Core deep analysis engine shared between CLI and MCP."""

    def __init__(
        self,
        output_dir: str = "./analysis_output",
        region: str = "china",
        use_llm: bool = False,
        llm_provider: str = "anthropic",
        enhance_mode: str = "auto",
    ):
        self.output_dir = output_dir
        self.region = region
        self.use_llm = use_llm
        self.llm_provider = llm_provider
        self.enhance_mode = enhance_mode

    def analyze(
        self,
        dataset_id: str,
        sample_size: int = 500,
        split: str = None,
        target_size: int = None,
    ) -> AnalysisResult:
        """Run full deep analysis on a dataset.

        Args:
            dataset_id: Dataset identifier (e.g., "Anthropic/hh-rlhf")
            sample_size: Number of samples to analyze
            split: Dataset split (auto-detect if None)
            target_size: Target size for cost estimation

        Returns:
            AnalysisResult with all analysis data
        """
        result = AnalysisResult(dataset_id=dataset_id)

        try:
            from datasets import load_dataset

            from datarecipe.analyzers import ContextStrategyDetector
            from datarecipe.extractors import PromptExtractor, RubricsAnalyzer
            from datarecipe.generators import HumanMachineSplitter, TaskType
            from datarecipe.integrations.radar import RadarIntegration

            # Create output directory with organized structure
            safe_name = dataset_id.replace("/", "_").replace("\\", "_").replace(":", "_")
            dataset_output_dir = os.path.join(self.output_dir, safe_name)
            output_mgr = OutputManager(dataset_output_dir)
            result.output_dir = dataset_output_dir

            # Auto-detect split
            if split is None:
                try:
                    ds = load_dataset(dataset_id, split="train", streaming=True)
                    split = "train"
                except ValueError:
                    for try_split in ["test", "validation", "dev"]:
                        try:
                            ds = load_dataset(dataset_id, split=try_split, streaming=True)
                            split = try_split
                            break
                        except ValueError:
                            continue
                    else:
                        raise ValueError("Cannot find available split")
            else:
                ds = load_dataset(dataset_id, split=split, streaming=True)

            # Initialize collectors
            schema_info = {}
            category_set = set()
            sub_category_set = set()
            system_prompts_by_domain = {}
            rubrics_examples = []
            sample_items = []
            rubrics = []
            messages = []
            contexts = []

            # RLHF preference dataset support
            is_preference_dataset = False
            preference_pairs = []
            preference_topics = {}
            preference_patterns = {
                "chosen_longer": 0,
                "rejected_longer": 0,
                "same_length": 0,
                "chosen_more_detailed": 0,
                "chosen_more_helpful": 0,
                "chosen_safer": 0,
            }

            # SWE-bench support
            is_swe_dataset = False
            swe_stats = {
                "repos": {},
                "languages": {},
                "issue_types": {},
                "issue_categories": {},
                "patch_lines": [],
                "examples": [],
            }

            # Collect samples
            sample_count = 0
            for i, item in enumerate(ds):
                if i >= sample_size:
                    break
                sample_count = i + 1

                # Schema info (first 10 items)
                if i < 10:
                    for fld, value in item.items():
                        if fld not in schema_info:
                            schema_info[fld] = {
                                "type": type(value).__name__,
                                "examples": [],
                                "nested_type": None,
                            }
                            if isinstance(value, list) and value:
                                schema_info[fld]["nested_type"] = type(value[0]).__name__
                            elif isinstance(value, dict) and value:
                                schema_info[fld]["nested_type"] = list(value.keys())
                        if len(schema_info[fld]["examples"]) < 3:
                            if isinstance(value, str) and len(value) > 500:
                                schema_info[fld]["examples"].append(value[:500] + "...")
                            elif not isinstance(value, (list, dict)):
                                schema_info[fld]["examples"].append(value)

                # Sample items (first 5)
                if i < 5:
                    sample_items.append(item)

                # Categories from metadata
                if "metadata" in item and isinstance(item["metadata"], dict):
                    meta = item["metadata"]
                    if "context_category" in meta:
                        category_set.add(meta["context_category"])
                    if "sub_category" in meta:
                        sub_category_set.add(meta["sub_category"])
                    if "category" in meta:
                        category_set.add(meta["category"])

                # Rubrics
                item_rubrics = []
                for fld in ["rubrics", "rubric", "criteria"]:
                    if fld in item:
                        value = item[fld]
                        if isinstance(value, list):
                            rubrics.extend(value)
                            item_rubrics.extend(value)
                        elif isinstance(value, str):
                            rubrics.append(value)
                            item_rubrics.append(value)

                if item_rubrics and len(rubrics_examples) < 10:
                    rubrics_examples.append(
                        {
                            "rubrics": item_rubrics,
                            "metadata": item.get("metadata", {}),
                            "messages": item.get("messages", []),
                        }
                    )

                # Messages and system prompts
                if "messages" in item and isinstance(item["messages"], list):
                    messages.extend(item["messages"])
                    for msg in item["messages"]:
                        if isinstance(msg, dict) and msg.get("role") == "system":
                            content = msg.get("content", "")
                            if content and len(content) > 50:
                                domain = "general"
                                if "metadata" in item and isinstance(item["metadata"], dict):
                                    domain = item["metadata"].get(
                                        "context_category",
                                        item["metadata"].get("category", "general"),
                                    )
                                if domain not in system_prompts_by_domain:
                                    system_prompts_by_domain[domain] = []
                                if len(system_prompts_by_domain[domain]) < 3:
                                    system_prompts_by_domain[domain].append(
                                        {"content": content, "metadata": item.get("metadata", {})}
                                    )

                # Contexts
                context_found = False
                for fld in ["context", "input", "text", "document", "passage", "content"]:
                    if fld in item and isinstance(item[fld], str) and len(item[fld]) > 50:
                        contexts.append(item[fld])
                        context_found = True
                        break
                if not context_found and "messages" in item:
                    for msg in item.get("messages", []):
                        if isinstance(msg, dict) and msg.get("role") == "user":
                            content = msg.get("content", "")
                            if isinstance(content, str) and len(content) > 100:
                                contexts.append(content)
                                break

                # RLHF preference detection
                if "chosen" in item and "rejected" in item:
                    is_preference_dataset = True
                    self._analyze_preference_pair(
                        item, preference_pairs, preference_topics, preference_patterns
                    )

                # SWE-bench detection
                if "repo" in item and "patch" in item and "problem_statement" in item:
                    is_swe_dataset = True
                    self._analyze_swe_item(item, swe_stats)

            result.sample_count = sample_count
            result.fields = list(schema_info.keys())
            actual_size = target_size or sample_count

            # Detect dataset type
            detected_type = ""
            if is_swe_dataset:
                detected_type = "swe_bench"
            elif is_preference_dataset:
                detected_type = "preference"
            elif rubrics:
                detected_type = "evaluation"

            # Run analyzers
            rubrics_result = None
            if rubrics:
                analyzer = RubricsAnalyzer()
                rubrics_result = analyzer.analyze(rubrics, task_count=sample_count)
                result.rubric_patterns = rubrics_result.unique_patterns

                # Save rubric analysis to data/
                with open(
                    output_mgr.get_path("data", "rubrics_analysis.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(analyzer.to_dict(rubrics_result), f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("data", "rubrics_analysis.json")
                )

                # Save rubric templates to annotation/
                with open(
                    output_mgr.get_path("annotation", "rubric_template.yaml"), "w", encoding="utf-8"
                ) as f:
                    f.write(analyzer.to_yaml_templates(rubrics_result))
                result.files_generated.append(
                    output_mgr.get_relative_path("annotation", "rubric_template.yaml")
                )

                with open(
                    output_mgr.get_path("annotation", "rubric_template.md"), "w", encoding="utf-8"
                ) as f:
                    f.write(analyzer.to_markdown_templates(rubrics_result))
                result.files_generated.append(
                    output_mgr.get_relative_path("annotation", "rubric_template.md")
                )

            prompt_library = None
            if messages:
                extractor = PromptExtractor()
                prompt_library = extractor.extract(messages)
                result.prompt_templates = prompt_library.unique_count

                with open(
                    output_mgr.get_path("data", "prompt_templates.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(extractor.to_dict(prompt_library), f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("data", "prompt_templates.json")
                )

            strategy_result = None
            if contexts:
                detector = ContextStrategyDetector()
                strategy_result = detector.analyze(contexts[:100])
                with open(
                    output_mgr.get_path("data", "context_strategy.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(detector.to_dict(strategy_result), f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("data", "context_strategy.json")
                )

            # Preference analysis
            if is_preference_dataset and preference_pairs:
                preference_analysis = {
                    "is_preference_dataset": True,
                    "total_pairs": sample_count,
                    "topic_distribution": preference_topics,
                    "patterns": preference_patterns,
                    "examples": preference_pairs[:10],
                }
                with open(
                    output_mgr.get_path("data", "preference_analysis.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(preference_analysis, f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("data", "preference_analysis.json")
                )

            # SWE analysis
            if is_swe_dataset and swe_stats["repos"]:
                avg_patch = (
                    sum(swe_stats["patch_lines"]) / len(swe_stats["patch_lines"])
                    if swe_stats["patch_lines"]
                    else 0
                )
                swe_analysis = {
                    "is_swe_dataset": True,
                    "total_tasks": sample_count,
                    "repos_count": len(swe_stats["repos"]),
                    "repo_distribution": dict(
                        sorted(swe_stats["repos"].items(), key=lambda x: -x[1])[:20]
                    ),
                    "language_distribution": swe_stats["languages"],
                    "avg_patch_lines": avg_patch,
                    "examples": swe_stats["examples"],
                }
                with open(
                    output_mgr.get_path("data", "swe_analysis.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(swe_analysis, f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("data", "swe_analysis.json")
                )

            # LLM analysis
            llm_analysis = None
            is_known_type = is_preference_dataset or is_swe_dataset or rubrics or messages
            if self.use_llm and not is_known_type:
                try:
                    from datarecipe.analyzers.llm_dataset_analyzer import LLMDatasetAnalyzer

                    llm_analyzer = LLMDatasetAnalyzer(provider=self.llm_provider)
                    llm_analysis = llm_analyzer.analyze(
                        dataset_id=dataset_id,
                        schema_info=schema_info,
                        sample_items=sample_items,
                        sample_count=sample_count,
                    )
                    detected_type = llm_analysis.dataset_type

                    llm_result_dict = {
                        "dataset_type": llm_analysis.dataset_type,
                        "purpose": llm_analysis.purpose,
                        "structure_description": llm_analysis.structure_description,
                        "key_fields": llm_analysis.key_fields,
                        "production_steps": llm_analysis.production_steps,
                        "quality_criteria": llm_analysis.quality_criteria,
                        "estimated_difficulty": llm_analysis.estimated_difficulty,
                        "similar_datasets": llm_analysis.similar_datasets,
                    }
                    with open(
                        output_mgr.get_path("data", "llm_analysis.json"), "w", encoding="utf-8"
                    ) as f:
                        json.dump(llm_result_dict, f, indent=2, ensure_ascii=False)
                    result.files_generated.append(
                        output_mgr.get_relative_path("data", "llm_analysis.json")
                    )
                except Exception as e:
                    result.warnings.append(f"LLM æ•°æ®é›†åˆ†æžè·³è¿‡: {e}")

            result.dataset_type = detected_type

            # Precise token-based API cost calculation
            precise_api_cost = None
            token_stats = None
            try:
                from datarecipe.cost import PreciseCostCalculator

                cost_calc = PreciseCostCalculator()
                precise_estimate = cost_calc.calculate(
                    samples=sample_items,
                    target_size=actual_size,
                    model="gpt-4o",
                    iteration_factor=1.2,
                )
                precise_api_cost = precise_estimate.adjusted_cost
                token_stats = precise_estimate.token_stats

                # Save token analysis to cost/
                with open(
                    output_mgr.get_path("cost", "token_analysis.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(precise_estimate.to_dict(), f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("cost", "token_analysis.json")
                )

                # Model comparison
                comparisons = cost_calc.compare_models(
                    samples=sample_items,
                    target_size=actual_size,
                    models=["gpt-4o", "gpt-4o-mini", "claude-3.5-sonnet", "deepseek-v3"],
                )
                comparison_data = {m: e.to_dict() for m, e in comparisons.items()}
                with open(
                    output_mgr.get_path("cost", "cost_comparison.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(comparison_data, f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("cost", "cost_comparison.json")
                )

            except Exception as e:
                result.warnings.append(f"Token æˆæœ¬è®¡ç®—è·³è¿‡: {e}")

            # Complexity analysis for dynamic cost adjustment
            complexity_metrics = None
            try:
                from datarecipe.cost import ComplexityAnalyzer

                complexity_analyzer = ComplexityAnalyzer()
                complexity_metrics = complexity_analyzer.analyze(
                    samples=sample_items,
                    schema_info=schema_info,
                    rubrics=rubrics if rubrics else None,
                )

                # Save complexity analysis to data/
                with open(
                    output_mgr.get_path("data", "complexity_analysis.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(complexity_metrics.to_dict(), f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("data", "complexity_analysis.json")
                )

            except Exception as e:
                result.warnings.append(f"å¤æ‚åº¦åˆ†æžè·³è¿‡: {e}")

            # Human-machine allocation
            splitter = HumanMachineSplitter(region=self.region)
            allocation = splitter.analyze(
                dataset_size=actual_size,
                task_types=[
                    TaskType.CONTEXT_CREATION,
                    TaskType.TASK_DESIGN,
                    TaskType.RUBRICS_WRITING,
                    TaskType.DATA_GENERATION,
                    TaskType.QUALITY_REVIEW,
                ],
            )

            # Apply complexity multipliers to human cost
            human_cost = allocation.total_human_cost
            if complexity_metrics:
                human_cost = human_cost * complexity_metrics.cost_multiplier

            # Use precise API cost if available, otherwise use allocation estimate
            api_cost = precise_api_cost if precise_api_cost else allocation.total_machine_cost

            # Calibrate using historical data
            calibration_result = None
            try:
                from datarecipe.cost import CostCalibrator

                calibrator = CostCalibrator()
                calibration_result = calibrator.calibrate(
                    dataset_type=detected_type or "unknown",
                    human_cost=human_cost,
                    api_cost=api_cost,
                    complexity_metrics=complexity_metrics,
                    sample_count=sample_count,
                )

                # Use calibrated costs
                human_cost = calibration_result.calibrated_human_cost
                api_cost = calibration_result.calibrated_api_cost

                # Save calibration analysis to cost/
                with open(
                    output_mgr.get_path("cost", "cost_calibration.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(calibration_result.to_dict(), f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("cost", "cost_calibration.json")
                )

            except Exception as e:
                result.warnings.append(f"æˆæœ¬æ ¡å‡†è·³è¿‡: {e}")

            total_cost = human_cost + api_cost

            # Phased cost breakdown
            phased_breakdown = None
            try:
                from datarecipe.cost import PhasedCostModel

                phased_model = PhasedCostModel(region=self.region)

                # Calculate API cost per sample for phased model
                api_per_sample = api_cost / actual_size if actual_size > 0 else 0.01
                complexity_mult = complexity_metrics.cost_multiplier if complexity_metrics else 1.0
                quality_req = (
                    complexity_metrics.quality_requirement if complexity_metrics else "standard"
                )

                phased_breakdown = phased_model.calculate(
                    target_size=actual_size,
                    dataset_type=detected_type or "unknown",
                    human_percentage=allocation.human_work_percentage,
                    api_cost_per_sample=api_per_sample,
                    complexity_multiplier=complexity_mult,
                    quality_requirement=quality_req,
                )

                # Save phased cost analysis to cost/
                with open(
                    output_mgr.get_path("cost", "phased_cost.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(phased_breakdown.to_dict(), f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("cost", "phased_cost.json")
                )

                # Save phased cost report to cost/
                phased_report = phased_model.format_report(phased_breakdown)
                with open(
                    output_mgr.get_path("cost", "COST_BREAKDOWN.md"), "w", encoding="utf-8"
                ) as f:
                    f.write(phased_report)
                result.files_generated.append(
                    output_mgr.get_relative_path("cost", "COST_BREAKDOWN.md")
                )

            except Exception as e:
                result.warnings.append(f"åˆ†é˜¶æ®µæˆæœ¬åˆ†æžè·³è¿‡: {e}")

            result.reproduction_cost = {
                "human": round(human_cost, 2),
                "api": round(api_cost, 2),
                "total": round(total_cost, 2),
            }

            # Add phased total if available (includes contingency)
            if phased_breakdown:
                result.reproduction_cost["phased_total"] = round(phased_breakdown.grand_total, 2)

            result.human_percentage = round(
                human_cost / total_cost * 100 if total_cost > 0 else 0, 1
            )

            # Add analysis details to allocation output
            allocation_dict = splitter.to_dict(allocation)
            if token_stats:
                allocation_dict["token_analysis"] = token_stats.to_dict()
            if precise_api_cost:
                allocation_dict["precise_api_cost"] = round(precise_api_cost, 2)
            if complexity_metrics:
                allocation_dict["complexity"] = {
                    "domain": complexity_metrics.primary_domain.value,
                    "difficulty_score": complexity_metrics.difficulty_score,
                    "time_multiplier": complexity_metrics.time_multiplier,
                    "cost_multiplier": complexity_metrics.cost_multiplier,
                }
            if calibration_result:
                allocation_dict["calibration"] = {
                    "method": calibration_result.calibration_method,
                    "confidence": calibration_result.confidence,
                    "based_on": calibration_result.based_on_datasets,
                    "range": {
                        "low": round(calibration_result.cost_range_low, 2),
                        "high": round(calibration_result.cost_range_high, 2),
                    },
                }

            # Final adjusted costs
            allocation_dict["final_costs"] = {
                "human": round(human_cost, 2),
                "api": round(api_cost, 2),
                "total": round(total_cost, 2),
            }

            # Save allocation to cost/
            with open(output_mgr.get_path("cost", "allocation.json"), "w", encoding="utf-8") as f:
                json.dump(allocation_dict, f, indent=2, ensure_ascii=False)
            result.files_generated.append(output_mgr.get_relative_path("cost", "allocation.json"))

            # LLM Enhancement Layer (optional, generates rich context for all reports)
            enhanced_context = None
            if self.use_llm:
                try:
                    from datarecipe.generators.llm_enhancer import LLMEnhancer

                    enhancer = LLMEnhancer(mode=self.enhance_mode, provider=self.llm_provider)
                    enhanced_context = enhancer.enhance(
                        dataset_id=dataset_id,
                        dataset_type=detected_type or "unknown",
                        schema_info=schema_info,
                        sample_items=sample_items,
                        sample_count=sample_count,
                        complexity_metrics=complexity_metrics,
                        allocation=allocation,
                        rubrics_result=rubrics_result,
                        llm_analysis=llm_analysis,
                    )
                    if enhanced_context and enhanced_context.generated:
                        enhanced_dict = {
                            k: v
                            for k, v in enhanced_context.__dict__.items()
                            if k not in ("raw_response",)
                        }
                        with open(
                            output_mgr.get_path("data", "enhanced_context.json"),
                            "w",
                            encoding="utf-8",
                        ) as f:
                            json.dump(enhanced_dict, f, indent=2, ensure_ascii=False, default=str)
                        result.files_generated.append(
                            output_mgr.get_relative_path("data", "enhanced_context.json")
                        )
                except Exception as e:
                    result.warnings.append(f"LLM å¢žå¼ºè·³è¿‡: {e}")

            # Generate reports to guide/
            report = self._generate_analysis_report(
                dataset_id,
                sample_count,
                actual_size,
                rubrics_result,
                prompt_library,
                strategy_result,
                allocation,
                self.region,
                enhanced_context=enhanced_context,
            )
            with open(
                output_mgr.get_path("guide", "ANALYSIS_REPORT.md"), "w", encoding="utf-8"
            ) as f:
                f.write(report)
            result.files_generated.append(
                output_mgr.get_relative_path("guide", "ANALYSIS_REPORT.md")
            )

            guide = self._generate_reproduction_guide(
                dataset_id,
                schema_info,
                category_set,
                sub_category_set,
                system_prompts_by_domain,
                rubrics_examples,
                sample_items,
                rubrics_result,
                prompt_library,
                allocation,
                is_preference_dataset,
                preference_pairs,
                preference_topics,
                preference_patterns,
                is_swe_dataset,
                swe_stats,
                llm_analysis,
                enhanced_context=enhanced_context,
            )
            with open(
                output_mgr.get_path("guide", "REPRODUCTION_GUIDE.md"), "w", encoding="utf-8"
            ) as f:
                f.write(guide)
            result.files_generated.append(
                output_mgr.get_relative_path("guide", "REPRODUCTION_GUIDE.md")
            )

            # Annotation specification (forward-looking production guide)
            try:
                from datarecipe.generators.annotation_spec import AnnotationSpecGenerator

                spec_generator = AnnotationSpecGenerator()
                annotation_spec = spec_generator.generate(
                    dataset_id=dataset_id,
                    dataset_type=detected_type or "unknown",
                    schema_info=schema_info,
                    sample_items=sample_items,
                    rubrics_result=rubrics_result,
                    llm_analysis=llm_analysis,
                    complexity_metrics=complexity_metrics,
                    enhanced_context=enhanced_context,
                )

                # Save as Markdown to annotation/
                spec_md = spec_generator.to_markdown(annotation_spec)
                with open(
                    output_mgr.get_path("annotation", "ANNOTATION_SPEC.md"), "w", encoding="utf-8"
                ) as f:
                    f.write(spec_md)
                result.files_generated.append(
                    output_mgr.get_relative_path("annotation", "ANNOTATION_SPEC.md")
                )

                # Save as JSON to annotation/
                spec_dict = spec_generator.to_dict(annotation_spec)
                with open(
                    output_mgr.get_path("annotation", "annotation_spec.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(spec_dict, f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("annotation", "annotation_spec.json")
                )

            except Exception as e:
                result.warnings.append(f"æ ‡æ³¨è§„èŒƒç”Ÿæˆå¤±è´¥: {e}")

            # Milestone plan (for project management)
            try:
                from datarecipe.generators.milestone_plan import MilestonePlanGenerator

                milestone_generator = MilestonePlanGenerator()
                milestone_plan = milestone_generator.generate(
                    dataset_id=dataset_id,
                    dataset_type=detected_type or "unknown",
                    target_size=actual_size,
                    reproduction_cost=result.reproduction_cost,
                    human_percentage=result.human_percentage,
                    complexity_metrics=complexity_metrics,
                    phased_breakdown=phased_breakdown,
                    enhanced_context=enhanced_context,
                )

                # Save as Markdown to project/
                milestone_md = milestone_generator.to_markdown(milestone_plan)
                with open(
                    output_mgr.get_path("project", "MILESTONE_PLAN.md"), "w", encoding="utf-8"
                ) as f:
                    f.write(milestone_md)
                result.files_generated.append(
                    output_mgr.get_relative_path("project", "MILESTONE_PLAN.md")
                )

                # Save as JSON to project/
                milestone_dict = milestone_generator.to_dict(milestone_plan)
                with open(
                    output_mgr.get_path("project", "milestone_plan.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(milestone_dict, f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("project", "milestone_plan.json")
                )

            except Exception as e:
                result.warnings.append(f"é‡Œç¨‹ç¢‘è®¡åˆ’ç”Ÿæˆå¤±è´¥: {e}")

            # Executive summary (for decision makers)
            try:
                from datarecipe.generators.executive_summary import ExecutiveSummaryGenerator

                exec_generator = ExecutiveSummaryGenerator()
                exec_assessment = exec_generator.generate(
                    dataset_id=dataset_id,
                    dataset_type=detected_type or "unknown",
                    sample_count=sample_count,
                    reproduction_cost=result.reproduction_cost,
                    human_percentage=result.human_percentage,
                    complexity_metrics=complexity_metrics,
                    phased_breakdown=phased_breakdown,
                    llm_analysis=llm_analysis,
                    enhanced_context=enhanced_context,
                )

                # Save as Markdown to decision/
                exec_md = exec_generator.to_markdown(
                    assessment=exec_assessment,
                    dataset_id=dataset_id,
                    dataset_type=detected_type or "unknown",
                    reproduction_cost=result.reproduction_cost,
                    phased_breakdown=phased_breakdown,
                )
                with open(
                    output_mgr.get_path("decision", "EXECUTIVE_SUMMARY.md"), "w", encoding="utf-8"
                ) as f:
                    f.write(exec_md)
                result.files_generated.append(
                    output_mgr.get_relative_path("decision", "EXECUTIVE_SUMMARY.md")
                )

                # Save as JSON to decision/
                exec_dict = exec_generator.to_dict(exec_assessment)
                with open(
                    output_mgr.get_path("decision", "executive_summary.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(exec_dict, f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("decision", "executive_summary.json")
                )

            except Exception as e:
                result.warnings.append(f"æ‰§è¡Œæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")

            # Industry benchmark comparison
            try:
                from datarecipe.generators.industry_benchmark import IndustryBenchmarkGenerator

                benchmark_generator = IndustryBenchmarkGenerator()
                benchmark_comparison = benchmark_generator.generate(
                    dataset_id=dataset_id,
                    dataset_type=detected_type or "unknown",
                    sample_count=actual_size,
                    reproduction_cost=result.reproduction_cost,
                    human_percentage=result.human_percentage,
                )

                # Save as Markdown to project/
                benchmark_md = benchmark_generator.to_markdown(benchmark_comparison)
                with open(
                    output_mgr.get_path("project", "INDUSTRY_BENCHMARK.md"), "w", encoding="utf-8"
                ) as f:
                    f.write(benchmark_md)
                result.files_generated.append(
                    output_mgr.get_relative_path("project", "INDUSTRY_BENCHMARK.md")
                )

                # Save as JSON to project/
                benchmark_dict = benchmark_generator.to_dict(benchmark_comparison)
                with open(
                    output_mgr.get_path("project", "industry_benchmark.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(benchmark_dict, f, indent=2, ensure_ascii=False)
                result.files_generated.append(
                    output_mgr.get_relative_path("project", "industry_benchmark.json")
                )

            except Exception as e:
                result.warnings.append(f"è¡Œä¸šåŸºå‡†å¯¹æ¯”ç”Ÿæˆå¤±è´¥: {e}")

            # Recipe summary (stays in root)
            summary = RadarIntegration.create_summary(
                dataset_id=dataset_id,
                dataset_type=detected_type,
                purpose=llm_analysis.purpose if llm_analysis else "",
                allocation=allocation,
                rubrics_result=rubrics_result,
                prompt_library=prompt_library,
                schema_info=schema_info,
                sample_count=sample_count,
                llm_analysis=llm_analysis,
                output_dir=dataset_output_dir,
                complexity_metrics=complexity_metrics,
            )
            RadarIntegration.save_summary(summary, dataset_output_dir)
            result.files_generated.append("recipe_summary.json")

            # Generate README.md for directory navigation
            readme_content = output_mgr.generate_readme(dataset_id, detected_type or "unknown")
            with open(os.path.join(dataset_output_dir, "README.md"), "w", encoding="utf-8") as f:
                f.write(readme_content)
            result.files_generated.append("README.md")

            # Generate AI Agent layer
            try:
                self._generate_ai_agent_layer(
                    output_mgr=output_mgr,
                    result=result,
                    dataset_id=dataset_id,
                    dataset_type=detected_type or "unknown",
                    sample_count=sample_count,
                    actual_size=actual_size,
                    allocation=allocation,
                    complexity_metrics=complexity_metrics,
                    rubrics_result=rubrics_result,
                    prompt_library=prompt_library,
                    llm_analysis=llm_analysis,
                    is_preference_dataset=is_preference_dataset,
                    is_swe_dataset=is_swe_dataset,
                )
            except Exception as e:
                result.warnings.append(f"AI Agent å±‚ç”Ÿæˆå¤±è´¥: {e}")

            # Update knowledge base
            try:
                from datarecipe.knowledge import KnowledgeBase

                kb = KnowledgeBase()
                kb.ingest_analysis(
                    dataset_id=dataset_id,
                    summary=summary,
                    rubrics_result=rubrics_result,
                    prompt_library=prompt_library,
                )
            except Exception:
                pass  # Knowledge base is optional

            # Update cache
            try:
                from datarecipe.cache import AnalysisCache

                cache = AnalysisCache()
                cache.put(
                    dataset_id=dataset_id,
                    output_dir=dataset_output_dir,
                    dataset_type=detected_type,
                    sample_count=sample_count,
                )
            except Exception:
                pass  # Cache is optional

        except Exception as e:
            result.success = False
            result.error = str(e)

        return result

    def _analyze_preference_pair(self, item, pairs, topics, patterns):
        """Analyze a preference pair item."""
        import re

        chosen = item.get("chosen", "")
        rejected = item.get("rejected", "")

        if not isinstance(chosen, str) or not isinstance(rejected, str):
            return

        # Parse conversation
        def parse_conv(text):
            turns = []
            for h_pat, a_pat in [
                (r"\n\nHuman:", r"\n\nAssistant:"),
                (r"\nHuman:", r"\nAssistant:"),
            ]:
                if h_pat.replace(r"\n", "\n") in text:
                    parts = re.split(r"(" + h_pat + "|" + a_pat + ")", text)
                    role, content = None, ""
                    for part in parts:
                        if re.match(h_pat, part):
                            if role and content:
                                turns.append({"role": role, "content": content.strip()})
                            role, content = "human", ""
                        elif re.match(a_pat, part):
                            if role and content:
                                turns.append({"role": role, "content": content.strip()})
                            role, content = "assistant", ""
                        else:
                            content += part
                    if role and content:
                        turns.append({"role": role, "content": content.strip()})
                    break
            return turns

        chosen_turns = parse_conv(chosen)

        # Topic classification
        topic = "general"
        for turn in chosen_turns:
            if turn.get("role") == "human":
                t = turn.get("content", "")[:100].lower()
                if any(w in t for w in ["code", "program", "python", "function"]):
                    topic = "coding"
                elif any(w in t for w in ["write", "story", "poem", "essay"]):
                    topic = "creative_writing"
                elif any(w in t for w in ["explain", "what is", "how does"]):
                    topic = "explanation"
                elif any(w in t for w in ["help", "advice", "suggest"]):
                    topic = "advice"
                elif any(w in t for w in ["translate", "chinese", "spanish"]):
                    topic = "translation"
                break

        topics[topic] = topics.get(topic, 0) + 1

        # Length patterns
        if len(chosen) > len(rejected) * 1.2:
            patterns["chosen_longer"] += 1
        elif len(rejected) > len(chosen) * 1.2:
            patterns["rejected_longer"] += 1
        else:
            patterns["same_length"] += 1

        # Safety patterns
        safety_words = ["sorry", "can't", "cannot", "won't", "inappropriate"]
        if any(w in rejected.lower() for w in safety_words) and not any(
            w in chosen.lower() for w in safety_words
        ):
            patterns["chosen_safer"] += 1

        # Save example
        if len(pairs) < 20:
            pairs.append(
                {
                    "topic": topic,
                    "turn_count": len(chosen_turns),
                    "human_query": chosen_turns[0].get("content", "")[:300] if chosen_turns else "",
                }
            )

    def _analyze_swe_item(self, item, stats):
        """Analyze a SWE-bench style item."""
        import ast

        repo = item.get("repo", "unknown")
        stats["repos"][repo] = stats["repos"].get(repo, 0) + 1

        lang = item.get("repo_language", "unknown")
        stats["languages"][lang] = stats["languages"].get(lang, 0) + 1

        # Issue types
        issue_spec = item.get("issue_specificity", "")
        if isinstance(issue_spec, str) and issue_spec.startswith("["):
            try:
                types = ast.literal_eval(issue_spec)
                for t in types:
                    stats["issue_types"][t] = stats["issue_types"].get(t, 0) + 1
            except Exception:
                pass

        # Patch lines
        patch = item.get("patch", "")
        if isinstance(patch, str):
            lines = len([l for l in patch.split("\n") if l.startswith("+") or l.startswith("-")])
            stats["patch_lines"].append(lines)

        # Examples
        if len(stats["examples"]) < 5:
            stats["examples"].append(
                {
                    "repo": repo,
                    "language": lang,
                    "problem_statement": item.get("problem_statement", "")[:800],
                }
            )

    def _generate_analysis_report(
        self,
        dataset_id,
        sample_count,
        actual_size,
        rubrics_result,
        prompt_library,
        strategy_result,
        allocation,
        region,
        enhanced_context=None,
    ) -> str:
        """Generate analysis report markdown."""
        lines = []
        lines.append(f"# ðŸ”¬ {dataset_id} æ·±åº¦é€†å‘åˆ†æžæŠ¥å‘Š")
        lines.append("")
        lines.append(f"> **åˆ†æžæ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> **æ•°æ®é›†**: {dataset_id}")
        lines.append(f"> **åˆ†æžæ ·æœ¬**: {sample_count} æ¡")
        lines.append(f"> **ç›®æ ‡è§„æ¨¡**: {actual_size:,} æ¡")
        lines.append("")

        # LLM-enhanced purpose summary
        if (
            enhanced_context
            and enhanced_context.generated
            and enhanced_context.dataset_purpose_summary
        ):
            lines.append(f"> {enhanced_context.dataset_purpose_summary}")
            lines.append("")

        lines.append("---")
        lines.append("")

        lines.append("## ðŸ“Š æ‰§è¡Œæ‘˜è¦")
        lines.append("")
        lines.append("| ç»´åº¦ | å‘çŽ° |")
        lines.append("|------|------|")

        if rubrics_result:
            lines.append(
                f"| **è¯„åˆ†æ ‡å‡†** | {rubrics_result.total_rubrics:,} æ¡ï¼Œ{rubrics_result.unique_patterns:,} ç§ç‹¬ç‰¹æ¨¡å¼ |"
            )
        if prompt_library:
            lines.append(
                f"| **Promptæ¨¡æ¿** | {prompt_library.unique_count} ä¸ªåŽ»é‡åŽçš„ç³»ç»Ÿæç¤ºæ¨¡æ¿ |"
            )
        if strategy_result:
            lines.append(
                f"| **æ•°æ®æ¥æº** | æ··åˆç­–ç•¥ï¼ˆåˆæˆ {strategy_result.synthetic_score * 100:.0f}% + æ”¹ç¼– {strategy_result.modified_score * 100:.0f}%ï¼‰ |"
            )

        lines.append(
            f"| **å¤çŽ°æˆæœ¬** | çº¦ ${allocation.total_cost:,.0f}ï¼ˆäººå·¥ ${allocation.total_human_cost:,.0f} + API ${allocation.total_machine_cost:,.0f}ï¼‰ |"
        )
        lines.append(
            f"| **äººæœºåˆ†é…** | äººå·¥ {allocation.human_work_percentage:.0f}%ï¼Œæœºå™¨ {allocation.machine_work_percentage:.0f}% |"
        )
        lines.append("")

        # LLM-enhanced methodology insights
        if (
            enhanced_context
            and enhanced_context.generated
            and enhanced_context.key_methodology_insights
        ):
            lines.append("## ðŸ” æ–¹æ³•å­¦æ´žå¯Ÿ")
            lines.append("")
            for insight in enhanced_context.key_methodology_insights:
                lines.append(f"- {insight}")
            lines.append("")

        # LLM-enhanced competitive positioning
        if (
            enhanced_context
            and enhanced_context.generated
            and enhanced_context.competitive_positioning
        ):
            lines.append("## ðŸ† ç«žäº‰å®šä½")
            lines.append("")
            lines.append(enhanced_context.competitive_positioning)
            lines.append("")

        # LLM-enhanced domain tips
        if (
            enhanced_context
            and enhanced_context.generated
            and enhanced_context.domain_specific_tips
        ):
            lines.append("## ðŸ’¡ é¢†åŸŸå»ºè®®")
            lines.append("")
            for tip in enhanced_context.domain_specific_tips:
                lines.append(f"- {tip}")
            lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> æŠ¥å‘Šç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ")

        return "\n".join(lines)

    def _generate_reproduction_guide(
        self,
        dataset_id,
        schema_info,
        category_set,
        sub_category_set,
        system_prompts_by_domain,
        rubrics_examples,
        sample_items,
        rubrics_result,
        prompt_library,
        allocation,
        is_preference_dataset,
        preference_pairs,
        preference_topics,
        preference_patterns,
        is_swe_dataset,
        swe_stats,
        llm_analysis,
        enhanced_context=None,
    ) -> str:
        """Generate reproduction guide markdown."""
        lines = []
        lines.append(f"# ðŸ“‹ {dataset_id} å¤åˆ»æŒ‡å—")
        lines.append("")

        if is_swe_dataset:
            lines.append("> **è¿™æ˜¯ä¸€ä¸ªè½¯ä»¶å·¥ç¨‹è¯„æµ‹æ•°æ®é›† (SWE-bench é£Žæ ¼)ã€‚**")
        elif is_preference_dataset:
            lines.append("> **è¿™æ˜¯ä¸€ä¸ª RLHF åå¥½æ•°æ®é›†ã€‚**")
        elif (
            enhanced_context
            and enhanced_context.generated
            and enhanced_context.dataset_purpose_summary
        ):
            lines.append(f"> {enhanced_context.dataset_purpose_summary}")
        elif llm_analysis and llm_analysis.dataset_type != "unknown":
            lines.append(f"> **æ•°æ®é›†ç±»åž‹: {llm_analysis.dataset_type}ã€‚{llm_analysis.purpose}**")
        else:
            lines.append("> **æœ¬æŒ‡å—æä¾›å¯ç›´æŽ¥æ“ä½œçš„æ¨¡æ¿å’Œè§„èŒƒã€‚**")
        lines.append("")
        lines.append("---")
        lines.append("")

        # LLM-enhanced reproduction strategy
        if (
            enhanced_context
            and enhanced_context.generated
            and enhanced_context.reproduction_strategy
        ):
            lines.append("## ðŸŽ¯ å¤åˆ»ç­–ç•¥")
            lines.append("")
            lines.append(enhanced_context.reproduction_strategy)
            lines.append("")

        # LLM-enhanced methodology insights
        if (
            enhanced_context
            and enhanced_context.generated
            and enhanced_context.key_methodology_insights
        ):
            lines.append("## ðŸ” æ–¹æ³•å­¦æ´žå¯Ÿ")
            lines.append("")
            for insight in enhanced_context.key_methodology_insights:
                lines.append(f"- {insight}")
            lines.append("")

        # LLM analysis section (for unknown types analyzed by LLM)
        if llm_analysis and llm_analysis.dataset_type != "unknown":
            from datarecipe.analyzers.llm_dataset_analyzer import generate_llm_guide_section

            lines.append(generate_llm_guide_section(llm_analysis))
            lines.append("")

        # Schema section
        lines.append("## ðŸ“ æ•°æ®ç»“æž„è§„èŒƒ (Schema)")
        lines.append("")
        lines.append("| å­—æ®µå | ç±»åž‹ | è¯´æ˜Ž |")
        lines.append("|--------|------|------|")
        for fld, info in schema_info.items():
            lines.append(f"| `{fld}` | `{info['type']}` | â€” |")
        lines.append("")

        # Cost section
        if allocation:
            lines.append("## ðŸ’° æˆæœ¬ä¼°ç®—")
            lines.append("")
            lines.append(f"- **äººå·¥æˆæœ¬**: ${allocation.total_human_cost:,.0f}")
            lines.append(f"- **API æˆæœ¬**: ${allocation.total_machine_cost:,.0f}")
            lines.append(f"- **æ€»è®¡**: ${allocation.total_cost:,.0f}")
            lines.append(f"- **äººå·¥å æ¯”**: {allocation.human_work_percentage:.0f}%")
            lines.append("")

        # LLM-enhanced domain tips
        if (
            enhanced_context
            and enhanced_context.generated
            and enhanced_context.domain_specific_tips
        ):
            lines.append("## ðŸ’¡ é¢†åŸŸå»ºè®®")
            lines.append("")
            for tip in enhanced_context.domain_specific_tips:
                lines.append(f"- {tip}")
            lines.append("")

        # LLM-enhanced risks
        if enhanced_context and enhanced_context.generated and enhanced_context.tailored_risks:
            lines.append("## âš ï¸ é£Žé™©æç¤º")
            lines.append("")
            lines.append("| ç­‰çº§ | é£Žé™© | ç¼“è§£æŽªæ–½ |")
            lines.append("|------|------|----------|")
            for risk in enhanced_context.tailored_risks:
                if isinstance(risk, dict):
                    lines.append(
                        f"| {risk.get('level', '')} | {risk.get('description', '')} | {risk.get('mitigation', '')} |"
                    )
            lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> æŒ‡å—ç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ")

        return "\n".join(lines)

    def _generate_ai_agent_layer(
        self,
        output_mgr: "OutputManager",
        result: AnalysisResult,
        dataset_id: str,
        dataset_type: str,
        sample_count: int,
        actual_size: int,
        allocation: Any,
        complexity_metrics: Any,
        rubrics_result: Any,
        prompt_library: Any,
        llm_analysis: Any,
        is_preference_dataset: bool,
        is_swe_dataset: bool,
    ):
        """Generate AI Agent layer files."""
        subdirs = OUTPUT_SUBDIRS

        # Generate agent_context.json
        self._generate_ai_agent_context(
            output_mgr,
            result,
            dataset_id,
            dataset_type,
            sample_count,
            actual_size,
            allocation,
            complexity_metrics,
            subdirs,
        )

        # Generate workflow_state.json
        self._generate_ai_workflow_state(output_mgr, result, dataset_id, dataset_type, subdirs)

        # Generate reasoning_traces.json
        self._generate_ai_reasoning_traces(
            output_mgr,
            result,
            dataset_id,
            dataset_type,
            actual_size,
            allocation,
            complexity_metrics,
            rubrics_result,
            prompt_library,
            subdirs,
        )

        # Generate pipeline.yaml
        self._generate_ai_pipeline(
            output_mgr,
            result,
            dataset_id,
            dataset_type,
            is_preference_dataset,
            is_swe_dataset,
            subdirs,
        )

        # Generate README.md for AI Agent directory
        self._generate_ai_agent_readme(output_mgr, result, dataset_id, dataset_type, subdirs)

    def _generate_ai_agent_context(
        self,
        output_mgr: "OutputManager",
        result: AnalysisResult,
        dataset_id: str,
        dataset_type: str,
        sample_count: int,
        actual_size: int,
        allocation: Any,
        complexity_metrics: Any,
        subdirs: dict,
    ):
        """Generate agent_context.json - aggregated entry point for AI agents."""
        context = {
            "_meta": {
                "version": "1.0",
                "generated_at": datetime.now().isoformat(),
                "generator": "DataRecipe",
                "purpose": "AI Agent èšåˆå…¥å£ï¼Œå¼•ç”¨å…¶ä»–æ–‡ä»¶è€Œéžå¤åˆ¶",
            },
            "project": {
                "name": dataset_id,
                "type": dataset_type or "unknown",
                "source": "huggingface",
                "sample_count": sample_count,
                "target_size": actual_size,
            },
            "summary": {
                "total_cost": result.reproduction_cost.get("total", 0),
                "human_cost": result.reproduction_cost.get("human", 0),
                "api_cost": result.reproduction_cost.get("api", 0),
                "human_percentage": result.human_percentage,
                "rubric_patterns": result.rubric_patterns,
                "prompt_templates": result.prompt_templates,
                "field_count": len(result.fields),
            },
            "key_decisions": [
                {
                    "decision": "dataset_type",
                    "value": dataset_type or "unknown",
                    "reasoning_ref": "#/reasoning/dataset_type",
                },
                {
                    "decision": "human_percentage",
                    "value": result.human_percentage,
                    "reasoning_ref": "#/reasoning/human_percentage",
                },
                {
                    "decision": "cost_estimate",
                    "value": result.reproduction_cost.get("total", 0),
                    "reasoning_ref": "#/reasoning/cost",
                },
            ],
            "complexity": None,
            "file_references": {
                "executive_summary": f"../{subdirs['decision']}/EXECUTIVE_SUMMARY.md",
                "milestone_plan": f"../{subdirs['project']}/MILESTONE_PLAN.md",
                "annotation_spec": f"../{subdirs['annotation']}/ANNOTATION_SPEC.md",
                "reproduction_guide": f"../{subdirs['guide']}/REPRODUCTION_GUIDE.md",
                "cost_breakdown": f"../{subdirs['cost']}/COST_BREAKDOWN.md",
                "allocation": f"../{subdirs['cost']}/allocation.json",
                "recipe_summary": "../recipe_summary.json",
            },
            "quick_actions": [
                {
                    "action": "review_spec",
                    "description": "å®¡æ ¸æ ‡æ³¨è§„èŒƒ",
                    "file": f"../{subdirs['annotation']}/ANNOTATION_SPEC.md",
                    "assignee": "human",
                },
                {
                    "action": "review_cost",
                    "description": "å®¡æ ¸æˆæœ¬ä¼°ç®—",
                    "file": f"../{subdirs['cost']}/COST_BREAKDOWN.md",
                    "assignee": "human",
                },
                {
                    "action": "start_reproduction",
                    "description": "å¼€å§‹å¤åˆ»ç”Ÿäº§",
                    "file": f"../{subdirs['guide']}/REPRODUCTION_GUIDE.md",
                    "assignee": "human",
                },
            ],
        }

        # Add complexity info if available
        if complexity_metrics:
            context["complexity"] = {
                "domain": complexity_metrics.primary_domain.value
                if hasattr(complexity_metrics.primary_domain, "value")
                else str(complexity_metrics.primary_domain),
                "difficulty_score": complexity_metrics.difficulty_score,
                "time_multiplier": complexity_metrics.time_multiplier,
                "cost_multiplier": complexity_metrics.cost_multiplier,
            }

        path = output_mgr.get_path("ai_agent", "agent_context.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(context, f, indent=2, ensure_ascii=False)
        result.files_generated.append(
            output_mgr.get_relative_path("ai_agent", "agent_context.json")
        )

    def _generate_ai_workflow_state(
        self,
        output_mgr: "OutputManager",
        result: AnalysisResult,
        dataset_id: str,
        dataset_type: str,
        subdirs: dict,
    ):
        """Generate workflow_state.json - workflow state tracking."""
        state = {
            "_meta": {
                "version": "1.0",
                "generated_at": datetime.now().isoformat(),
                "purpose": "å·¥ä½œæµçŠ¶æ€è¿½è¸ªï¼Œä¾› AI Agent äº†è§£å½“å‰è¿›åº¦å’Œä¸‹ä¸€æ­¥",
            },
            "current_phase": "analysis_complete",
            "phases": {
                "data_loading": {
                    "status": "completed",
                    "description": "æ•°æ®é›†åŠ è½½",
                },
                "analysis": {
                    "status": "completed",
                    "description": "æ·±åº¦é€†å‘åˆ†æž",
                    "outputs": [
                        f"../{subdirs['data']}/complexity_analysis.json",
                        f"../{subdirs['cost']}/allocation.json",
                    ],
                },
                "report_generation": {
                    "status": "completed",
                    "description": "æŠ¥å‘Šç”Ÿæˆ",
                    "outputs": [
                        f"../{subdirs['decision']}/EXECUTIVE_SUMMARY.md",
                        f"../{subdirs['project']}/MILESTONE_PLAN.md",
                        f"../{subdirs['annotation']}/ANNOTATION_SPEC.md",
                        f"../{subdirs['guide']}/REPRODUCTION_GUIDE.md",
                    ],
                },
                "review": {
                    "status": "pending",
                    "description": "äººå·¥å®¡æ ¸åˆ†æžç»“æžœ",
                    "blocked_by": [],
                    "assignee": "human",
                },
                "reproduction_planning": {
                    "status": "pending",
                    "description": "åˆ¶å®šå¤åˆ»è®¡åˆ’",
                    "blocked_by": ["review"],
                    "assignee": "human",
                },
                "production": {
                    "status": "pending",
                    "description": "å¼€å§‹æ•°æ®ç”Ÿäº§",
                    "blocked_by": ["reproduction_planning"],
                    "assignee": "human",
                },
            },
            "next_actions": [
                {
                    "action": "review_executive_summary",
                    "description": "å®¡æ ¸æ‰§è¡Œæ‘˜è¦ï¼Œç¡®è®¤åˆ†æžç»“è®º",
                    "file": f"../{subdirs['decision']}/EXECUTIVE_SUMMARY.md",
                    "assignee": "human",
                    "priority": "high",
                },
                {
                    "action": "review_cost_estimate",
                    "description": "å®¡æ ¸æˆæœ¬ä¼°ç®—ï¼Œç¡®è®¤é¢„ç®—",
                    "file": f"../{subdirs['cost']}/COST_BREAKDOWN.md",
                    "assignee": "human",
                    "priority": "high",
                },
                {
                    "action": "review_annotation_spec",
                    "description": "å®¡æ ¸æ ‡æ³¨è§„èŒƒï¼Œå‡†å¤‡ç”Ÿäº§",
                    "file": f"../{subdirs['annotation']}/ANNOTATION_SPEC.md",
                    "assignee": "human",
                    "priority": "medium",
                },
            ],
            "blockers": [],
            "decisions_needed": [
                {
                    "question": "æ˜¯å¦é‡‡ç”¨æ­¤æ•°æ®é›†çš„æ–¹æ³•è®ºï¼Ÿ",
                    "options": ["approved", "needs_modification", "rejected"],
                    "impact": "å½±å“åŽç»­å¤åˆ»ç­–ç•¥",
                },
                {
                    "question": "æˆæœ¬é¢„ç®—æ˜¯å¦å¯æŽ¥å—ï¼Ÿ",
                    "options": ["approved", "needs_adjustment"],
                    "impact": "å½±å“é¡¹ç›®è§„æ¨¡å’Œæ—¶é—´çº¿",
                },
            ],
        }

        path = output_mgr.get_path("ai_agent", "workflow_state.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(state, f, indent=2, ensure_ascii=False)
        result.files_generated.append(
            output_mgr.get_relative_path("ai_agent", "workflow_state.json")
        )

    def _generate_ai_reasoning_traces(
        self,
        output_mgr: "OutputManager",
        result: AnalysisResult,
        dataset_id: str,
        dataset_type: str,
        actual_size: int,
        allocation: Any,
        complexity_metrics: Any,
        rubrics_result: Any,
        prompt_library: Any,
        subdirs: dict,
    ):
        """Generate reasoning_traces.json - reasoning chains for all conclusions."""
        total_cost = result.reproduction_cost.get("total", 0)
        human_cost = result.reproduction_cost.get("human", 0)
        api_cost = result.reproduction_cost.get("api", 0)

        traces = {
            "_meta": {
                "version": "1.0",
                "generated_at": datetime.now().isoformat(),
                "purpose": "æ‰€æœ‰ç»“è®ºçš„æŽ¨ç†é“¾ï¼Œä¾›äººç±»ç†è§£å’Œ AI éªŒè¯",
            },
            "reasoning": {
                "dataset_type": {
                    "conclusion": {
                        "value": dataset_type or "unknown",
                        "display": f"æ•°æ®é›†ç±»åž‹: {dataset_type or 'unknown'}",
                    },
                    "chain": [],
                    "confidence": 0.0,
                    "human_explanation": "",
                },
                "human_percentage": {
                    "conclusion": {
                        "value": result.human_percentage,
                        "display": f"äººå·¥æ¯”ä¾‹: {result.human_percentage}%",
                    },
                    "chain": [],
                    "confidence": 0.0,
                    "human_explanation": "",
                },
                "cost": {
                    "conclusion": {"value": total_cost, "display": f"æ€»æˆæœ¬: ${total_cost:,.0f}"},
                    "chain": [],
                    "confidence": 0.0,
                    "range": {
                        "low": round(total_cost * 0.7, 2),
                        "high": round(total_cost * 1.4, 2),
                    },
                    "human_explanation": "",
                },
            },
        }

        # Build dataset type reasoning chain
        type_chain = []
        type_confidence = 0.5

        if dataset_type == "preference":
            type_chain.append(
                {
                    "step": "æ£€æµ‹åå¥½æ•°æ®ç»“æž„",
                    "evidence": "å‘çŽ° chosen/rejected å­—æ®µå¯¹",
                    "impact": "åˆ¤å®šä¸º RLHF åå¥½æ•°æ®é›†",
                }
            )
            type_confidence = 0.95
        elif dataset_type == "evaluation":
            type_chain.append(
                {
                    "step": "æ£€æµ‹è¯„åˆ†æ ‡å‡†",
                    "evidence": f"å‘çŽ° {result.rubric_patterns} ç§è¯„åˆ†æ¨¡å¼",
                    "impact": "åˆ¤å®šä¸ºè¯„æµ‹æ•°æ®é›†",
                }
            )
            type_confidence = 0.9
        elif dataset_type == "swe_bench":
            type_chain.append(
                {
                    "step": "æ£€æµ‹ SWE ç»“æž„",
                    "evidence": "å‘çŽ° repo/patch/problem_statement å­—æ®µ",
                    "impact": "åˆ¤å®šä¸ºè½¯ä»¶å·¥ç¨‹è¯„æµ‹æ•°æ®é›†",
                }
            )
            type_confidence = 0.95

        traces["reasoning"]["dataset_type"]["chain"] = type_chain
        traces["reasoning"]["dataset_type"]["confidence"] = type_confidence
        traces["reasoning"]["dataset_type"]["human_explanation"] = (
            f"é€šè¿‡åˆ†æžæ•°æ®ç»“æž„å’Œå­—æ®µï¼Œåˆ¤å®šä¸º {dataset_type or 'unknown'} ç±»åž‹æ•°æ®é›†ã€‚"
        )

        # Build human percentage reasoning chain
        human_chain = []
        human_confidence = 0.7

        if allocation:
            human_chain.append(
                {
                    "step": "åˆ†æžä»»åŠ¡ç±»åž‹",
                    "evidence": f"åŒ…å« {len(allocation.tasks)} ç§ä»»åŠ¡ç±»åž‹",
                    "impact": f"äººå·¥å æ¯” {result.human_percentage}%",
                }
            )
            human_confidence = 0.8

        if complexity_metrics:
            domain = (
                complexity_metrics.primary_domain.value
                if hasattr(complexity_metrics.primary_domain, "value")
                else str(complexity_metrics.primary_domain)
            )
            human_chain.append(
                {
                    "step": "è¯„ä¼°å¤æ‚åº¦",
                    "evidence": f"é¢†åŸŸ: {domain}, éš¾åº¦åˆ†æ•°: {complexity_metrics.difficulty_score:.2f}",
                    "impact": f"æˆæœ¬ä¹˜æ•°: {complexity_metrics.cost_multiplier:.2f}",
                }
            )
            human_confidence += 0.1

        traces["reasoning"]["human_percentage"]["chain"] = human_chain
        traces["reasoning"]["human_percentage"]["confidence"] = min(human_confidence, 0.95)
        traces["reasoning"]["human_percentage"]["human_explanation"] = (
            f"åŸºäºŽä»»åŠ¡åˆ†æžï¼Œé¢„ä¼°äººå·¥æ¯”ä¾‹ä¸º {result.human_percentage}%ã€‚"
        )

        # Build cost reasoning chain
        cost_chain = [
            {
                "step": "è®¡ç®—äººå·¥æˆæœ¬",
                "evidence": f"äººå·¥ä»»åŠ¡æˆæœ¬ ${human_cost:,.0f}",
                "value": human_cost,
            },
            {
                "step": "è®¡ç®— API æˆæœ¬",
                "evidence": f"API è°ƒç”¨æˆæœ¬ ${api_cost:,.0f}",
                "value": api_cost,
            },
        ]

        if complexity_metrics:
            cost_chain.append(
                {
                    "step": "åº”ç”¨å¤æ‚åº¦ä¹˜æ•°",
                    "evidence": f"å¤æ‚åº¦ä¹˜æ•° {complexity_metrics.cost_multiplier:.2f}",
                    "multiplier": complexity_metrics.cost_multiplier,
                }
            )

        cost_chain.append(
            {
                "step": "è®¡ç®—æ€»æˆæœ¬",
                "evidence": f"äººå·¥ ${human_cost:,.0f} + API ${api_cost:,.0f}",
                "result": total_cost,
            }
        )

        traces["reasoning"]["cost"]["chain"] = cost_chain
        traces["reasoning"]["cost"]["confidence"] = 0.75
        traces["reasoning"]["cost"]["human_explanation"] = (
            f"åŸºäºŽä»»åŠ¡åˆ†è§£å’Œ Token åˆ†æžï¼Œé¢„ä¼°æ€»æˆæœ¬ ${total_cost:,.0f}ï¼Œ"
            f"ç½®ä¿¡åŒºé—´ ${total_cost * 0.7:,.0f} - ${total_cost * 1.4:,.0f}ã€‚"
        )

        path = output_mgr.get_path("ai_agent", "reasoning_traces.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(traces, f, indent=2, ensure_ascii=False)
        result.files_generated.append(
            output_mgr.get_relative_path("ai_agent", "reasoning_traces.json")
        )

    def _generate_ai_pipeline(
        self,
        output_mgr: "OutputManager",
        result: AnalysisResult,
        dataset_id: str,
        dataset_type: str,
        is_preference_dataset: bool,
        is_swe_dataset: bool,
        subdirs: dict,
    ):
        """Generate pipeline.yaml - executable pipeline for AI agents."""
        lines = []
        lines.append("# æ•°æ®å¤åˆ»æµæ°´çº¿")
        lines.append("# ä¾› AI Agent æ‰§è¡Œçš„å¯æ“ä½œæ­¥éª¤")
        lines.append("")
        lines.append("name: æ•°æ®å¤åˆ»æµæ°´çº¿")
        lines.append("version: '1.0'")
        lines.append(f"source_dataset: {dataset_id}")
        lines.append(f"dataset_type: {dataset_type or 'unknown'}")
        lines.append(f"generated_at: {datetime.now().isoformat()}")
        lines.append("")

        # Variables section
        lines.append("variables:")
        lines.append(f'  source_dataset: "{dataset_id}"')
        lines.append("  target_size: 1000  # å¯è°ƒæ•´")
        lines.append(f"  human_percentage: {result.human_percentage}")
        lines.append(f"  estimated_cost: {result.reproduction_cost.get('total', 0)}")
        lines.append("")

        # Phases
        lines.append("phases:")
        lines.append("")

        # Phase 1: Analysis Review
        lines.append("  - name: analysis_review")
        lines.append("    description: å®¡æ ¸åˆ†æžç»“æžœ")
        lines.append("    steps:")
        lines.append("      - action: review_executive_summary")
        lines.append("        description: å®¡æ ¸æ‰§è¡Œæ‘˜è¦")
        lines.append(f"        input: ../{subdirs['decision']}/EXECUTIVE_SUMMARY.md")
        lines.append("        assignee: human")
        lines.append("        required: true")
        lines.append("")
        lines.append("      - action: review_cost_estimate")
        lines.append("        description: å®¡æ ¸æˆæœ¬ä¼°ç®—")
        lines.append(f"        input: ../{subdirs['cost']}/COST_BREAKDOWN.md")
        lines.append("        assignee: human")
        lines.append("")
        lines.append("      - action: approve_methodology")
        lines.append("        description: ç¡®è®¤å¤åˆ»æ–¹æ³•è®º")
        lines.append(f"        input: ../{subdirs['guide']}/REPRODUCTION_GUIDE.md")
        lines.append("        assignee: human")
        lines.append("        required: true")
        lines.append("")

        # Phase 2: Setup
        lines.append("  - name: setup")
        lines.append("    description: çŽ¯å¢ƒå‡†å¤‡")
        lines.append("    depends_on: [analysis_review]")
        lines.append("    steps:")
        lines.append("      - action: setup_annotation_tool")
        lines.append("        description: é…ç½®æ ‡æ³¨å·¥å…·")
        lines.append(f"        spec: ../{subdirs['annotation']}/ANNOTATION_SPEC.md")
        lines.append("        assignee: agent")
        lines.append("")
        lines.append("      - action: prepare_rubric_templates")
        lines.append("        description: å‡†å¤‡è¯„åˆ†æ¨¡æ¿")
        lines.append(f"        input: ../{subdirs['annotation']}/rubric_template.yaml")
        lines.append("        assignee: agent")
        lines.append("")

        # Phase 3: Pilot
        lines.append("  - name: pilot")
        lines.append("    description: è¯•ç‚¹ç”Ÿäº§")
        lines.append("    depends_on: [setup]")
        lines.append("    steps:")
        lines.append("      - action: create_pilot_batch")
        lines.append("        description: åˆ›å»ºè¯•ç‚¹æ‰¹æ¬¡ (50 æ¡)")
        lines.append("        count: 50")
        lines.append("        assignee: human")
        lines.append("")
        lines.append("      - action: quality_review_pilot")
        lines.append("        description: è¯•ç‚¹è´¨é‡å®¡æ ¸")
        lines.append("        assignee: human")
        lines.append("")

        # Phase 4: Production
        lines.append("  - name: production")
        lines.append("    description: ä¸»ä½“ç”Ÿäº§")
        lines.append("    depends_on: [pilot]")
        lines.append("    steps:")
        lines.append("      - action: batch_production")
        lines.append("        description: æ‰¹é‡ç”Ÿäº§")
        lines.append('        count: "{{ target_size }}"')
        lines.append("        assignee: human")
        lines.append("")
        lines.append("      - action: incremental_qa")
        lines.append("        description: å¢žé‡è´¨æ£€")
        lines.append("        sample_rate: 0.2")
        lines.append("        assignee: human")
        lines.append("")

        # Phase 5: Final QA
        lines.append("  - name: final_qa")
        lines.append("    description: æœ€ç»ˆè´¨é‡å®¡æ ¸")
        lines.append("    depends_on: [production]")
        lines.append("    steps:")
        lines.append("      - action: full_qa_review")
        lines.append("        description: å…¨é‡è´¨æ£€")
        lines.append("        assignee: human")
        lines.append("")
        lines.append("      - action: generate_qa_report")
        lines.append("        description: ç”Ÿæˆè´¨æ£€æŠ¥å‘Š")
        lines.append("        assignee: agent")
        lines.append("")
        lines.append("      - action: final_approval")
        lines.append("        description: æœ€ç»ˆå®¡æ‰¹")
        lines.append("        assignee: human")
        lines.append("        required: true")
        lines.append("")

        # Error handling
        lines.append("error_handling:")
        lines.append("  on_qa_failure:")
        lines.append("    action: flag_for_revision")
        lines.append("    notify: human")
        lines.append("  on_budget_exceeded:")
        lines.append("    action: pause_and_review")
        lines.append("    notify: human")

        path = output_mgr.get_path("ai_agent", "pipeline.yaml")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(output_mgr.get_relative_path("ai_agent", "pipeline.yaml"))

    def _generate_ai_agent_readme(
        self,
        output_mgr: "OutputManager",
        result: AnalysisResult,
        dataset_id: str,
        dataset_type: str,
        subdirs: dict,
    ):
        """Generate README.md for AI Agent directory."""
        lines = []
        lines.append(f"# {dataset_id} - AI Agent å…¥å£")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        lines.append(f"> æ•°æ®é›†ç±»åž‹: {dataset_type or 'unknown'}")
        lines.append("")
        lines.append("æœ¬ç›®å½•åŒ…å«ä¾› AI Agent æ¶ˆè´¹çš„ç»“æž„åŒ–æ•°æ®ï¼Œä¸Žäººç±»å¯è¯»çš„ Markdown æ–‡æ¡£äº’è¡¥ã€‚")
        lines.append("")
        lines.append("---")
        lines.append("")

        lines.append("## æ–‡ä»¶è¯´æ˜Ž")
        lines.append("")
        lines.append("| æ–‡ä»¶ | ç”¨é€” | æ¶ˆè´¹è€… |")
        lines.append("|------|------|--------|")
        lines.append("| `agent_context.json` | èšåˆå…¥å£ï¼Œå¼•ç”¨å…¶ä»–æ–‡ä»¶ | AI Agent |")
        lines.append("| `workflow_state.json` | å·¥ä½œæµçŠ¶æ€ï¼Œå½“å‰é˜¶æ®µå’Œä¸‹ä¸€æ­¥ | AI Agent |")
        lines.append("| `reasoning_traces.json` | æŽ¨ç†é“¾ï¼Œè§£é‡Šæ¯ä¸ªç»“è®ºçš„åŽŸå›  | AI Agent + äººç±» |")
        lines.append("| `pipeline.yaml` | å¯æ‰§è¡Œæµæ°´çº¿ï¼Œå®šä¹‰æ ‡å‡†æ“ä½œæ­¥éª¤ | AI Agent |")
        lines.append("")

        lines.append("## å¿«é€Ÿå¼€å§‹")
        lines.append("")
        lines.append("### 1. èŽ·å–é¡¹ç›®ä¸Šä¸‹æ–‡")
        lines.append("")
        lines.append("```python")
        lines.append("import json")
        lines.append("")
        lines.append("with open('agent_context.json') as f:")
        lines.append("    context = json.load(f)")
        lines.append("")
        lines.append("print(f\"æ•°æ®é›†: {context['project']['name']}\")")
        lines.append("print(f\"ç±»åž‹: {context['project']['type']}\")")
        lines.append("print(f\"æ€»æˆæœ¬: ${context['summary']['total_cost']}\")")
        lines.append("```")
        lines.append("")

        lines.append("### 2. æ£€æŸ¥å·¥ä½œæµçŠ¶æ€")
        lines.append("")
        lines.append("```python")
        lines.append("with open('workflow_state.json') as f:")
        lines.append("    state = json.load(f)")
        lines.append("")
        lines.append("print(f\"å½“å‰é˜¶æ®µ: {state['current_phase']}\")")
        lines.append("for action in state['next_actions']:")
        lines.append("    print(f\"ä¸‹ä¸€æ­¥: {action['description']} ({action['assignee']})\")")
        lines.append("```")
        lines.append("")

        lines.append("### 3. ç†è§£å†³ç­–æŽ¨ç†")
        lines.append("")
        lines.append("```python")
        lines.append("with open('reasoning_traces.json') as f:")
        lines.append("    traces = json.load(f)")
        lines.append("")
        lines.append("cost = traces['reasoning']['cost']")
        lines.append("print(f\"æˆæœ¬: {cost['conclusion']['display']}\")")
        lines.append("print(f\"ç½®ä¿¡åº¦: {cost['confidence']}\")")
        lines.append("print(f\"åŽŸå› : {cost['human_explanation']}\")")
        lines.append("```")
        lines.append("")

        lines.append("---")
        lines.append("")
        lines.append("> ç”± DataRecipe è‡ªåŠ¨ç”Ÿæˆ")

        path = output_mgr.get_path("ai_agent", "README.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        result.files_generated.append(output_mgr.get_relative_path("ai_agent", "README.md"))
